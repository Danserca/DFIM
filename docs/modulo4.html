<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Módulo 4</title>

<script src="site_libs/header-attrs-2.20/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script>
document.addEventListener("DOMContentLoaded", function() {
  var codeBlocks = document.querySelectorAll("pre code");
  codeBlocks.forEach(function(block) {
    var button = document.createElement("button");
    button.className = "copy-button";
    button.type = "button";
    button.innerHTML = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M10 1.5v1h2a.5.5 0 0 1 .5.5v11a.5.5 0 0 1-.5.5H4a.5.5 0 0 1-.5-.5v-11a.5.5 0 0 1 .5-.5h2v-1a.5.5 0 0 1 1 0v1h2v-1a.5.5 0 0 1 1 0zM4.5 3v11h7V3h-7zm4-1v1h-3V2h3z"/></svg>';

    button.addEventListener("click", function() {
      var code = block.innerText;
      navigator.clipboard.writeText(code).then(function() {
        button.innerHTML = 'Copiado!';
        setTimeout(function() {
          button.innerHTML = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M10 1.5v1h2a.5.5 0 0 1 .5.5v11a.5.5 0 0 1-.5.5H4a.5.5 0 0 1-.5-.5v-11a.5.5 0 0 1 .5-.5h2v-1a.5.5 0 0 1 1 0v1h2v-1a.5.5 0 0 1 1 0zM4.5 3v11h7V3h-7zm4-1v1h-3V2h3z"/></svg>';
        }, 2000);
      });
    });

    var pre = block.parentNode;
    pre.style.position = "relative";
    pre.insertBefore(button, block);
  });
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="assets/css/styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">DFIM</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Inicio</a>
</li>
<li>
  <a href="modulo1.html">Módulo 1</a>
</li>
<li>
  <a href="modulo2.html">Módulo 2</a>
</li>
<li>
  <a href="modulo3.html">Módulo 3</a>
</li>
<li>
  <a href="modulo4.html">Módulo 4</a>
</li>
<li>
  <a href="modulo5.html">Módulo 5</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Módulo 4</h1>
<h3 class="subtitle">Metodología</h3>

</div>


<style type="text/css">
.navbar {
  background-color: #0077C8 !important; /* Cambia este valor para ajustar el color del cintillo */
  border-color: #0077C8 !important; /* Opcional: cambia también el color del borde si es necesario */
}


TOC {
  color: #0077C8; 
}

.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    z-index: 2;
    color: #003057;
    background-color: #0077C8;
    border-color: #0077C8;
}

hr.cintillo {
  border: 3px solid #0077C8; /* Color y grosor del cintillo */
  margin: 20px 0; /* Espaciado arriba y abajo del cintillo */
}
</style>
<div style="color: #003057;">


<hr style="border: 3px solid#0077C8;" />
<div id="siglas-y-acrónimos" class="section level1">
<h1>1. Siglas y acrónimos</h1>
<div class="two-columns">
<div class="column">
<p>
<strong><b>ACNUR.</b></strong> Oficina del Alto Comisionado de las
Naciones Unidas para los Refugiados.
</p>
<p>
<strong><b>AIC.</b></strong> Criterio de Información de Akaike.
</p>
<p>
<strong><b>BIC.</b></strong> Criterio de Información Bayesiano.
</p>
<p>
<strong><b>CCI.</b></strong> Corte Penal Internacional.
</p>
<p>
<strong><b>CEAVE.</b></strong> Comisión Ejecutiva de Atención a Víctimas
del Estado de Chihuahua.
</p>
<p>
<strong><b>CENSO 2020.</b></strong> Censo de Población y Vivienda 2020.
</p>
<p>
<strong><b>CMDPDH.</b></strong> Comisión Mexicana de Defensa y Promoción
de los Derechos Humanos.
</p>
<p>
<strong><b>CV.</b></strong> Coeficientes de Variación.
</p>
<p>
<strong><b>DFI.</b></strong> Desplazamiento Forzado Interno/Desplazada
Forzada Interna.
</p>
<p>
<strong><b>DP2.</b></strong> Índice de intensidad migratoria.
</p>
<p>
<strong><b>ECADEFI-CHIH.</b></strong> Encuesta para Caracterizar a la
Población en Situación de Desplazamiento Forzado Interno en el Estado de
Chihuahua.
</p>
<p>
<strong><b>EGRISS.</b></strong> Grupo de Expertos en Estadísticas sobre
Refugiados Desplazados Internos y Apátridas.
</p>
<p>
<strong><b>ENADID.</b></strong> Encuesta Nacional de la Dinámica
Demográfica.
</p>
<p>
<strong><b>ENOE.</b></strong> Encuesta Nacional de Ocupación y Empleo.
</p>
</div>
<div class="column">
<p>
<strong><b>ENVIPE.</b></strong> Encuesta Nacional de Victimización y
Percepción sobre Seguridad Pública.
</p>
<p>
<strong><b>HFSSSD.</b></strong> Encuesta de Alta Frecuencia de Sudán del
Sur.
</p>
<p>
<strong><b>ICTY.</b></strong> Tribunal Penal Internacional para la ex
Yugoslavia.
</p>
<p>
<strong><b>IDMC.</b></strong> Centro de Monitoreo de Desplazamiento
Interno.
</p>
<p>
<strong><b>INE.</b></strong> Instituto Nacional de Estadística.
</p>
<p>
<strong><b>INEGI/Instituto.</b></strong> Instituto Nacional de
Estadística y Geografía.
</p>
<p>
<strong><b>IRIS.</b></strong> Recomendaciones Internacionales sobre
Estadísticas de Personas Desplazadas Internamente.
</p>
<p>
<strong><b>IRRS.</b></strong> Recomendaciones Internacionales sobre
Estadísticas de Refugiados.
</p>
<p>
<strong><b>JIPS.</b></strong> Joint IDP Profiling Service.
</p>
<p>
<strong><b>ONE.</b></strong> Oficinas nacionales de estadística.
</p>
<p>
<strong><b>PRINCIPIOS RECTORES.</b></strong> Principios Rectores de los
Desplazamientos Internos de la Organización de las Naciones Unidas.
</p>
<p>
<strong><b>SNIEG.</b></strong> Sistema Nacional de Información
Estadística y Geográfica.
</p>
<p>
<strong><b>SNIGSPIJ.</b></strong> Subsistema Nacional de Información
sobre Gobierno Seguridad Pública e Impartición de Justicia.
</p>
</div>
</div>
<br><br>
<hr style="border: 3px solid#0077C8;" />
</div>
<div id="integración-y-reconciliación" class="section level1">
<h1>2. Integración y reconciliación</h1>
<p>El EGRISS reconoce la complejidad de encontrar una única fuente de
datos que proporcione una imagen completa y precisa de la población en
situación DFI. Ante este desafío, el EGRISS (2023) propone la
integración de diversas fuentes de información como estrategia
fundamental para mejorar la cobertura, disponibilidad, frecuencia,
oportunidad, calidad y precisión de las estadísticas sobre DFI.</p>
<p>La integración de diferentes fuentes permite aprovechar las
fortalezas de cada una y compensar sus posibles limitaciones
individuales. Esto facilita la obtención de una visión más amplia y
confiable de la situación del DFI, incluyendo a grupos poblacionales
específicos que podrían quedar excluidos de una sola fuente. Además,
estas técnicas racionalizan las tareas de las Oficinas Nacionales de
Estadística (ONE) al enriquecer la información recopilada para producir
estadísticas sin aumentar costos ni la carga de respuesta de los
informantes (Naciones Unidas, 2020).</p>
<p>La integración de fuentes puede ofrecer una imagen más completa del
fenómeno, sobre todo en casos donde una encuesta tiene cobertura amplia
pero información limitada sobre variables de DFI, y otra tiene cobertura
limitada pero información más detallada sobre estas variables. La
integración de datos es especialmente útil cuando ninguna de las fuentes
cubre por completo ni de manera precisa a una población determinada,
cuando hay diferentes estimaciones para el mismo fenómeno, como es el
caso de las estadísticas sobre DFI en México, donde ninguna cifra ni
estimación coincide, sino que, por el contrario, difieren
considerablemente con se aprecia en la Tabla 3. Existen aproximaciones
nacionales y por entidad federativa, en el marco y fuera del marco del
del Sistema Nacional de Información Estadística y Geográfica
(SNIEG).</p>
<table class="table table-striped table-hover table-condensed" style="margin-left: auto; margin-right: auto;">
<caption>
<span style="font-size: 20px;">Comparación de Características de Fuentes
de Datos</span>
</caption>
<thead>
<tr>
<th style="text-align:center;">
Características
</th>
<th style="text-align:center;">
ENVIPE_2018
</th>
<th style="text-align:center;">
ENADID_2018
</th>
<th style="text-align:center;">
Censo_2020
</th>
<th style="text-align:center;">
IDMC
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
Objetivo general
</td>
<td style="text-align:center;">
Estimar el número de víctimas y número de delitos ocurridos, cifra
negra, costos de delitos, percepción de inseguridad…
</td>
<td style="text-align:center;">
Proporcionar información estadística … de la dinámica demográfica:
fecundidad, mortalidad y migración (interna e internacional).
</td>
<td style="text-align:center;">
Producir información sobre la dimensión, estructura y distribución
espacial de la población
</td>
<td style="text-align:center;">
Proveer la Base de Datos Global sobre Desplazamientos Internos.
</td>
</tr>
<tr>
<td style="text-align:center;">
Periodo de referencia
</td>
<td style="text-align:center;">
Enero – diciembre para victimización.
</td>
<td style="text-align:center;">
Cinco años antes (agosto 2013).
</td>
<td style="text-align:center;">
Cinco años antes (marzo 2015).
</td>
<td style="text-align:center;">
<ul>
<li></td>
</tr>
<tr>
<td style="text-align:center;">
Selección de la muestra
</td>
<td style="text-align:center;">
Probabilístico: trietápico, estratificado y por conglomerados.
</td>
<td style="text-align:center;">
Probabilístico, bietápico y por conglomerado estratificado.
</td>
<td style="text-align:center;">
<ul>
<li></td>
<td style="text-align:center;">
<ul>
<li></td>
</tr>
<tr>
<td style="text-align:center;">
Unidades de observación
</td>
<td style="text-align:center;">
Las viviendas seleccionadas, los hogares, los residentes del hogar y la
persona seleccionada en el hogar.
</td>
<td style="text-align:center;">
Vivienda particular habitada, residente habitual, Hogar, migrante
internacional, Mujer de 15 a 54 años.
</td>
<td style="text-align:center;">
<ul>
<li></td>
<td style="text-align:center;">
Hogares, Personas
</td>
</tr>
<tr>
<td style="text-align:center;">
Población objeto de estudio
</td>
<td style="text-align:center;">
Población de 18 años y más
</td>
<td style="text-align:center;">
Residente habitual del hogar, residentes habituales y migrantes
internacionales.
</td>
<td style="text-align:center;">
Residentes habituales del territorio nacional, las viviendas
particulares, y migrantes internacionales.
</td>
<td style="text-align:center;">
DFI
</td>
</tr>
<tr>
<td style="text-align:center;">
Periodo de levantamiento
</td>
<td style="text-align:center;">
Marzo - abril 2018
</td>
<td style="text-align:center;">
3 de agosto al 5 de octubre de 2018.
</td>
<td style="text-align:center;">
2 al 27 de marzo de 2020
</td>
<td style="text-align:center;">
<ul>
<li></td>
</tr>
<tr>
<td style="text-align:center;">
Cobertura geográfica
</td>
<td style="text-align:center;">
A nivel Nacional, Nacional urbano, Nacional rural, Entidad Federativa y
34 Áreas Metropolitanas de interés
</td>
<td style="text-align:center;">
Nacional (urbano de 2 500 habitantes y más, rural hasta 2 499
habitantes), entidad federativa y tamaños de localidad.
</td>
<td style="text-align:center;">
Nacional, entidad federativa, municipio o demarcación territorial,
localidad, AGEB, manzana urbana.
</td>
<td style="text-align:center;">
Mundial
</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>El SNIEG aborda el tema del DFI desde, al menos, dos de sus
subsistemas, desde el Subsistema Nacional de Información de Gobierno,
Seguridad Pública e Impartición de Justicia y desde el Subsistema de
Información Nacional de Información Demográfica. En el marco del primer
subsistema, la ENVIPE proporciona información sobre las personas que se
cambian de vivienda o lugar de residencia para protegerse de la
delincuencia. En el marco del segundo, la Encuesta Nacional de la
Dinámica Demográfica (ENADID) y el Censo 2020 permiten aproximarse al
DFI.</p>
<p>Esas fuentes tienen como objetivo recopilar información sobre temas
similares, como la migración por inseguridad pública, delictiva o
violencia, pero difieren en metodología de muestreo, diseño de
cuestionario, periodo de referencia y población objetivo.</p>
<p>Los métodos de estimación multifuente abordan desafíos específicos
asociados con la comparación y reconciliación de datos de diferentes
fuentes, como la alineación de categorías de variables, el manejo de
valores atípicos y la corrección de sesgos de cobertura. Esto puede
mejorar la calidad de las estimaciones y proporcionar una base más
sólida para la toma de decisiones y políticas públicas relacionadas con
la migración por violencia. Las técnicas de integración se dividen en
dos grandes tipos: de consolidación y vinculación. Las técnicas de
consolidación consisten en crear un conjunto unificado de datos, de esta
manera se consigue un mayor número de observaciones. Las técnicas de
vinculación, en cambio, se refieren a los procedimientos mediante los
que dos o más conjuntos de datos se unen a partir de una variable de
identificación única, llave o clave, de esta manera se consiguen más
atributos de las observaciones. En ambos casos las variables deben ser
seleccionadas, transformadas y cargadas (proceso ETL, por sus siglas en
inglés). A lo largo de este proceso es necesario verificar la coherencia
y armonización de los conjuntos de datos (Naciones Unidas 2020, l.
808).</p>
<p>Es de esperarse que entre fuentes haya diferencias en las
definiciones de variables, de ahí que a partir de los valores observados
disponibles es necesario estimar los valores para las variables objetivo
de acuerdo con la definición deseada, a este paso se le denomina
alineación de mediciones. Una vez realizadas la consolidación y
vinculación, es posible implementar diversos métodos de estimación, a
nivel micro y agregado. Cuando se trata de datos de encuesta se calculan
las ponderaciones mediante regresión para todas las observaciones del
nuevo conjunto de datos obtenido a partir de la integración (Naciones
Unidas 2020). Las ponderaciones originales de cada una de las encuestas
se ajustan calibrándolas, es decir, la ponderación de regresión se
calibra sobre estos valores conocidos o estimados previamente.</p>
<br><br> <br><br>
<div style="text-align: center;">
<p>
<strong>Figura 2. Tasa de personas que cambiaron de lugar de residencia
y atribuyeron a la violencia dicho cambio por cada cien mil habitantes,
2015-2019</strong>
</p>
<p><img src="imagenes/integraci%C3%B3n_diagrama_ONU_2020.png" /></p>
</div>
<br><br>
<hr style="border: 3px solid#0077C8;" />
</div>
<div
id="estimación-a-partir-de-integración-de-enadid-2018-y-envipe-2018"
class="section level1">
<h1>3. Estimación a partir de integración de ENADID (2018) y ENVIPE
(2018)</h1>
<div id="descarga-el-proyecto-apretando-sobre-este-texto"
class="section level3">
<h3><a
href="https://365inegi-my.sharepoint.com/:f:/g/personal/daniela_serrano_inegi_org_mx/EkfBlBSyLU5Jp3PURUpAJfwBMzCnl9w3Ci2eZlAES3pCfQ?e=1m6shK"><strong>Descarga
el proyecto apretando sobre este texto</strong></a></h3>
<p><br><br></p>
<p>A partir de la integración de datos de distintas fuentes, se siguen
diversas estrategias empíricas para estimar el número de personas en
situación de DFI asociado a la delincuencia, inseguridad y violencia.
Para estimar el número de personas en situación de DFI en México, se
consolidaron y armonizaron los conjuntos de datos de la ENVIPE 2018 y la
ENADID 2018. Basándose en las definiciones del EGRISS (2020, 2023), se
crearon variables para identificar a las personas en situación de DFI.
Los ponderadores de la muestra se calibraron para corregir posibles
sesgos y asegurar la representatividad de la población objetivo. Se
empleó el método de estimación Horvitz-Thompson para estimar el número
de personas en situación de DFI a nivel nacional y por entidad
federativa. Finalmente, se evaluó la confiabilidad de los resultados a
partir de la estimación de los coeficientes de variación (CV).</p>
<p>Por defecto, las funciones como svymean y svytotal en el paquete
survey utilizan el estimador de Horvitz-Thompson, siempre que los pesos
se hayan definido correctamente en svydesign. Esto se basa en la teoría
muestral de que el peso es el inverso de la probabilidad de selección,
lo cual es el principio del estimador Horvitz-Thompson.</p>
<p>Por otra parte, se analizaron las macro determinantes de la
emigración. Para esto, se consideraron las emigraciones por razón de
inseguridad delictiva y violencia y por razones distintas a esta, tales
como las emigraciones por razones de trabajo, escuela o familia. Se
utilizaron datos agregados a nivel municipal del Censo 2020 y de las
estadísticas vitales. Se hicieron los cálculos para cada municipio y se
establecieron las variables auxiliares. Se estimaron modelos de
regresión.</p>
</div>
<div
id="calibración-del-ponderador-en-integración-de-la-envipe-2018-y-enadid-2018"
class="section level2">
<h2>3.1. Calibración del ponderador en integración de la ENVIPE 2018 y
ENADID 2018</h2>
<p>Para estimar el número de personas en situación de DFI atribuido a la
violencia se emplearon métodos de integración de datos, se calibraron
los ponderadores, de esta manera, se ajustaron los pesos muestrales para
que las estimaciones del nuevo conjunto de datos integrado reflejen
adecuadamente a la población conocida. Esta técnica se informa de las
características conocidas de la población y de muestreos de base. Las
primeras se obtuvieron del Censo 2020; para los segundos se consultaron
y siguieron los marcos conceptuales y diseños muestrales de los
programas estadísticos utilizados (INEGI 2019c; 2020; 2019a; 2018b;
2019b) La disposición de una mayor información puede contribuir a
mejorar la precisión de los estimadores, reduciendo el error muestral y
los coeficientes de variación.</p>
<div id="datos" class="section level3">
<h3>3.1.1. Datos</h3>
<p>Se usaron datos de la ENVIPE 2018 y la ENADID 2018, por sus
similitudes en periodos de levantamiento de información y diseño
muestral. Con estos datos se llevó a cabo un proceso de preparación,
transformación y creación de variables de interés para ambas
encuestas.</p>
<p>En el caso de la ENADID 2018 se unieron las tablas que componen su
base de datos, a partir de las llaves primarias. La base de datos de
esta encuesta se estructura en 4 tablas. Solo se unieron las tablas con
las variables de interés. En este caso se unieron la Tabla de hogares
(THogar) y la tabla de datos sociodemográficos de (TSDem). La primera
tabla THogar concentra información relativa a los hogares dentro de las
viviendas seleccionadas para la aplicación de la encuesta,
específicamente en esta tabla se incluyen la contabilización del número
de integrantes del hogar, de identificación, UPM, VIV_SEL, y consecutivo
del hogar; variables de Identificación Geográfica, ENT y TAM_LOC. Se
incluye un ponderador FAV_VIV y variables de estratificación. La tabla
THogar incluye dos variables de identificación única a nivel registro:
Llave de vivienda y Llave de hogar. La segunda tabla, TSDem, contiene
las variables de identificación upm, viv_sel, hogar, n_ren que sirven
para relacionar todas las tablas de la encuesta, así como variables de
identificación geográfica: la Entidad federativa (ent) y Tamaño de
localidad (tam_loc). Además, contiene variables de interés como: Sexo,
Edad, Entidad de residencia en 2017, Causa de migración reciente.</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
Tabla 2. Aproximaciones nacionales al DFI
</caption>
<thead>
<tr>
<th style="text-align:left;">
Instrumento
</th>
<th style="text-align:left;">
Años
</th>
<th style="text-align:left;">
Pregunta
</th>
<th style="text-align:left;">
Categorias.de.respuesta
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Censo Nacional de Población y Vivienda 2020 (Censo 2020)
</td>
<td style="text-align:left;">
2020
</td>
<td style="text-align:left;">
¿En qué municipio (alcaldía) vivía (NOMBRE) en marzo de 2015? ¿Por qué
(NOMBRE) dejó de vivir en (MUNICIPIO O ALCALDÍA O PAÍS)?
</td>
<td style="text-align:left;">
Por inseguridad delictiva o violencia
</td>
</tr>
<tr>
<td style="text-align:left;">
Encuesta Nacional de Victimización y Percepción sobre Seguridad Pública
(ENVIPE)
</td>
<td style="text-align:left;">
2011-2021
</td>
<td style="text-align:left;">
Durante (AÑO), para protegerse de la delincuencia, ¿en este hogar se
realizó algún tipo de medida como… cambiarse de vivienda o lugar de
residencia?
</td>
<td style="text-align:left;">
Delincuencia
</td>
</tr>
<tr>
<td style="text-align:left;">
Encuesta Nacional de la Dinámica Demográfica (ENADID)
</td>
<td style="text-align:left;">
2014 y 2018
</td>
<td style="text-align:left;">
Hace un año, en agosto de 2017, ¿en qué estado de la República Mexicana
o país vivía (NOMBRE)? ¿Por qué (NOMBRE) dejó de vivir en (ENTIDAD O
PAÍS…?
</td>
<td style="text-align:left;">
Por inseguridad pública o violencia
</td>
</tr>
<tr>
<td style="text-align:left;">
Encuesta Nacional de Ocupación y Empleo (ENOE)
</td>
<td style="text-align:left;">
2005-2021
</td>
<td style="text-align:left;">
¿En qué municipio (alcaldía) vivía … hace un año? ¿Cuál es el motivo
principal por el que se fue? …Inseguridad pública
</td>
<td style="text-align:left;">
Inseguridad pública
</td>
</tr>
<tr>
<td style="text-align:left;">
Encuesta para Caracterizar a la Población en situación de Desplazamiento
Forzado Interno en el Estado de Chihuahua (ECADEFI - CHIH)
</td>
<td style="text-align:left;">
2021
</td>
<td style="text-align:left;">
De enero de 2008 a la fecha, ¿algún integrante de esta (su) vivienda,
incluido(a) usted, tuvo que cambiar de vivienda?
</td>
<td style="text-align:left;">
Por enterarse o ser testigo de delitos en el entorno como robo, asalto,
extorsión, homicidios, etcétera
</td>
</tr>
<tr>
<td style="text-align:left;">
IDMC
</td>
<td style="text-align:left;">
2009-2023
</td>
<td style="text-align:left;">
Seguimiento de los desplazamientos por eventos para analizar y producir
estimaciones basadas en la ubicación, fecha del incidente,
desencadenantes, causas y duración.
</td>
<td style="text-align:left;">
N/A
</td>
</tr>
<tr>
<td style="text-align:left;">
Recomendaciones EGRISS
</td>
<td style="text-align:left;">
2023
</td>
<td style="text-align:left;">
P1. Mientras vivía en [país de la encuesta], ¿[usted/NOMBRE] alguna vez
tuvo que huir de su hogar? P2. ¿Cuál es la razón principal por la que
[usted/NOMBRE] tuvo que huir de casa?
</td>
<td style="text-align:left;">
Razones de seguridad, conflictos armados y violencia generalizada
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup>a</sup> Fuente: elaboración propia con datos de INEGI, EGRISS e
IDMC.
</td>
</tr>
</tfoot>
</table>
</div>
<div id="entorno-de-trabajo-y-librerías" class="section level3">
<h3>Entorno de trabajo y librerías</h3>
<p>Antes de importar los datos, cargamos las librerías necesarias para
el análisis:</p>
<p>Para realizar análisis de datos de manera eficiente, se utilizaron
los siguientes paquetes en R: dplyr es esencial para manipular y
transformar los datos de forma ágil, como seleccionar variables, filtrar
observaciones y crear nuevas columnas. foreign permite leer archivos
DBF, un formato común en bases de datos sociales. survey es fundamental
para el análisis de datos provenientes de encuestas complejas,
considerando el diseño muestral. janitor facilita la limpieza y
organización de los nombres de las columnas, asegurando una mejor
legibilidad y consistencia. Finalmente, openxlsx se empleó para exportar
los resultados del análisis a archivos de Excel, un formato ampliamente
utilizado para compartir y presentar los hallazgos.</p>
<pre class="r"><code>rm(list=ls()); graphics.off(); options(scipen = 999)
paquetes &lt;- c(
  &quot;dplyr&quot;, &quot;janitor&quot;, &quot;survey&quot;, &quot;openxlsx&quot;, &quot;tidyverse&quot;,
  &quot;data.table&quot;, &quot;readxl&quot;, &quot;writexl&quot;, &quot;PracTools&quot;, &quot;haven&quot;, 
  &quot;knitr&quot;, &quot;kableExtra&quot;
)
for (i in paquetes) {if (!require(i, character.only = TRUE)) {install.packages(i);library(i, character.only = TRUE)} else {library(i, character.only = TRUE)}}</code></pre>
</div>
<div id="cargar-y-preparar-datos-de-enadid-2018" class="section level3">
<h3>Cargar y preparar datos de ENADID 2018</h3>
<p>En esta fase describe la limpieza de los datos y su preparación para
el análisis. Está formada por subprocesos que verifican, limpian y
transforman los datos de entrada, de modo que puedan analizarse. Leemos
y preparamos los datos de ENADID. Esto incluye la limpieza de variables
y la creación de nuevas variables necesarias para el análisis.</p>
<p>Empezamos cargando los datos de la <a
href="https://www.inegi.org.mx/contenidos/programas/enadid/2018/datosabiertos/conjunto_de_datos_enadid_2018_csv.zip">ENADID
2018</a></p>
<pre class="r"><code># Cargar ENADID para integración
# Leer datos y asignarlos a objeto
enadid_thogar &lt;- read_csv(&quot;data/conjunto_de_datos_thogar_enadid_2018/conjunto_de_datos/conjunto_de_datos_thogar_enadid_2018.csv&quot;)
enadid_tsdem &lt;- read_csv(&quot;data/TSDem.csv&quot;)


# Unir conjunto de datos 
enadid &lt;- enadid_tsdem %&gt;% left_join(enadid_thogar) %&gt;% clean_names()</code></pre>
<p>Una vez que se unieron ambas tablas, se obtuvo un conjunto de datos a
nivel individual de 385,978 observaciones. A partir de este conjunto se
creó la variable emi_int_viol para identificar a las personas que
experimentaron un cambio de residencia entre la fecha del levantamiento
de la información y agosto de 2017 (un año atrás) y atribuyeron el
cambio a situaciones de inseguridad pública o violencia. Con fines de
validación, se estimó la población de 5 años y más de edad migrante
interna, por entidad federativa de residencia en agosto de 2013 según
causa de la migración. A partir del método de cálculo utilizado se llegó
al mismo resultado que el INEGI publica en los Tabulados. No se pudo
validar directamente la cifra de interés, es decir, la población de 5
años y más de edad migrante interna, por entidad federativa de
residencia en agosto de 2017 según causa de la migración ya que no es
parte de la información que se publica en los Tabulados. Para fines de
calibración, adicionalmente, se creó la variable edad_quin para
clasificar a los individuos en grupos quinquenales de edad, tomando como
base la variable continua de edad. Para poder unir esta base a la de la
ENVIPE, se renombraron las variables de manera que coincidieran.
Asimismo, se seleccionaron solo las variables iniciales de interés.</p>
<pre class="r"><code># Renombrar y crear variables 
enadid &lt;- enadid %&gt;% 
  rename(cve_ent=ent)
enadid$p3_19ac &lt;- ifelse(enadid$p3_19ac == &quot;99&quot;, NA, enadid$p3_19ac)
enadid$p3_12ac &lt;- ifelse(enadid$p3_12ac == &quot;99&quot;, NA, enadid$p3_12ac)
enadid$edad &lt;- ifelse(enadid$edad == &quot;999&quot;, NA, enadid$edad)
enadid$edad &lt;- as.numeric(enadid$edad)
enadid$misma_res &lt;- ifelse(enadid$cve_ent == enadid$p3_12ac, 1, 0) #identificar a las personas que experimentaron un cambio de residencia entre la fecha del levantamiento de la información y agosto de 2017 (un año atrás) 

# Se considera emigrante por inseguridad y violencia
enadid$emi_int_viol &lt;- ifelse(enadid$p3_13== 8 &amp; enadid$misma_res == 0, 1, 0) #Atribuyeron el cambio a situaciones de inseguridad pública o violencia 


# Conversión a tipo numérico las variables de tipo factor y homologacion de nombre con ENVIPE
enadid$fac_ele &lt;- as.numeric(as.character(enadid$fac_viv))</code></pre>
<p>Creamos una nueva variable, edad_quin, a partir de la variable edad
de la ENADID. Esta nueva variable categoriza a los individuos en
quinquenios de edad, lo cual es útil para análisis demográficos y
sociales y recalibrar.</p>
<pre class="r"><code># Crear la variable de factor edad_quin con los niveles especificados

# Crear los cortes para definir los niveles de la variable edad_quin
cortes &lt;- c(0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 130)

# Crear la variable factor edad_quin
enadid$edad_quin &lt;- cut(as.numeric(enadid$edad), breaks = cortes, labels = c(&quot;pob_00_04&quot;, &quot;pob_05_09&quot;, 
                                                                             &quot;pob_10_14&quot;, &quot;pob_15_19&quot;, &quot;pob_20_24&quot;, 
                                                                             &quot;pob_25_29&quot;, &quot;pob_30_34&quot;, &quot;pob_35_39&quot;, 
                                                                             &quot;pob_40_44&quot;, &quot;pob_45_49&quot;, &quot;pob_50_54&quot;, 
                                                                             &quot;pob_55_59&quot;, &quot;pob_60_64&quot;, &quot;pob_65_mm&quot;), include.lowest = TRUE)


enadid_int &lt;- enadid %&gt;% dplyr::select(edad, edad_quin, sexo, upm, upm_dis, est_dis, fac_ele, cve_ent, emi_int_viol)


# Conversión a tipo numérico las variables de tipo factor
enadid_int$sexo &lt;- as.factor(as.numeric(enadid_int$sexo))</code></pre>
</div>
<div id="cargar-y-preparar-datos-de-envipe-2018" class="section level3">
<h3>Cargar y preparar datos de ENVIPE 2018</h3>
<p>De manera similar, preparamos los datos de ENVIPE.</p>
<p>La base de datos de la ENVIPE 2021 se integra por 6 tablas, de las
cuales solo se necesitaron dos, Tabla de residentes del hogar (TSDem) y
Tabla de percepción sobre seguridad y desempeño institucional
(TPer_Vic1). La primera contiene las características sociodemográficas
de los integrantes del hogar y sirvió para conocer el número de
integrantes de cada hogar. La Tabla de percepción sobre seguridad y
desempeño institucional (TPer_Vic1) contiene información relacionada con
la percepción sobre seguridad pública en el ámbito geográfico del
informante, las conductas antisociales en su entorno inmediato y los
cambios de hábitos por temor a ser víctima de la delincuencia en 2017. A
partir de esta Tabla se identificaron los hogares que habían cambiado de
vivienda o residencia por temor a la delincuencia entre 2017 y la fecha
del levantamiento de la ENVIPE. Al igual que se hizo con la ENADID 2018,
se creó la variable emi_int_viol. En este caso, la variable identifica a
las personas que declararon haberse cambiado de vivienda o residencia
como medida de protección ante la delincuencia, dada a partir de la
variable original AP4_11_10. La variable emi_int_viol tomó el valor de 1
cuando AP4_11_10 tomó el valor de “1”, y 0 cuando AP4_11_10 tomó el
valor de “0”. Una vez que se obtuvieron las variables necesarias, ambas
tablas se unieron mediante el identificador de hogar (ID_HOG) y se
seleccionaron solo las 9 variables de interés.</p>
<p>Se obtuvo un conjunto de datos a nivel individual de 91,541
observaciones.</p>
<p>La ENVIPE 2021 cuenta con ocho factores de expansión, FAC_VIV,
FAC_HOG, FAC_ELE, FAC_DEL, FAC_VIV_AM, FAC_HOG_AM , FAC_ELE_AM y
FAC_DEL_AM. El primero está asociado a la vivienda (FAC_VIV), el segundo
corresponde a los hogares (FAC_HOG); el tercero está relacionado con las
personas de 18 años y más (FAC_ELE) y el cuarto expande cada delito
captado en el módulo sobre victimización (FAC_DEL). Además, debido al
interés para obtener estimaciones sobre determinados indicadores de la
ENVIPE 2021 en ciudades o áreas metropolitanas, se han añadido el factor
vivienda de área metropolitana (FAC_VIV_AM), el factor hogar de área
metropolitana (FAC_HOG_AM), el factor de personas elegidas en el área
metropolitana (FAC_ELE_AM) y el factor delito del área metropolitana
(FAC_DEL_AM). Estos cuatro factores son análogos a los asociados a los
hogares, pero ajustados a las 33 áreas metropolitanas que se detallan en
la tabla posterior (INEGI 2018a).</p>
<p>El factor vivienda (FAC_VIV) es el ponderador que se utiliza para
estimar resultados de las preguntas que se refieren a las viviendas y la
población en general. El factor hogar (FAC_HOG) es el ponderador que se
utiliza para estimar resultados de las preguntas que se refieren a los
hogares y la población en general. El factor de personas elegidas
(FAC_ELE) es el ponderador para estimar resultados de las preguntas de
percepción de la seguridad pública y la victimización de la población de
18 años y más. El factor delito es el ponderador que se utiliza para
estimar resultados de los delitos registrados en el “Módulo de delitos”.
Los factores con terminación _AM están ajustados para las 33 áreas
metropolitanas (INEGI, 2018a).</p>
<p>De acuerdo con el Marco conceptual de la ENVIPE 2018, la prevención y
protección ante el delito es uno de los componentes y aspectos
conceptuales en las encuestas de victimización. En este componente se
incluyen preguntas sobre el tipo de precauciones para protegerse o
prevenir el delito. En la ENVIPE 2018 se consideró que el cambio de
rutinas es un importante indicador de inseguridad porque es una
manifestación real del temor a la delincuencia que incide en la vida de
las personas. Una de las medidas de protección que considera es el
cambio de vivienda, desde esta perspectiva, el cambio de vivienda sería
un proxy de temor al delito, es decir, una medida indirecta de la
variable de interés, delincuencia. Las variables proxy son comúnmente
utilizadas en estudios empíricos cuando la variable de interés no puede
ser observada directamente o cuando los datos disponibles son limitados.
Sin embargo, es importante tener en cuenta que una variable proxy no
siempre refleja con precisión la variable subyacente que se intenta
medir, y su uso conlleva limitaciones y riesgos de sesgos.</p>
<p>Considerando lo anterior y como primera aproximación, se buscó
determinar el número de hogares que, durante el año 2017, tomaron
medidas para protegerse de la delincuencia, como mudarse de vivienda o
residencia. La estimación inicial, basada en la pregunta de la encuesta
“¿Durante 2017, para protegerse de la delincuencia, ¿en este hogar se
realizó algún tipo de medida como cambiarse de vivienda o lugar de
residencia?”, arrojó un total de 315,330 hogares.</p>
<p>Al tratarse de una pregunta sobre la percepción de la seguridad
pública, surge la posibilidad de utilizar el factor de expansión de
personas elegidas (FAC_ELE) para realizar estimaciones poblacionales.
Este método, sin embargo, no coincide con los resultados publicados en
los tabulados oficiales. El estimador Horvitz-Thompson, basado en el
FAC_ELE, estima un total de 715 mil 971 personas que tomaron medidas
para protegerse de la delincuencia, mientras que los tabulados oficiales
reportan un total de un millón 133 mil 041 personas. Asimismo se buscó
replicar el método de cálculo del INEGI. Para conseguirlo, el factor de
expansión que corresponde a los hogares se multiplicó por el número de
integrantes del hogar y así se obtuvo un nuevo factor base de expansión.
Este método supone que todas las personas integrantes del hogar
cambiaron de vivienda o lugar de residencia por la misma razón y más o
menos en las mismas condiciones. Sin embargo, el EGRISS (2023) sugiere
que no siempre todos los integrantes de un hogar son personas en
situación de DFI, sino que las personas en dicha situación “pueden vivir
en campamentos exclusivos o en casas entre la población general, ya sea
como parte de otro hogar o como un hogar independiente” (EGRISS 2023c,
16). Adicionalmente, la información sobre actitudes, intenciones de
mudarse y poder de toma de decisiones en el hogar puede variar según el
género de la persona informante. Para garantizar datos más confiables es
recomendable utilizar un protocolo de selección aleatoria de mujeres y
hombres encuestados dentro de un hogar muestreado, o entrevistar a más
de un miembro de cada hogar (EGRISS 2023c, 16).</p>
<p>Por lo anterior, se usaron dos factores de expansión base para la
ENVIPE, FAC_ELE y FAC_HOGAR multiplicado por el número de integrantes
del hogar. A partir de estos pesos base de la ENVIPE 2018 y de los pesos
base de la ENADID 2018 se realizó la calibración. Para lograrlo, ambos
conjuntos de datos se integraron en uno solo, de manera vertical. Se
obtuvo un solo conjunto con 477 mil 519 observaciones y nueve
variables.</p>
<pre class="r"><code># Preparar ENVIPE
# Tablas que se usarán para los cálculos de Prevalencia Delictiva, Incidencia Delictiva y Cifra Negra
tsd &lt;- read.dbf(&quot;data/TSDem.dbf&quot;) # Tabla del Sociodemográfico
tpv1&lt;- read.dbf(&quot;data/TPer_Vic1.dbf&quot;) %&gt;% 
  dplyr::select(&quot;ID_VIV&quot;, &quot;ID_HOG&quot;, &quot;ID_PER&quot;, &quot;UPM&quot;, &quot;VIV_SEL&quot;, &quot;HOGAR&quot;, &quot;RESUL_H&quot;, &quot;R_SEL&quot;, &quot;SEXO&quot;, &quot;EDAD&quot;, &quot;AREAM&quot;, &quot;CVE_ENT&quot;, &quot;NOM_ENT&quot;, &quot;CVE_MUN&quot;, &quot;FAC_ELE&quot;, &quot;FAC_HOG&quot;, 
                &quot;DOMINIO&quot;, &quot;ESTRATO&quot;, &quot;EST_DIS&quot;, &quot;UPM_DIS&quot;, &quot;CVE_ENT&quot;, &quot;AP4_11_10&quot;) # Tabla Principal de Victimización 1</code></pre>
<pre class="r"><code># Conversión a tipo numérico las variables de tipo factor
tpv1$FAC_ELE &lt;- as.numeric(as.character(tpv1$FAC_ELE))
tpv1$FAC_HOG &lt;- as.numeric(as.character(tpv1$FAC_HOG))
tsd$FAC_HOG &lt;- as.numeric(as.character(tsd$FAC_HOG))


# Construcción de la variable de Entidad, se substrae los dos primeros dígitos de la variable UPM
tpv1$ENT &lt;- substr(tpv1$UPM,1,2)</code></pre>
<pre class="r"><code>#Crear llaves

# Unir bases
# Pegar 
int_hog &lt;- tsd %&gt;% 
  group_by(ID_HOG) %&gt;% 
  summarise(integrantes=n())

envipe &lt;- tpv1 %&gt;% 
  left_join(int_hog) %&gt;% 
  dplyr::select(-ends_with(&quot;.y&quot;)) %&gt;% 
  clean_names()</code></pre>
<p>Al igual que ocn la ENADID, creamos una nueva variable, edad_quin, a
partir de la variable edad de la ENADID. Esta nueva variable categoriza
a los individuos en quinquenios de edad, lo cual es útil para análisis
demográficos y sociales y recalibrar.</p>
<pre class="r"><code># Construcción de las variables
# envipe$emi_int_viol &lt;- ifelse(envipe$ap4_11_10%in%&quot;1&quot;,1,0)
envipe$emi_int_viol &lt;- case_when(envipe$ap4_11_10==&quot;1&quot;~1, TRUE ~ 0)

# Crear los cortes para definir los niveles de la variable edad_quin
cortes &lt;- c(0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 130)

# Crear la variable factor edad_quin
envipe$edad_quin &lt;- cut(as.numeric(envipe$edad), breaks = cortes, labels = c(&quot;pob_00_04&quot;, &quot;pob_05_09&quot;, 
                                                                             &quot;pob_10_14&quot;, &quot;pob_15_19&quot;, &quot;pob_20_24&quot;, 
                                                                             &quot;pob_25_29&quot;, &quot;pob_30_34&quot;, &quot;pob_35_39&quot;, 
                                                                             &quot;pob_40_44&quot;, &quot;pob_45_49&quot;, &quot;pob_50_54&quot;, 
                                                                             &quot;pob_55_59&quot;, &quot;pob_60_64&quot;, &quot;pob_65_mm&quot;), include.lowest = TRUE)

# Se genera un factor a partir del factor de expansion de hogar multiplicado por el número de habitantes promedio en 2017/ TAMAÑO DEL HOGAR (VER https://www.inegi.org.mx/contenidos/programas/enh/2017/doc/enh2017_resultados.pdf)
envipe &lt;- envipe %&gt;% 
  mutate(fac_ele=as.numeric(fac_hog)*integrantes)
# Seleccionar variables para integrar

envipe_int &lt;- envipe %&gt;% dplyr::select(edad, edad_quin, edad, sexo, upm, upm_dis, est_dis, fac_ele, cve_ent, emi_int_viol)

# Conversión a tipo numérico las variables de tipo factor
envipe_int$upm &lt;- as.character(as.factor(envipe_int$upm))
envipe_int$upm_dis &lt;- as.character(as.factor(envipe_int$upm_dis))
envipe_int$est_dis &lt;- as.character(as.factor(envipe_int$est_dis))
envipe_int$cve_ent &lt;- as.character(as.factor(envipe_int$cve_ent))</code></pre>
<p>Unimos las bases de datos de ENADID y ENVIPE para la estimación
conjunta.</p>
<pre class="r"><code>## Unir bases
integrar &lt;- rbind(enadid_int, envipe_int) </code></pre>
</div>
<div id="raking" class="section level3">
<h3>Raking</h3>
<p>Un método comúnmente utilizado de algoritmos de calibración es el
raking (también conocido como ajuste proporcional iterativo) (Deming y
Stephan 1940). Este método emplea el algoritmo de Newton- Raphson
(Deville y Sarndal 1992; Deville, Sarndal, y Sautory 1993). De acuerdo
con Deville, Sarndal, y Sautory (1993), este algoritmo ofrece una
solución general, eficiente y distinta de la versión clásica W. E.
Deming (ajuste proporcional iterativo). La diferencia, consiste en que
el primero es una generalización del segundo, busca la convergencia y
optimización mediante la minimización de la función de distancia entre
los pesos base u originales y los nuevos pesos. Para calcular los nuevos
pesos, se resuelven las ecuaciones de calibración.</p>
<p>El muestreo estratificado junto al ajuste proporcional iterativo, en
particular, preserva las ventajas de la estratificación (que puede
basarse en estratos geográficos) y de los recuentos de grupos de edad
conocidos (Deville, Sarndal, y Sautory 1993, 1015).</p>
<p>El Instituto Nacional de Estadísticas y Estudios Económicos (INSEE
por sus siglas en francés), agencia nacional de estadística francesa,
desde 1940 emplea los métodos raking de calibración para producir
ponderaciones para encuestas de hogares. Para esto desarrollaron el
programa CALMAR de SAS.</p>
<p>En este trabajo, las ponderaciones se crearon con la función raking
del paquete survey de R (Lumley 2023), utilizando las distribuciones
marginales de las variables de ajuste derivadas de datos de población de
CONAPO, se usaron las categorías de los grupos quinquenales de edad y
sexo y los pesos (factores de expansión) originales. De acuerdo con
INEGI (2019b) los pesos son el inverso de la probabilidad de selección.
Sin embargo, al usar dos factores de expansión base para la ENVIPE 2018,
se hicieron dos recalibraciones y, por tanto, se obtuvieron dos
resultados. Con los nuevos pesos obtenidos mediante la recalibración, se
obtuvieron los estimadores Horvitz-Thompson del número de personas en
situación de DFI en el país y para cada entidad federativa, junto con
sus varianzas, errores y coeficientes de variación. A partir de estos
últimos se evaluó la calidad de los resultados.</p>
<p>El INEGI categoriza la confiabilidad de las estimaciones con base en
los coeficientes de variación asociados (INEGI 2021). Si el coeficiente
de variación está entre 0% y 15%, los datos se considera que tiene un
alto grado de fiabilidad; si el coeficiente de variación es superior o
igual al 15 % e inferior al 30 %, se considera que los datos tienen un
grado tolerable de fiabilidad; y si el coeficiente de variación es mayor
o igual al 30%, los datos deben ser recibidos con ciertas reservas
debido a su baja confiabilidad. De esta forma, la estimación de
emigrantes que atribuyen a la inseguridad pública y violencia el cambio
de municipio de residencia entre marzo de 2015 y agostos de 2020 en
México tuvo un alto nivel de confiabilidad. Respecto a las estimaciones
a nivel entidad federativa, 27 entidades tuvieron estimaciones con un
grado de confiabilidad tolerable y 5 con un grado de confiabilidad
bajo.</p>
<p>Para ajustar los ponderadores base y obtener los ponderadores finales
que reflejen adecuadamente el universo de estudio, calibramos los
ponderadores base. Este proceso implica ajustar los ponderadores base,
es decir, los calculados a partir de los diseños muestrales de las
encuestas. Para lograrlo, se usa información adicional sobre la
población total.</p>
<pre class="r"><code># 6. Calibración del ponderador
# Leer y unir bases de población
poblacion_municipal &lt;- bind_rows(
  read_csv(&quot;data/data_t5_conapo/base_municipios_final_datos_01.csv&quot;, locale = locale(encoding = &quot;latin1&quot;)),
  read_csv(&quot;data/data_t5_conapo/base_municipios_final_datos_02.csv&quot;, locale = locale(encoding = &quot;latin1&quot;))
) %&gt;% clean_names()

# Arreglar claves de los municipios
poblacion_municipal &lt;- poblacion_municipal %&gt;% 
  mutate(entidad = ifelse(nchar(clave_ent) == 1,
                          paste0(&quot;0&quot;, clave_ent), clave_ent),
         clave_2 = ifelse(nchar(clave) == 4,
                          paste0(&quot;0&quot;, clave), clave))


# Filtrar por año y grupo de edad
poblacion_municipal$year &lt;- poblacion_municipal$ano
pob_por_gpos_edad &lt;- poblacion_municipal %&gt;%
  filter(year == 2017)

# Calcular sumas por grupos
grupos &lt;- pob_por_gpos_edad %&gt;%
  group_by(edad_quin) %&gt;%
  summarise(poblacion = sum(pob))

sexo &lt;- pob_por_gpos_edad %&gt;%
  group_by(sexo) %&gt;%
  summarise(poblacion = sum(pob))

cve_ent &lt;- pob_por_gpos_edad %&gt;%
  group_by(entidad) %&gt;%
  summarise(poblacion = sum(pob))

total &lt;- pob_por_gpos_edad %&gt;%
  summarise(poblacion = sum(pob))
edad_quin &lt;- grupos[[2]]
sexo &lt;- sexo[[2]]
cve_ent &lt;- cve_ent[[2]]
N &lt;-  total[[1]]</code></pre>
<p>Los pasos que se siguen son:</p>
<ol style="list-style-type: decimal">
<li><p>Identificar variables auxiliares para la calibración: seleccionar
variables clave de la población total, como distribuciones por edad,
género, ubicación geográfica…</p></li>
<li><p>Calcular totales de población para variables de calibración:
estos totales se obtienen del Censo 2020, también se pueden obtener de
otras encuestas.</p></li>
</ol>
<p>Usamos <em>calibrate</em>: Esta función ajusta los pesos del diseño
muestral para que coincidan con las distribuciones conocidas de ciertas
variables en la población total, mejorando así la representatividad de
la muestra.</p>
<p>formula = ~as.factor(edad_quin) + as.factor(sexo): Especifica las
variables que se utilizarán para calibrar los pesos. Aquí, edad_quin y
sexo son las variables categóricas seleccionadas para la
calibración.</p>
<p>calfun = “raking”: Especifica el método de calibración. El método
“raking” ajusta iterativamente los pesos para que las margenes de las
variables en la muestra coincidan con las margenes conocidas en la
población.</p>
<p>population = c(“(Intercept)” = N, edad_quin[-1], sexo[-1]): Define
las distribuciones conocidas de las variables en la población total que
se utilizan para la calibración. Aquí, N representa el tamaño total de
la población, y edad_quin[-1] y sexo[-1] representan las distribuciones
de las categorías de edad y sexo (excepto la primera categoría, que se
incluye implícitamente).</p>
<p>Finalmente actualizamos los pesos muestrales.</p>
<p>weights(calibracion): Extrae los pesos calibrados del objeto de
calibración.</p>
<p>integrar$fac_nuevo: Almacena los nuevos pesos calibrados en una nueva
variable fac_nuevo dentro del conjunto de datos integrar.</p>
<p>Este paso finaliza el proceso de calibración al actualizar los pesos
muestrales en el conjunto de datos, lo que permite realizar análisis
posteriores utilizando estos pesos ajustados.</p>
<p>La calibración es esencial para corregir posibles sesgos en la
muestra, asegurando que las estimaciones sean representativas de la
población total. Esto es especialmente relevante cuando se sabe que
ciertas variables están sub o sobre-representadas en la muestra.</p>
<pre class="r"><code># Paso 2: Crear el diseño muestral
diseño &lt;- svydesign(
  ids = ~upm,
  strata = ~est_dis,
  weights = ~fac_ele,
  data = integrar,
  nest = TRUE
)
#options(survey.lonely.psu = &quot;adjust&quot;)

calibracion &lt;- calibrate(design = diseño,
                         formula = ~as.factor(edad_quin) + as.factor(sexo),
                         calfun = &quot;raking&quot;,
                         population = c(&quot;(Intercept)&quot; = N, edad_quin[-1], sexo[-1]))


integrar$fac_nuevo=weights(calibracion)</code></pre>
<p>Con los nuevos pesos, configuramos el nuevo diseño muestral para
estimar el número de PDFI en México.</p>
<pre class="r"><code># Paso 3: Crear el NUEVO diseño muestral
diseño2 &lt;- svydesign(
  ids = ~upm,
  strata = ~est_dis,
  weights = ~fac_nuevo,
  data = integrar,
  nest = TRUE
)
#options(survey.lonely.psu = &quot;adjust&quot;)</code></pre>
<p>Y procedemos a obtener las estimaciones a nivel nacional y
estatal.</p>
<pre class="r"><code># Cálculo del número de personas DFI por violencia
# Nacional
n_dfi_viol &lt;- svytotal(~emi_int_viol, diseño2, na.rm = TRUE) 
# Entidad federativa
e_dfi_viol &lt;- svyby(~emi_int_viol, by=~cve_ent, diseño2, 
                    svytotal, na.rm = TRUE) 

# Estimaciones, Error estándar, Coeficiente de variación, Intervalos de confianza
# Nacional
est_n_dfi_viol &lt;- n_dfi_viol[[1]]
se_n_dfi_viol&lt;- SE(n_dfi_viol)
cv_n_dfi_viol &lt;- cv(n_dfi_viol)*100
li_n_dfi_viol &lt;- confint(n_dfi_viol,level=0.90)[1,1]
ls_n_dfi_viol &lt;- confint(n_dfi_viol,level=0.90)[1,2]
# Entidad federativa
est_e_dfi_viol&lt;- e_dfi_viol[[2]]
se_e_dfi_viol &lt;- SE(e_dfi_viol)
cv_e_dfi_viol &lt;- cv(e_dfi_viol)*100
li_e_dfi_viol &lt;- confint(e_dfi_viol,level=0.90)[,1]
ls_e_dfi_viol &lt;- confint(e_dfi_viol,level=0.90)[,2]


# Formato #
Entidades&lt;-c(&quot;Estados Unidos Mexicanos&quot;, &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja California Sur&quot;, 
             &quot;Campeche&quot;, &quot;Coahuila de Zaragoza&quot;, &quot;Colima&quot;, &quot;Chiapas&quot;, &quot;Chihuahua&quot;, &quot;Ciudad de México&quot;, 
             &quot;Durango&quot;, &quot;Guanajuato&quot;, &quot;Guerrero&quot;, &quot;Hidalgo&quot;, &quot;Jalisco&quot;, &quot;Estado de México&quot;, 
             &quot;Michoacán de Ocampo&quot;, &quot;Morelos&quot;, &quot;Nayarit&quot;, &quot;Nuevo León&quot;, &quot;Oaxaca&quot;, &quot;Puebla&quot;, &quot;Querétaro&quot;, 
             &quot;Quintana Roo&quot;, &quot;San Luis Potosí&quot;, &quot;Sinaloa&quot;, &quot;Sonora&quot;, &quot;Tabasco&quot;, &quot;Tamaulipas&quot;, &quot;Tlaxcala&quot;, 
             &quot;Veracruz de Ignacio de la Llave&quot;, &quot;Yucatán&quot;, &quot;Zacatecas&quot;) 


est_dfi_viol &lt;- data.frame(Entidades, est_dfi_viol = round(as.numeric(c(est_n_dfi_viol, est_e_dfi_viol)), 0))
se_dfi_viol &lt;- data.frame(Entidades, se_dfi_viol = round(as.numeric(c(se_n_dfi_viol, se_e_dfi_viol)), 0))
cv_dfi_viol &lt;- data.frame(Entidades, cv_dfi_viol = round(as.numeric(c(cv_n_dfi_viol, cv_e_dfi_viol)), 2))
lim_dfi_viol &lt;- data.frame(Entidades, 
                           linf_dfi_viol = as.integer(c(li_n_dfi_viol, li_e_dfi_viol)),
                           lsup_dfi_viol = as.integer(c(ls_n_dfi_viol, ls_e_dfi_viol)))

# Elimina los nombres de fila
row.names(est_dfi_viol) &lt;- row.names(se_dfi_viol) &lt;- row.names(cv_dfi_viol) &lt;- 
  row.names(lim_dfi_viol) &lt;- NULL

# Exportar salida a un archivo de Excel #
# Lista de estimaciones
list_of_datasets &lt;- list(&quot;Estimaciones&quot; = est_dfi_viol, 
                         &quot;Error Estandar&quot; = se_dfi_viol, 
                         &quot;Coef Variacion&quot; = cv_dfi_viol, 
                         &quot;Int Confianza&quot; = lim_dfi_viol)

write.xlsx(list_of_datasets, 
           file = paste0(&quot;DFI_viol_envipe_enadid_FAC_HOG_&quot;, format(Sys.Date(), &quot;%y%m%d&quot;), &quot;.xlsx&quot;))</code></pre>
<p>Una alternativa al método anterior de cálculo es usar solo el factor
de expansión de la persona entrevistada <em>FAC_ELE</em>, en ambas
encuestas. En lugar de usar FAC_HOG y multiplicarlo por los integrantes
del hogar, como se hace con la ENVIPE. Es importante discutir e indagar
más acerca de la probabilidad de que en efecto los desplazamientos
forzados en su mayoría ocurran en familia. La revisión de la literatura
no arrojó información ni se encontró recomenDación que aplique a este
caso.</p>
<p><br><br></p>
<pre class="r"><code>## B. MÉTODO DE CÁLCULO USANDO FAC_ELE == fac_ele

# rm(list=ls()); graphics.off(); options(scipen = 999)
# paquetes=c(&quot;tidyverse&quot;, &quot;date&quot;, &quot;data.table&quot;, &quot;questionr&quot;, &quot;readxl&quot;, &quot;writexl&quot;, &quot;openxlsx&quot;, &quot;foreign&quot;, &quot;survey&quot;, &quot;PracTools&quot;, &quot;haven&quot;,&quot;knitr&quot;,&quot;kableExtra&quot;)
# for (i in paquetes) {if (!require(i, character.only = TRUE)) {install.packages(i);library(i, character.only = TRUE)} else {library(i, character.only = TRUE)}}

# Cargar ENADID para integración
# Leer datos y asignarlos a objeto
enadid_thogar &lt;- read_csv(&quot;data/conjunto_de_datos_thogar_enadid_2018/conjunto_de_datos/conjunto_de_datos_thogar_enadid_2018.csv&quot;)
enadid_tsdem &lt;- read_csv(&quot;data/TSDem.csv&quot;)


# Unir conjunto de datos 
enadid &lt;- enadid_tsdem %&gt;% left_join(enadid_thogar) %&gt;% clean_names()

# Renombrar y crear variables 
enadid &lt;- enadid %&gt;% 
  rename(cve_ent=ent)
enadid$p3_19ac &lt;- ifelse(enadid$p3_19ac == &quot;99&quot;, NA, enadid$p3_19ac)
enadid$p3_12ac &lt;- ifelse(enadid$p3_12ac == &quot;99&quot;, NA, enadid$p3_12ac)
enadid$edad &lt;- ifelse(enadid$edad == &quot;999&quot;, NA, enadid$edad)
enadid$edad &lt;- as.numeric(enadid$edad)
enadid$misma_res_5A &lt;- ifelse(enadid$cve_ent == enadid$p3_19ac, 1, 0)
enadid$misma_res &lt;- ifelse(enadid$cve_ent == enadid$p3_12ac, 1, 0)

# Se considera emigrante por inseguridad y violencia
enadid$emi_int_viol_5A &lt;- ifelse(enadid$p3_20 == 8 &amp; enadid$misma_res_5A == 0, 1, 0)
enadid$emi_int_viol &lt;- ifelse(enadid$p3_13== 8 &amp; enadid$misma_res == 0, 1, 0)


# Conversión a tipo numérico las variables de tipo factor y homologacion de nombre con ENVIPE
enadid$fac_ele &lt;- as.numeric(as.character(enadid$fac_viv))

# Crear la variable de factor edad_quin con los niveles especificados

# Crear los cortes para definir los niveles de la variable edad_quin
cortes &lt;- c(0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 130)

# Crear la variable factor edad_quin
enadid$edad_quin &lt;- cut(as.numeric(enadid$edad), breaks = cortes, labels = c(&quot;pob_00_04&quot;, &quot;pob_05_09&quot;, 
                                                                             &quot;pob_10_14&quot;, &quot;pob_15_19&quot;, &quot;pob_20_24&quot;, 
                                                                             &quot;pob_25_29&quot;, &quot;pob_30_34&quot;, &quot;pob_35_39&quot;, 
                                                                             &quot;pob_40_44&quot;, &quot;pob_45_49&quot;, &quot;pob_50_54&quot;, 
                                                                             &quot;pob_55_59&quot;, &quot;pob_60_64&quot;, &quot;pob_65_mm&quot;), include.lowest = TRUE)


enadid_int &lt;- enadid %&gt;% dplyr::select(edad, edad_quin, sexo, upm, upm_dis, est_dis, fac_ele, cve_ent, emi_int_viol)


# Conversión a tipo numérico las variables de tipo factor
enadid_int$sexo &lt;- as.factor(as.numeric(enadid_int$sexo))
# Preparar ENVIPE
# Tablas que se usarán para los cálculos de Prevalencia Delictiva, Incidencia Delictiva y Cifra Negra
envipe &lt;- read.dbf(&quot;data/TPer_Vic1.dbf&quot;) %&gt;%
  dplyr::select(&quot;ID_VIV&quot;, &quot;ID_HOG&quot;, &quot;ID_PER&quot;, &quot;UPM&quot;, &quot;VIV_SEL&quot;, &quot;HOGAR&quot;, &quot;RESUL_H&quot;, &quot;R_SEL&quot;, &quot;SEXO&quot;, &quot;EDAD&quot;, &quot;AREAM&quot;, &quot;CVE_ENT&quot;, &quot;NOM_ENT&quot;, &quot;CVE_MUN&quot;, &quot;FAC_ELE&quot;, &quot;FAC_HOG&quot;,
                &quot;DOMINIO&quot;, &quot;ESTRATO&quot;, &quot;EST_DIS&quot;, &quot;UPM_DIS&quot;, &quot;CVE_ENT&quot;, &quot;AP4_11_10&quot;) # Tabla Principal de Victimización 1

# Conversión a tipo numérico las variables de tipo factor
envipe$FAC_ELE &lt;- as.numeric(as.character(envipe$FAC_ELE))


# Construcción de la variable de Entidad, se substrae los dos primeros dígitos de la variable UPM
envipe$ENT &lt;- substr(envipe$UPM,1,2)
envipe &lt;- envipe %&gt;% clean_names()

#Crear llaves

# Construcción de las variables
# envipe$emi_int_viol &lt;- ifelse(envipe$ap4_11_10%in%&quot;1&quot;,1,0)
envipe$emi_int_viol &lt;- case_when(envipe$ap4_11_10==&quot;1&quot;~1, TRUE ~ 0)

# Crear los cortes para definir los niveles de la variable edad_quin
cortes &lt;- c(0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 130)

# Crear la variable factor edad_quin
envipe$edad_quin &lt;- cut(as.numeric(envipe$edad), breaks = cortes, labels = c(&quot;pob_00_04&quot;, &quot;pob_05_09&quot;,
                                                                             &quot;pob_10_14&quot;, &quot;pob_15_19&quot;, &quot;pob_20_24&quot;,
                                                                             &quot;pob_25_29&quot;, &quot;pob_30_34&quot;, &quot;pob_35_39&quot;,
                                                                             &quot;pob_40_44&quot;, &quot;pob_45_49&quot;, &quot;pob_50_54&quot;,
                                                                             &quot;pob_55_59&quot;, &quot;pob_60_64&quot;, &quot;pob_65_mm&quot;), include.lowest = TRUE)

# Seleccionar variables para integrar
envipe_int &lt;- envipe %&gt;% dplyr::select(edad, edad_quin, edad, sexo, upm, upm_dis, est_dis, fac_ele, cve_ent, emi_int_viol)

# Conversión a tipo numérico las variables de tipo factor
envipe_int$upm &lt;- as.character(as.factor(envipe_int$upm))
envipe_int$upm_dis &lt;- as.character(as.factor(envipe_int$upm_dis))
envipe_int$est_dis &lt;- as.character(as.factor(envipe_int$est_dis))
envipe_int$cve_ent &lt;- as.character(as.factor(envipe_int$cve_ent))

## Unir bases
integrar &lt;- rbind(enadid_int, envipe_int)

# 6. Calibración del ponderador
# Leer y unir bases de población
poblacion_municipal &lt;- bind_rows(
  read_csv(&quot;data/data_t5_conapo/base_municipios_final_datos_01.csv&quot;, locale = locale(encoding = &quot;latin1&quot;)),
  read_csv(&quot;data/data_t5_conapo/base_municipios_final_datos_02.csv&quot;, locale = locale(encoding = &quot;latin1&quot;))
) %&gt;% clean_names()

# Arreglar claves de los municipios
poblacion_municipal &lt;- poblacion_municipal %&gt;%
  mutate(entidad = ifelse(nchar(clave_ent) == 1,
                          paste0(&quot;0&quot;, clave_ent), clave_ent),
         clave_2 = ifelse(nchar(clave) == 4,
                          paste0(&quot;0&quot;, clave), clave))


# Filtrar por año y grupo de edad
poblacion_municipal$year &lt;- poblacion_municipal$ano
pob_por_gpos_edad &lt;- poblacion_municipal %&gt;%
  filter(year == 2017)

# Calcular sumas por grupos
grupos &lt;- pob_por_gpos_edad %&gt;%
  group_by(edad_quin) %&gt;%
  summarise(poblacion = sum(pob))

sexo &lt;- pob_por_gpos_edad %&gt;%
  group_by(sexo) %&gt;%
  summarise(poblacion = sum(pob))

cve_ent &lt;- pob_por_gpos_edad %&gt;%
  group_by(entidad) %&gt;%
  summarise(poblacion = sum(pob))

total &lt;- pob_por_gpos_edad %&gt;%
  summarise(poblacion = sum(pob))


edad_quin &lt;- grupos[[2]]
sexo &lt;- sexo[[2]]
cve_ent &lt;- cve_ent[[2]]
N &lt;-  total[[1]]

# Paso 2: Crear el diseño muestral
diseño &lt;- svydesign(
  ids = ~upm,
  strata = ~est_dis,
  weights = ~fac_ele,
  data = integrar,
  nest = TRUE
)
#options(survey.lonely.psu = &quot;adjust&quot;)

calibracion &lt;- calibrate(design = diseño,
                         formula = ~as.factor(edad_quin) + as.factor(sexo),
                         calfun = &quot;raking&quot;,
                         population = c(&quot;(Intercept)&quot; = N, edad_quin[-1], sexo[-1]))


integrar$fac_nuevo=weights(calibracion)

# Paso 3: Crear el NUEVO diseño muestral
diseño2 &lt;- svydesign(
  ids = ~upm,
  strata = ~est_dis,
  weights = ~fac_nuevo,
  data = integrar,
  nest = TRUE
)
#options(survey.lonely.psu = &quot;adjust&quot;)

# Cálculo del número de personas DFI por violencia
# Nacional
n_dfi_viol &lt;- svytotal(~emi_int_viol, diseño2, na.rm = TRUE) 
# Entidad federativa
e_dfi_viol &lt;- svyby(~emi_int_viol, by=~cve_ent, diseño2, 
                    svytotal, na.rm = TRUE) 

# Estimaciones, Error estándar, Coeficiente de variación, Intervalos de confianza
# Nacional
est_n_dfi_viol &lt;- n_dfi_viol[[1]]
se_n_dfi_viol&lt;- SE(n_dfi_viol)
cv_n_dfi_viol &lt;- cv(n_dfi_viol)*100
li_n_dfi_viol &lt;- confint(n_dfi_viol,level=0.90)[1,1]
ls_n_dfi_viol &lt;- confint(n_dfi_viol,level=0.90)[1,2]
# Entidad federativa
est_e_dfi_viol&lt;- e_dfi_viol[[2]]
se_e_dfi_viol &lt;- SE(e_dfi_viol)
cv_e_dfi_viol &lt;- cv(e_dfi_viol)*100
li_e_dfi_viol &lt;- confint(e_dfi_viol,level=0.90)[,1]
ls_e_dfi_viol &lt;- confint(e_dfi_viol,level=0.90)[,2]


# Formato #
Entidades&lt;-c(&quot;Estados Unidos Mexicanos&quot;, &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja California Sur&quot;, 
             &quot;Campeche&quot;, &quot;Coahuila de Zaragoza&quot;, &quot;Colima&quot;, &quot;Chiapas&quot;, &quot;Chihuahua&quot;, &quot;Ciudad de México&quot;, 
             &quot;Durango&quot;, &quot;Guanajuato&quot;, &quot;Guerrero&quot;, &quot;Hidalgo&quot;, &quot;Jalisco&quot;, &quot;Estado de México&quot;, 
             &quot;Michoacán de Ocampo&quot;, &quot;Morelos&quot;, &quot;Nayarit&quot;, &quot;Nuevo León&quot;, &quot;Oaxaca&quot;, &quot;Puebla&quot;, &quot;Querétaro&quot;, 
             &quot;Quintana Roo&quot;, &quot;San Luis Potosí&quot;, &quot;Sinaloa&quot;, &quot;Sonora&quot;, &quot;Tabasco&quot;, &quot;Tamaulipas&quot;, &quot;Tlaxcala&quot;, 
             &quot;Veracruz de Ignacio de la Llave&quot;, &quot;Yucatán&quot;, &quot;Zacatecas&quot;) 


est_dfi_viol &lt;- data.frame(Entidades, est_dfi_viol = round(as.numeric(c(est_n_dfi_viol, est_e_dfi_viol)), 0))
se_dfi_viol &lt;- data.frame(Entidades, se_dfi_viol = round(as.numeric(c(se_n_dfi_viol, se_e_dfi_viol)), 0))
cv_dfi_viol &lt;- data.frame(Entidades, cv_dfi_viol = round(as.numeric(c(cv_n_dfi_viol, cv_e_dfi_viol)), 2))
lim_dfi_viol &lt;- data.frame(Entidades, 
                           linf_dfi_viol = as.integer(c(li_n_dfi_viol, li_e_dfi_viol)),
                           lsup_dfi_viol = as.integer(c(ls_n_dfi_viol, ls_e_dfi_viol)))


# Elimina los nombres de fila
row.names(est_dfi_viol) &lt;- row.names(se_dfi_viol) &lt;- row.names(cv_dfi_viol) &lt;- 
  row.names(lim_dfi_viol) &lt;- NULL

# Exportar salida a un archivo de Excel #
# Lista de estimaciones
list_of_datasets &lt;- list(&quot;Estimaciones&quot; = est_dfi_viol, 
                         &quot;Error Estandar&quot; = se_dfi_viol, 
                         &quot;Coef Variacion&quot; = cv_dfi_viol, 
                         &quot;Int Confianza&quot; = lim_dfi_viol)


write.xlsx(list_of_datasets, 
           file = paste0(&quot;DFI_viol_envipe_enadid_FAC_ELE_&quot;, format(Sys.Date(), &quot;%y%m%d&quot;), &quot;.xlsx&quot;))</code></pre>
<p><br><br> <br><br></p>
</div>
</div>
</div>
<div id="análisis-de-las-macrodeterminantes-de-la-emigración"
class="section level1">
<h1>4. Análisis de las macrodeterminantes de la emigración</h1>
<div id="descarga-el-proyecto-apretando-sobre-este-texto-1"
class="section level3">
<h3><a
href="https://365inegi-my.sharepoint.com/:f:/g/personal/daniela_serrano_inegi_org_mx/EtKMm3U_wytAomc0hgaK1lABZL_QDCfHtujJNV0TAn0pAw?e=uDUnLg"><strong>Descarga
el proyecto apretando sobre este texto</strong></a></h3>
<p><br><br></p>
<p>Para estimar la tasa de personas que, en relación con la violencia,
cambiaron de municipio de residencia (por cada cien mil habitantes) se
configuraron las variables independiente y de control y se ajustaron
diversos modelos de regresión, lineal utilizando mínimos cuadrados
ordinarios (ols), rlm, errores espaciales, mínimos cuadrados
generalizados (gls), lineal ponderado y aditivo generalizado (gam). Se
puso a prueba un efecto no lineal de la violencia sobre el DFI.
Adicionalmente, se transformaron las variables dependiente e
independiente en logarítmicas y se suavizó la tasa promedio quinquenal
de homicidios municipales. La selección del modelo óptimo se basó en
pruebas de supuestos y criterios de información como el AIC (Criterio de
Información de Akaike) y BIC (Criterio de Información Bayesiano). La
variable dependiente es la tasa de personas que cambiaron de lugar de
residencia y atribuyeron la violencia como causa principal de dicho
cambio, por cada cien mil habitantes, durante el período 2015-2019
(tasa_dfi). Solo se consideraron los municipios al inicio del
quinquenio. Para lidiar con la heteroscedasticidad y anormalidad, esta
variable se transformó en su versión logarítmica más uno (log plus one)
. Considerando que entre 2015 y 2019 en más de la mitad de los
municipios del país no hubo un solo homicidio doloso y el logaritmo de
cero no está definido, se agregó una pequeña constante (como 1). A esto
se le suele denominar transformación “log más uno”. La estimación se
realizó a partir de la pregunta de lugar de residencia cinco años atrás,
la tasa de DFI se define como:</p>
<p><span class="math display">\[
\mathrm{TDFI}_i = \left(\frac{\mathrm{DFI}_i}{\mathrm{POB}_i}\right)
\times 100,000
\]</span></p>
<p>Donde: <span class="math display">\[
\mathrm{TDFI}_i : \text{Tasa de DFI (intermunicipal) del municipio } i
\text{ de 2015 a 2019 por cada cien mil habitantes}
\]</span> <span class="math display">\[
\mathrm{DFI}_i : \text{Personas en situación de DFI del municipio } i
\text{ de 2015 a 2019}
\]</span> <span class="math display">\[
\mathrm{POB}_i : \text{Población total del municipio } i \text{ a mitad
del periodo } t \text{ a } t+4
\]</span></p>
<br><br> <br><br> Para explorar más a fondo la variación de la tasa de
DFI por violencia, la Figura 2 muestra su distribución geográfica.
<br><br> <br><br>
<div style="text-align: center;">
<p>
<strong>Figura 2. Tasa de personas que cambiaron de lugar de residencia
y atribuyeron a la violencia dicho cambio por cada cien mil habitantes,
2015-2019</strong>
</p>
<p><img src="imagenes/figura2_mapa.png" /></p>
</div>
<p>La tasa suavizada se expresa como un promedio ponderado de la tasa
bruta, <span class="math inline">\(r\)</span>, y la estimación previa,
<span class="math inline">\(\theta\)</span>. Esta última se estima como
una tasa de referencia, el promedio estatal. En esencia, la técnica EB
consiste en calcular un promedio ponderado entre la tasa bruta de cada
municipio y el promedio estatal, con ponderaciones proporcionales a la
población subyacente en riesgo. Es decir, los municipios pequeños (es
decir, con una pequeña población en riesgo) tenderán a que sus tasas se
ajusten considerablemente, mientras que en los municipios más grandes
las tasas apenas cambiarán. De manera formal:</p>
<p><span class="math display">\[
\pi_i^{EB} = w_i r_i + \left(1 - w_i\right) \theta
\]</span></p>
<p>En esta expresión, los pesos son:</p>
<p><span class="math display">\[
w_i = \frac{\sigma^2}{\sigma^2 + \frac{\mu}{P_i}}
\]</span></p>
<p>donde <span class="math inline">\(P_i\)</span> es la población del
municipio <span class="math inline">\(i\)</span>. En el enfoque
bayesiano empírico, la media <span class="math inline">\(\mu\)</span> y
la varianza <span class="math inline">\(\sigma^2\)</span> previas son
estimadas a partir de los datos. La Figura 3 muestra la distribución de
esta variable independiente suavizada. Se aplicó una transformación
adicional: el logaritmo de la variable más uno, igual que la
transformación realizada en la variable dependiente.</p>
<br><br>
<div style="text-align: center;">
<p>
<strong>Figura 3. Tasa suavizada promedio de homicidios,
2015-2019</strong>
</p>
<p><img src="imagenes/figura3_mapa.png" /></p>
</div>
<br><br>
<hr style="border: 3px solid#0077C8;" />
</div>
<div id="variables" class="section level3">
<h3>Variables</h3>
<p>Con base en la literatura, inicialmente se consideraron ocho
variables de control: 1. Nivel de desempleo (empleo): porcentaje de
población desocupada (estimador Horvitz-Thompson a partir de datos del
cuestionario ampliado del Censo 2020). 2. Coeficiente de Gini
(coef_de_gini): coeficiente de gini, una medida de la desigualdad de
ingresos (Coneval). 3. Índice de intensidad migratoria (iim_dp2): valor
del índice de intensidad migratoria (DP2). Medida sintética que informa,
de forma integral, el fenómeno migratorio entre México-Estados Unidos
(Conapo). 4. Mediana del salario per cápita municipal
(salario_mun_mediano): mediana del salario en el municipio (estimador
Horvitz-Thompson a partir de datos del cuestionario ampliado del Censo
2020). 5. Presión demográfica (presion_demo): relación de la población
de 15 a 24 años, entre la población de 45 a 64 años por 100
(Proyecciones de la Población, 2010-2030, CONAPO). 6. Educación
(educación): nivel de educación, que podría estar representado por el
promedio de años de escolaridad, tasas de alfabetización, etc. 7. Grado
de marginación (gim_2020): grado de marginación es una variable ordinal
con cinco niveles posibles, la cual es construida con el método Dalenius
y Hodges (1959 citado en COPLADET et al., 2020) a partir del índice de
marginación. 8. Urbanización (urbanización): indica si el municipio es
igual o mayor a 15 mil habitantes.</p>
<p>Dado que los valores atípicos podían influir en los resultados, se
optó por un análisis de conglomerados k-medias (kmeans) para agrupar los
municipios según sus características sociodemográficas cuantitativas. Se
estableció una semilla aleatoria y se definió un número de cuatro
conglomerados (k = 4) para garantizar la reproducibilidad del análisis
(k = 4). La Tabla 4 presenta las estadísticas descriptivas generales y
por conglomerado, revelando diferencias significativas entre los grupos
de municipios. Al controlar por el tamaño de la población, los
conglomerados 2 y 3 muestran tasas de homicidio similares, pero difieren
en la cantidad de personas en situación de desplazamiento forzado
interno (DFI).</p>
<p>El conglomerado 2, aunque compuesto por solo 11 municipios, presenta
el promedio más alto de DFI (316), superando incluso al conglomerado 4,
que a pesar de tener en promedio el doble de homicidios por cada cien
mil habitantes, ocupa el segundo lugar en DFI. Además, el conglomerado 2
registra el salario municipal mediano más alto, lo que sugiere una
posible asociación entre el DFI, la violencia y los recursos
económicos.</p>
<br><br>
<div style="text-align: center;">
<p>
<strong>Tabla 4. Estadísticas descriptivas de las variables usadas en
los modelos de regresión</strong>
</p>
<p><img src="imagenes/tabla4.png" /></p>
</div>
<br><br>
<hr style="border: 3px solid#0077C8;" />
</div>
<div id="modelos-base-ols-y-rlm" class="section level2">
<h2>Modelos Base: OLS y RLM</h2>
<p>Se consideraron como modelos base los de regresión lineal con mínimos
cuadrados (OLS) y mínimos cuadrados robustos (RLM). La especificación
del modelo es la siguiente:</p>
</div>
<div id="ecuación-del-modelo" class="section level2">
<h2>Ecuación del Modelo</h2>
<p>La tasa de personas en situación de desplazamiento forzado interno
(DFI) por cada cien mil habitantes se modela utilizando la siguiente
ecuación:</p>
<p><span class="math display">\[
\mathrm{tasa\_dfi}_\mathrm{i} = \beta_0 + \beta_1 \cdot
\mathrm{tasa\_promedio\_hom}_\mathrm{i} + \beta_2 \cdot
\left(\mathrm{tasa\_promedio\_hom}_\mathrm{i}\right)^2 + \beta_3 \cdot
\mathrm{salario\_mun\_mediano}_\mathrm{i} \\
+ \beta_4 \cdot \mathrm{presion\_demo}_\mathrm{i} + \beta_5 \cdot
\log\left(\mathrm{pob\_promedio}_\mathrm{i}\right) + \beta_6 \cdot
\mathrm{cluster}_\mathrm{i} + \epsilon_i
\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_6\)</span> son los coeficientes que se estiman en el modelo para
cada una de las variables independientes.</li>
<li><span class="math inline">\(\epsilon_i\)</span> es el término de
error, que captura la variabilidad no explicada por las variables
independientes en el modelo.</li>
<li><span class="math inline">\(\mathrm{tasa\_dfi}_\mathrm{i}\)</span>
es la tasa de personas en situación de DFI por cada cien mil
habitantes.</li>
<li><span
class="math inline">\(\log(\mathrm{pob\_promedio}_\mathrm{i})\)</span>
es el logaritmo natural de la población promedio para la observación
<span class="math inline">\(i\)</span>.</li>
</ul>
<p>Incluye también el término cuadrático <span
class="math inline">\(\left(\mathrm{tasa\_promedio\_hom}_\mathrm{i}\right)^2\)</span>
para capturar posibles efectos no lineales de la violencia en la tasa de
DFI.</p>
<p>Se busca explicar la tasa de personas en situación de DFI por cada
cien mil habitantes a través de una combinación de variables
socioeconómicas, demográficas y contextuales, mientras se manejan
posibles valores atípicos, relaciones no lineales y lineales. De acuerdo
con la literatura, la violencia puede alentar ciertos comportamientos
humanos como el DFI solo hasta cierto punto. Sin embargo, más allá de un
nivel crítico de violencia, esos comportamientos pueden empezar a
decrecer.</p>
<p>El modelo de regresión lineal robusto se ajustó usando la función rlm
del paquete MASS de R. La función rlm es particularmente útil cuando los
datos contienen valores atípicos que podrían influir
desproporcionadamente en los estimadores de mínimos cuadrados ordinarios
(ols) (Wooldridge 2015). De esta manera, se obtienen estimaciones más
fiables cuando la suposición de normalidad de los errores no se cumple.
También se estimó un modelo lineal ponderado, a cada observación,
municipio, se le asignó un peso con base en el tamaño de la población
quinquenal promedio. Los pesos se calcularon como la inversa de la
población (1 / población), lo cual da más peso a las observaciones con
menor población. Esta aproximación puede ser útil para estabilizar la
varianza de los errores, pero no para capturar bien la relación entre
las variables. Esto último es posible sobre todo si las observaciones
con menor población no representan adecuadamente la variabilidad total
en la variable dependiente.</p>
</div>
<div id="modelos-espaciales" class="section level2">
<h2>Modelos espaciales</h2>
<p>Para lidiar con los problemas de correlación espacial y de
especificación del modelo debido a la falta de datos a nivel municipal,
se estimaron modelos espaciales de regresión, en particular se estimaron
modelos de “error espacial”, también conocidos como modelos de “media
móvil espacial” (errorsarlm). Estos modelos incorporan la dependencia
espacial en los residuales. Ante la presencia de estos errores, los
coeficientes de los ajustes de modelos ols son insesgados pero
ineficientes. Es decir, aunque el tamaño y el signo del coeficiente son
asintóticamente correctos, los errores estándar presentan subestimación.
El modelo considerado es de estimación de máxima verosimilitud de
modelos de error autorregresivos simultáneos espaciales de la forma:</p>
<p>El modelo de autocorrelación espacial se especifica como:</p>
<p><span class="math display">\[
y = X\beta + u, \quad u = \lambda Wu + \epsilon
\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(y\)</span> es la variable de respuesta
tomada en su forma logarítmica en cada una de las <span
class="math inline">\(N\)</span> posiciones.</li>
<li><span class="math inline">\(X\)</span> es la matriz de variables
independientes.</li>
<li><span class="math inline">\(\beta\)</span> es el vector de
coeficientes a estimar.</li>
<li><span class="math inline">\(u\)</span> es el vector de errores
espaciales.</li>
<li><span class="math inline">\(\lambda\)</span> es el parámetro de
autocorrelación espacial.</li>
<li><span class="math inline">\(W\)</span> es la matriz de pesos
espaciales.</li>
<li><span class="math inline">\(\epsilon\)</span> es el vector de
errores aleatorios.</li>
</ul>
<p>La matriz de pesos espaciales (Spatial Weights Matrix, 𝑊) tiene como
propósito definir las relaciones espaciales entre las observaciones que
indica la proximidad entre los municipios. Para los modelos espaciales
primero se construyó la matriz W. Para ello se usó el archivo shape . Se
extrajeron las coordenadas del centro de cada municipio. A partir de la
proyección cónica conforme de Lambert para México ITRF2008 se
encontraron las distancias euclidianas entre cada uno de los centros
municipales y sus k (5) vecinos más cercanos. Finalmente, se en esta
lista de vecinos se convirtió en una matriz de pesos espaciales
normalizada mediante la función nb2listw con el argumento style = “W”,
lo que asegura que la suma de los pesos de cada fila sea igual a uno.
Esta normalización es crucial para mantener una influencia total
constante de los vecinos para cada municipio, permitiendo comparaciones
justas y equilibradas en el análisis espacial.</p>
<p>Los modelos errorsarlm son una herramienta poderosa para el análisis
de datos espaciales, permitiendo capturar y modelar la autocorrelación
espacial en los errores y proporcionando así inferencias más precisas y
robustas, sobre todo cuando las covariables y el residual están
autocorrelacionados (Pebesma y Bivand 2023). La literatura aún no ha
resuelto la cuestión de cómo informar los resultados del modelo, ya que
cada covariable ahora está representada por tres impactos. Cuando se
incluyen covariables espacialmente rezagadas, dos coeficientes se
reemplazan por tres impactos (Pebesma y Bivand 2023).</p>
<p>Otros modelos</p>
<p>Ante la heterocedasticidad y la anormalidad observada, también se
optó por modelos de mínimos cuadrados generalizados (gls) y modelos
aditivos generalizados (gam). Los primeros son más flexibles que los
modelos ols en cuanto a las suposiciones sobre los errores, ya que
admiten la presencia de heterocedasticidad y autocorrelación en estos.
En este caso se buscó modelar la heterocedasticidad, especificando que
la varianza de los errores es diferente para cada clúster. Los segundos
permiten capturar relaciones no lineales, para ello se especifican
funciones suaves para las covariables.</p>
<p>Modelos con medidas alternativas de violencia</p>
<p>En la literatura no existe consenso respecto a la transformación de
variables como estrategia para lidiar con heterocedasticidad y
anormalidad (Clark 2019). Algunos trabajos sugieren que estos problemas
se pueden tratar a partir de la transformación de las variables
dependientes e independientes o de la eliminación de valores
atípicos.</p>
<p>Sin embargo, otros estudios señalan que, si bien transformar las
variables puede mejorar ciertos aspectos del modelo, esto se logra a
costa de empeorar otros, tales como la validez externa del modelo o la
interpretación de los resultados.</p>
<p>En ese mismo sentido, se argumenta que los valores considerados
atípicos podrían indicar que el modelo no captura adecuadamente el
proceso de generación de datos, y por tanto, eliminarlos sería
inadecuado. En suma, Clark (2019) sugiere utilizar modelos que se
adapten mejor a la complejidad del fenómeno, en lugar de manipular los
datos para ajustarlos a modelos simplificados. De ahí que en este
trabajo se estimaron modelos gam y gls. Por otro lado, pese a esta falta
de consenso en la literatura, se exploraron modelos con variables
suavizadas y logarítmicas, con el objetivo de aportar evidencia sobre el
efecto no lineal de la violencia en el DFI y distinguirlo de problemas
atribuibles a errores de medición. Se verificaron los supuestos
estadísticos de los modelos. Para verificar si los residuos se
distribuyen normalmente, se realizaron histogramas y gráficos
cuantil-cuantil (Q-Q), también se realizaron las pruebas analíticas de
Jarque-Bera. Para verificar el supuesto de homocedasticidad de los
residuales se consideraron las pruebas de Breusch-Pagan; para las
pruebas de normalidad y homocedasticidad, se espera obtener p-valores
superiores a 0.05, que muestran que se puede rechazar la hipótesis nula
de normalidad y homocedasticidad. Ante la no normalidad y
heterocedasticidad se calcularon modelos cuadrados robustos. Esto ayuda
a obtener estimaciones más precisas y confiables de los coeficientes de
regresión y sus intervalos de confianza en tales situaciones.
Adicionalmente se podrían obtener estimaciones con errores robustos a la
heterocedasticidad.</p>
<p>Para medir el grado de multicolinealidad se calculó el índice de
condición (IC) de la matriz de variables auxiliares finales, valores de
IC menores a 30 se consideran una prueba de multicolinealidad moderada,
lo que garantiza una estimación eficiente de los parámetros de ajuste.
Además, se analizaron los diagramas de dispersión obtenidos de las
estimaciones frente a residuos.</p>
<div id="entorno-de-trabajo-y-librerías-1" class="section level3">
<h3>Entorno de trabajo y librerías</h3>
<p>Aquí te dejo una descripción de cada paquete utilizado en tu script,
explicando brevemente para qué se utiliza en el análisis de datos y
procesamiento:</p>
<p>class: Este paquete proporciona funciones para la clasificación, como
el algoritmo K-nearest neighbors (KNN). Es útil para realizar tareas de
clasificación y análisis de patrones.</p>
<p>pscl: Proporciona herramientas para el análisis de datos de ciencia
política y sociales, incluyendo modelos de regresión para datos
dispersos (como modelos de ceros inflados y binomiales negativos).</p>
<p>MASS: Incluye funciones y conjuntos de datos para análisis
estadístico, como la estimación de modelos lineales robustos (rlm) y
otras técnicas avanzadas de modelado.</p>
<p>lmtest: Contiene herramientas para realizar pruebas de hipótesis
sobre modelos lineales, como la prueba de Breusch-Pagan para
heterocedasticidad.</p>
<p>sandwich: Ofrece estimadores robustos de la matriz de
varianza-covarianza para modelos lineales, utilizados para ajustar
modelos cuando las suposiciones de homocedasticidad no se cumplen.</p>
<p>raster: Este paquete es útil para el análisis de datos espaciales
rasterizados. Se utiliza en estudios de datos geográficos para la
manipulación y análisis de mapas y superficies.</p>
<p>sf: Proporciona una estructura de datos para representar objetos
espaciales simples, que es compatible con el estándar Simple Features.
Es esencial para manejar datos espaciales y realizar análisis
geoespaciales.</p>
<p>rgdal: Ofrece interfaces para la lectura y escritura de datos
geoespaciales en varios formatos, incluyendo shapefiles. Es fundamental
en la integración de datos geográficos.</p>
<p>ggplot2: Es uno de los paquetes más populares para la visualización
de datos en R. Facilita la creación de gráficos complejos de forma
intuitiva y altamente personalizable.</p>
<p>caret: Herramienta integral para la clasificación y regresión en R,
que incluye funciones para la preprocesamiento de datos, ajuste de
modelos y validación cruzada.</p>
<p>broom: Convierte modelos estadísticos en data frames ordenados, lo
que facilita la manipulación y visualización de los resultados de los
modelos.</p>
<p>foreign: Proporciona funciones para leer y escribir datos en varios
formatos de software estadístico como SPSS, SAS, y Stata, facilitando el
intercambio de datos entre diferentes programas.</p>
<p>readr: Parte del tidyverse, este paquete proporciona funciones para
leer datos de archivos de texto de manera rápida y fácil, como CSV o
TSV.</p>
<p>dplyr: Es un paquete esencial del tidyverse para la manipulación de
datos, ofreciendo verbos para filtrar, seleccionar, mutar y agrupar
datos de manera eficiente.</p>
<p>janitor: Ofrece herramientas para limpiar y examinar datos, como la
normalización de nombres de columnas y la creación de tablas de
contingencia.</p>
<p>survey: Proporciona herramientas para el análisis de encuestas
complejas, incluyendo el cálculo de estimadores, errores estándar y
pruebas de hipótesis con diseños muestrales complejos.</p>
<p>tidyverse: Un metapaquete que incluye ggplot2, dplyr, tidyr, readr,
purrr, tibble, stringr, y forcats, cubriendo la mayoría de las
necesidades de análisis de datos desde la importación hasta la
visualización.</p>
<p>data.table: Es una extensión de data.frame que ofrece herramientas
para la manipulación de grandes conjuntos de datos de manera rápida y
eficiente.</p>
<p>questionr: Herramientas útiles para la investigación en ciencias
sociales, incluyendo funciones para el análisis de cuestionarios y
encuestas.</p>
<p>readxl: Parte del tidyverse, este paquete permite leer datos desde
archivos Excel (.xls y .xlsx) de forma rápida y sin necesidad de
depender de Java.</p>
<p>writexl: Facilita la exportación de datos a archivos Excel (.xlsx) de
manera simple y sin necesidad de dependencias externas como Java.</p>
<p>haven: Permite importar y exportar archivos de datos de SPSS, Stata y
SAS, facilitando el trabajo con datos provenientes de estos programas en
R.</p>
<p>knitr: Se utiliza para generar reportes dinámicos en R, convirtiendo
código R y texto en documentos HTML, PDF, y más.</p>
<p>kableExtra: Extiende la funcionalidad de knitr::kable permitiendo la
creación de tablas altamente personalizables en HTML y PDF, ideal para
la presentación de resultados.</p>
<pre class="r"><code># Clear environment and setup options
rm(list = ls())
graphics.off()
options(warn = -1, scipen = 9999)
Sys.setlocale(&quot;LC_ALL&quot;, &quot;es_ES.UTF-8&quot;)

# Load and install necessary packages
required_packages &lt;- c(
  &quot;AER&quot;, &quot;beepr&quot;, &quot;brms&quot;, &quot;broom&quot;, &quot;caret&quot;, &quot;class&quot;, &quot;classInt&quot;, &quot;dplyr&quot;,
  &quot;fitdistrplus&quot;, &quot;flextable&quot;, &quot;foreign&quot;, &quot;ggplot2&quot;, &quot;gt&quot;, &quot;gtsummary&quot;,
  &quot;gridExtra&quot;, &quot;kableExtra&quot;, &quot;knitr&quot;, &quot;janitor&quot;, &quot;lmtest&quot;, &quot;lme4&quot;, &quot;mctest&quot;, &quot;MASS&quot;, &quot;mgcv&quot;, 
  &quot;nlme&quot;, &quot;openxlsx&quot;, &quot;pscl&quot;, &quot;raster&quot;, &quot;readr&quot;, &quot;readxl&quot;, &quot;rgdal&quot;, 
  &quot;robustbase&quot;, &quot;rsample&quot;, &quot;sandwich&quot;, &quot;sf&quot;, &quot;skimr&quot;, &quot;spatialreg&quot;, 
  &quot;spdep&quot;, &quot;tseries&quot;, &quot;tidyr&quot;, &quot;sjPlot&quot;
)

# Install any missing packages and load all
new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,&quot;Package&quot;])]
if(length(new_packages)) install.packages(new_packages)
lapply(required_packages, library, character.only = TRUE)

# Set default ggplot2 theme
theme_set(theme_void())</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Carga y Procesamiento de Datos<br />
2.1 Población Migrante por Municipio<br />
Primero, cargamos los datos de población de 5 años y más migrante por
municipio, entidad federativa o país de residencia en marzo de 2015 y
sexo según causa de la migración entre marzo de 2015 y marzo de
2020</li>
</ol>
<pre class="r"><code># Leer el archivo Excel y limpiar los nombres de las columnas
emigrantes_t &lt;- read_excel(&quot;data_lmd/LM 2462.xlsx&quot;, skip = 3) %&gt;% 
  clean_names()

# Cambiar los nombres de las columnas 
colnames(emigrantes_t) &lt;- c(&quot;nom_ent_cve&quot;, &quot;nom_mun_cve&quot;, &quot;sexo&quot;, &quot;poblacion_total&quot;,
                            &quot;buscar_trabajo&quot;, &quot;cambio_u_oferta_de_trabajo&quot;, 
                            &quot;reunirse_con_la_familia&quot;, &quot;se_caso_o_unio&quot;, &quot;estudiar&quot;, 
                            &quot;por_violencia&quot;, &quot;por_desastres&quot;, &quot;lo_deportaron&quot;, &quot;otra_causa&quot;,
                            &quot;no_especificado&quot;) 

# Filtrar los datos y seleccionar solo las filas donde &#39;sexo&#39; es &quot;Total&quot;
emigrantes_t &lt;- emigrantes_t %&gt;% 
  filter(sexo == &quot;Total&quot; &amp; nom_mun_cve != &quot;Total&quot; &amp; nom_mun_cve != &quot;Municipio no especificado&quot;)

# Crear las columnas &#39;ent&#39; y &#39;mun&#39; utilizando substr
emigrantes_t$ent &lt;- substr(emigrantes_t$nom_ent_cve, 1, 3)
emigrantes_t$mun &lt;- substr(emigrantes_t$nom_mun_cve, 1, 3)

# Eliminar solo un cero al inicio de &#39;ent&#39;
emigrantes_t$ent &lt;- ifelse(substr(emigrantes_t$ent, 1, 1) == &quot;0&quot;, substr(emigrantes_t$ent, 2, nchar(emigrantes_t$ent)), emigrantes_t$ent)

# Crear la columna &#39;nom_mun&#39;
emigrantes_t$nom_mun &lt;- substr(emigrantes_t$nom_mun_cve, 5, nchar(emigrantes_t$nom_mun_cve))

# Crear la variable caracter &#39;municipios&#39; que combina &#39;ent&#39; y &#39;mun&#39; La función trimws() se utiliza para eliminar cualquier espacio en blanco adicional alrededor del contenido de la columna mun
emigrantes_t$id_mun &lt;- paste0(emigrantes_t$ent, trimws(emigrantes_t$mun))

# Mostrar el resultado
print(emigrantes_t)</code></pre>
<p>2.2. Variable independiente. Solicitud: CAS-198910-X3H1D7
CRM:0874324. Tasa (bayesianos empíricos) municipal de homicidios por
cada cien mil habitantes 2015-2017 # Leer el archivo Excel y limpiar los
nombres de las columnas # Seleccionar las columnas necesarias</p>
<pre class="r"><code># Leer el archivo Excel y limpiar los nombres de las columnas
# Seleccionar las columnas necesarias
tasas_homicidios &lt;- read_excel(&quot;Tasas homicidios.xlsx&quot;, sheet = &quot;Datos&quot;) %&gt;% 
  clean_names() %&gt;% 
  dplyr::select(id_mun, ent_nom, mun_nom, tm15_17)</code></pre>
<p>2.3 Marco Geoestadístico 2018<br />
Aquí, cargamos y procesamos el marco geoestadístico, que es esencial
para asociar datos espaciales a los municipios y entidades de
México.</p>
<pre class="r"><code># Leer los shapefiles para municipios y entidades
marco_geoest &lt;- st_read(&quot;MG_2020_Integrado/00mun.shp&quot;) %&gt;% 
  clean_names() %&gt;% 
  rename(id_mun = cvegeo)

marco_geoest_ent &lt;- st_read(&quot;MG_2020_Integrado/00ent.shp&quot;) %&gt;% 
  clean_names()</code></pre>
<p>2.4 Censo 2020<br />
Se cargan datos socioeconómicos del Censo 2020 y se procesan para el
análisis. Se incluye información sobre empleo, educación, y
salarios.</p>
<pre class="r"><code># Leer datos del Censo 2020
ca_censo_2020 &lt;- read_csv(&quot;dfi_tasa_hom_itam.csv&quot;) %&gt;%
  mutate(municipios = as.numeric(municipios)) %&gt;%
  transmute(
    id_mun = sprintf(&quot;%05d&quot;, municipios),
    nom_ent.y,
    nom_mun,
    ent = as.character(ent),
    empleo,
    educacion,
    salario_mun_mediano,
    salario,
    presion_demo
  )</code></pre>
<p>2.5 Coeficiente de Gini e Índice de Marginación<br />
Se descarga y procesa la información sobre el coeficiente de Gini e
índice de marginación municipal para el año 2015.</p>
<pre class="r"><code># Descargar y procesar datos del coeficiente de Gini
nombres &lt;- c(&quot;clave_ent&quot;, &quot;entidad&quot;, &quot;id_mun&quot;, &quot;nom_mun&quot;, &quot;coef_de_gini&quot;, &quot;ingreso_ratio&quot;) 
# Definir la URL del archivo ZIP
url_gini &lt;- &quot;https://www.coneval.org.mx/Medicion/MP/Documents/Cohesion_social/Indicadores_cohesion_social_municipio_Mexico_2010-2020.zip&quot;

# Descargar y descomprimir el archivo ZIP
temp_zip_gini &lt;- tempfile()
download.file(url_gini, temp_zip_gini)
unzip(temp_zip_gini, exdir = &quot;temp_folder_gini&quot;)

# Leer el archivo Excel
archivo_gini &lt;- list.files(&quot;temp_folder_gini&quot;, pattern = &quot;.xlsx&quot;, full.names = TRUE)
gini &lt;- read_excel(archivo_gini,sheet = &quot;2015&quot;, range = &quot;B11:G2467&quot;, col_names = nombres)

# Convertir variables numericas
gini &lt;- mutate_at(gini, vars(coef_de_gini:ingreso_ratio), as.numeric)</code></pre>
<p>2.6 Establecer Causas de Defunción por Homicidio<br />
Primero, definimos un vector que contiene los códigos CIE-10
correspondientes a causas de defunción por homicidio.</p>
<pre class="r"><code># Establecer causas de defunción por homicidio de acuerdo con CIE-10 X85-Y09
causa_defunciones &lt;- c(&quot;X85&quot;, &quot;X86&quot;, &quot;X87&quot;, &quot;X88&quot;, &quot;X89&quot;, &quot;X90&quot;, &quot;X91&quot;, &quot;X92&quot;, 
                       &quot;X93&quot;, &quot;X94&quot;, &quot;X95&quot;, &quot;X96&quot;, &quot;X97&quot;, &quot;X98&quot;, &quot;X99&quot;, &quot;Y00&quot;, 
                       &quot;Y01&quot;, &quot;Y03&quot;, &quot;Y04&quot;, &quot;Y05&quot;, &quot;Y06&quot;, &quot;Y07&quot;, &quot;Y08&quot;, &quot;Y09&quot;)
2.2 Cargar y Procesar Datos de Defunciones
Se cargan los datos de defunciones desde 2015 hasta 2019. Los datos se filtran para incluir solo las defunciones que corresponden a homicidios según los códigos definidos.</code></pre>
<pre class="r"><code>Copiar código
# Crear una lista para almacenar los datos de defunciones por año
defunciones_list &lt;- list()

# Crear una secuencia de años desde 2015 hasta 2019
years &lt;- 2015:2019

# Iterar sobre los años
for (year in years) {
  # Definir el nombre del archivo DBF para el año actual
  file_name &lt;- file.path(&quot;data_t8_defunciones&quot;, paste0(&quot;defunciones_base_datos_&quot;, year, &quot;/&quot;, &quot;DEFUN&quot;, year %% 100, &quot;.DBF&quot;))
  
  # Leer y procesar el archivo DBF
  defunciones &lt;- read.dbf(file_name) %&gt;%
    clean_names() %&gt;%
    mutate(causa_defun = as.character(causa_def)) %&gt;%
    filter(substr(causa_defun, 1, 3) %in% causa_defunciones)
  
  # Almacenar los datos en la lista
  defunciones_list[[as.character(year)]] &lt;- defunciones
}

# Combinar todos los datos en un único marco de datos si es necesario
defunciones_combinada &lt;- bind_rows(defunciones_list)</code></pre>
<p>Después de combinar los datos de defunciones, se generan nuevas
variables y se agrupan los datos según la entidad y el municipio de
registro, así como por año.</p>
<pre class="r"><code># Generar variables
defunciones_combinada &lt;- defunciones_combinada %&gt;%
  mutate(
    causa = substring(causa_defun, 1, 3),
    id_mun = paste0(ent_regis, mun_regis)) %&gt;%
  dplyr::select(-causa_defun)  # Eliminar columna causa_defun si ya no es necesaria

# Agrupar defunciones por homicidio por año de registro, entidad y municipio de ocurrencia
hom_mun_quiquenal &lt;- defunciones_combinada %&gt;%
  group_by(id_mun, ent_regis, mun_regis) %&gt;% 
  summarise(hom15_20 = n()) 

# Agrupar defunciones por homicidio por año de registro, entidad y municipio de ocurrencia
homicidio_anuales &lt;- defunciones_combinada %&gt;%
  group_by(id_mun, ent_regis, mun_regis, anio_regis) %&gt;%
  summarise(homicidios = n())</code></pre>
<p>2.7 Carga de Otros Datos: Índice de Marginación e Intensidad
Migratoria<br />
Cargamos y procesamos datos adicionales como el índice de marginación y
la intensidad migratoria para complementar el análisis.</p>
<pre class="r"><code>url_imm &lt;- &quot;http://www.conapo.gob.mx/work/models/CONAPO/Marginacion/Datos_Abiertos/Municipio/IMM_DP2_2015.xlsx&quot;


# Descarga el archivo y guárdalo localmente
archivo_imm &lt;- tempfile()
download.file(url_imm, archivo_imm, mode = &quot;wb&quot;)

# Leer el archivo Excel
imm &lt;- readxl::read_xlsx(archivo_imm, sheet = &quot;IMM_2015&quot;) %&gt;% 
  clean_names() %&gt;% 
  rename(id_mun = cve_mun)


# 2.8. Intensidad migratoria municipal. Fuente CONAPO: https://www.datos.gob.mx/busca/dataset/indice-absoluto-de-intensidad-migratoria-mexico--estados-unidos-2000--2010
iaim_2020 &lt;- read_csv(&quot;https://conapo.segob.gob.mx/work/models/CONAPO/IIM/iim_base2020m.csv&quot;)
iaim_2020 &lt;- iaim_2020  %&gt;% 
  clean_names() %&gt;% 
  mutate(id_mun = ifelse(nchar(cve_geo) == 4,
                         paste0(&quot;0&quot;, cve_geo), cve_geo))</code></pre>
<p>2.8 Procesamiento de Datos de Población<br />
Se cargan y procesan los datos de población por grupos de edad,
necesarios para calcular tasas y realizar otros análisis.</p>
<pre class="r"><code># Leer y unir bases de población
poblacion_municipal &lt;- bind_rows(
  read_csv(&quot;data_t5_conapo/base_municipios_final_datos_01.csv&quot;, locale = locale(encoding = &quot;latin1&quot;)),
  read_csv(&quot;data_t5_conapo/base_municipios_final_datos_02.csv&quot;, locale = locale(encoding = &quot;latin1&quot;))
) %&gt;% clean_names() %&gt;% 
  rename(anio = ano)

# Arreglar claves de los municipios
poblacion_municipal &lt;- poblacion_municipal %&gt;% 
  mutate(entidad = ifelse(nchar(clave_ent) == 1,
                          paste0(&quot;0&quot;, clave_ent), clave_ent),
         id_mun = ifelse(nchar(clave) == 4,
                         paste0(&quot;0&quot;, clave), clave))

# Filtrar por año y grupo de edad
pob_5_y_mas_mun_2017 &lt;- poblacion_municipal %&gt;%
  filter(anio == 2017, edad_quin != &quot;pobm_00_04&quot;)%&gt;%
  group_by(id_mun) %&gt;%
  summarise(poblacion = sum(pob))

pob_14_25_2017 &lt;- poblacion_municipal %&gt;%
  filter(anio == 2017 &amp; (edad_quin == &quot;pobm_15_19&quot; | edad_quin == &quot;pobm_20_24&quot;)) %&gt;%
  group_by(id_mun) %&gt;%
  summarise(pob15_24 = sum(pob))

pob_45_64_2017 &lt;- poblacion_municipal %&gt;%
  filter(anio == 2017 &amp; (edad_quin == &quot;pobm_45_49&quot; | edad_quin == &quot;pobm_50_54&quot; | edad_quin == &quot;pobm_55_59&quot; | edad_quin == &quot;pobm_60_64&quot;)) %&gt;%
  group_by(id_mun) %&gt;%
  summarise(pob45_64 = sum(pob))

pob_anual &lt;- poblacion_municipal %&gt;%
  filter(anio &gt; 2014 &amp; anio &lt; 2020)%&gt;%
  group_by(id_mun, anio) %&gt;%
  summarise(poblacion_anual = sum(pob))</code></pre>
<p>2.9 Cálculo de Tasas Promedio de Homicidio<br />
Se calculan las tasas promedio de homicidio por cada 100,000 habitantes
utilizando los datos de población y defunciones.</p>
<pre class="r"><code># Calcular la tasa de homicidios por cada 100,000 habitantes
tasas &lt;- pob_anual %&gt;%
  left_join(homicidio_anuales, by = c(&quot;id_mun&quot;, &quot;anio&quot;=&quot;anio_regis&quot;)) %&gt;%
  dplyr::select(-ends_with(&quot;.x&quot;)) %&gt;%
  dplyr::select(-ends_with(&quot;regis&quot;)) %&gt;% 
  rename_with(~ stringr::str_remove(.x, pattern = &quot;.y&quot;), ends_with(&quot;.y&quot;))

# Imputar ceros en lugar de NA en la columna &#39;homicidios&#39; en minúsculas
tasas$homicidios &lt;- ifelse(is.na(tasas$homicidios), 0, tasas$homicidios)

# Calcular la tasa de homicidios por cada 100,000 habitantes
tasas &lt;- tasas %&gt;%
  mutate(
    tasa_hom_100mil = round((homicidios / poblacion_anual) * 100000, 1),  # Calcular la tasa y redondear a 1 decimal
    ent_regis = substr(id_mun, 1, 2)  # Extraer los dos primeros caracteres de id_mun y asignar a ent_regis
  )


##### Calcular tasas promedio suavizadas
# Agrupar los datos por entidad y año

# Calcular todos los parámetros necesarios para la fórmula de varianza
tasas_por_edo &lt;- tasas  %&gt;%
  group_by(ent_regis, anio) %&gt;%
  mutate(
    mu = sum(homicidios) / sum(poblacion_anual),  # Estimación &#39;mu&#39; para el estado/condado
    O_i = homicidios,                             # Valor &#39;O_i&#39; (total de casos)
    P_i = poblacion_anual,                         # Valor &#39;P_i&#39; (total de población)
    n = n(),                                       # Número de observaciones (total de registros)
    r_i = homicidios / poblacion_anual,            # Estimación &#39;r_i&#39; para cada observación
    variance = var(homicidios / poblacion_anual)  # Varianza de &#39;r_i&#39;
  ) %&gt;%
  mutate(
    w_i = variance / (variance + mu / P_i),         # Ponderación &#39;w_i&#39;
    eb_bayes = w_i * r_i + (1 - w_i) * mu           # Estimación &#39;eb_bayes&#39; final
  )

prome_hom_rate &lt;-  tasas_por_edo %&gt;% 
  group_by(id_mun) %&gt;% 
  summarise(tasa_hom_suav=(mean(eb_bayes)*10^5))

# Calcular tasa promedio
tasas_promedio &lt;- tasas %&gt;% 
  group_by(id_mun) %&gt;% 
  summarise(homicidios = round(sum(as.numeric(homicidios), na.rm = TRUE), 1),
            pob_promedio = round(mean(poblacion_anual), 1),
            tasa_promedio_hom = round(mean(tasa_hom_100mil, na.rm = TRUE), 1))
# th_suvizadas= round(mean(th_prom_suav, na.rm = TRUE), 1 )*10^5)
# tasa_prom_predic=mean(predicciones, na.rm = TRUE),
# tasa_prom_est=mean(estimaciones, na.rm = TRUE))
tasas_promedio &lt;- tasas_promedio %&gt;% 
  left_join(prome_hom_rate)
# # Imputar 0 en las variables &#39;tasa_prom_predic&#39; y &#39;tasa_prom_estimada&#39; si &#39;homicidios&#39; es igual a 0
# Actualizar tasa_hom_suav basado en la condición de homicidios
tasas_promedio$th_prom_suav &lt;- ifelse(tasas_promedio$homicidios == 0, 0, tasas_promedio$tasa_hom_suav)
# tasas_promedio$th_prom_suav &lt;- ifelse(tasas_promedio$homicidios == 0, 0, tasas_promedio$tasa_hom_suav)</code></pre>
<p>2.10 Integración de Datos y Proyección Geográfica<br />
Se integran los diferentes conjuntos de datos en un único marco de datos
y se transforman a la proyección geográfica correcta.</p>
<pre class="r"><code># Definir una función para realizar la unión y limpiar los nombres de columnas
unir_y_limpiar &lt;- function(datos, datos_nuevos, sufijo) {
  left_join(datos, datos_nuevos, by = &quot;id_mun&quot;) %&gt;%
    dplyr::select(-ends_with(&quot;.x&quot;)) %&gt;%
    rename_with(~ stringr::str_remove(.x, pattern = &quot;.y&quot;), ends_with(&quot;.y&quot;)) 
}

# Unir los datos y limpiar nombres de columnas utilizando la función definida
datos_integrados &lt;- marco_geoest %&gt;%
  unir_y_limpiar(tasas_homicidios, sufijo = &quot;.tasa_hom_b&quot;) %&gt;%
  unir_y_limpiar(emigrantes_t, sufijo = &quot;.emigrantes_t&quot;) %&gt;%
  unir_y_limpiar(ca_censo_2020, sufijo = &quot;.censo_2020&quot;) %&gt;%
  unir_y_limpiar(gini, sufijo = &quot;.gini&quot;) %&gt;%
  unir_y_limpiar(imm, sufijo = &quot;.imm&quot;) %&gt;%
  unir_y_limpiar(iaim_2020, sufijo = &quot;.iaim_2020&quot;) %&gt;%
  unir_y_limpiar(pob_5_y_mas_mun_2017, sufijo = &quot;.pob_5_y_mas_2017&quot;) %&gt;%
  unir_y_limpiar(pob_14_25_2017, sufijo = &quot;.pob_14_25_2017&quot;) %&gt;%
  unir_y_limpiar(pob_45_64_2017, sufijo = &quot;.pob_45_64_2017&quot;) %&gt;%
  unir_y_limpiar(tasas_promedio, sufijo = &quot;.tasas_promedio&quot;)

# Proyección cónica conforme de Lambert para México ITRF2008 (EPSG:6362)
datos_integrados &lt;- st_transform(datos_integrados, crs = &quot;+init=epsg:6362&quot;)</code></pre>
<p>2.11 Limpieza del ambiente</p>
<pre class="r"><code># Limpiar ambiente
rm(ca_censo_2020,
   defunciones,
   defunciones_combinada,
   defunciones_list,
   emigrantes_t,
   gini,
   hom_mun_quiquenal,
   homicidio_anuales,
   iaim_2010,
   imm,
   modelo,
   pob_14_25_2017,
   pob_45_64_2017,
   pob_5_y_mas_mun_2017,
   pob_anual,
   poblacion_municipal,
   tasas,
   tasas_homicidios,
   tasas_promedio,
   archivo_gini,
   archivo_imm,
   causa_defunciones,
   file_name,
   i,
   nombres,
   paquetes,
   temp_zip_gini,
   url_gini,
   url_imm,
   year,
   years,
   unir_y_limpiar
)

# Quitar municipios no establecidos antes de 2015
data_mun15 &lt;- datos_integrados %&gt;%
  filter(!is.na(ent_nom) &amp; !is.na(ent)) %&gt;%
  dplyr::select(-c(ingreso_ratio, sexo, clave_ent, entidad, sbasc, ovsde, ovsee, ovsae,
                   ovpt, vhac, pl_5000, po2sm, ent, mun, nom_mun, viv_rem, viv_circ, viv_ret, lugar)) %&gt;%
  mutate(tasa_dfi = (as.numeric(por_violencia) / pob_promedio) * 100000)</code></pre>
<p>2.12 Cálculo de tasas: El código calcula diferentes tasas
relacionadas con la población, la migración, el empleo, la familia, el
matrimonio, y la educación. Todas estas tasas se calculan en base a un
denominador común (pob_promedio) y se multiplican por 100,000 para
estandarizarlas por cada 100,000 personas.</p>
<p><strong>tasa_migracion:</strong> Calcula la tasa de migración. Es la
proporción de la población total (poblacion_total) en relación con la
población promedio (pob_promedio), multiplicada por 100,000.</p>
<p><strong>tasa_trabajo_b:</strong> Calcula la tasa de personas que
buscan trabajo. Es la proporción de la cantidad de personas que buscan
trabajo (buscar_trabajo) en relación con la población promedio,
multiplicada por 100,000.</p>
<p><strong>tasa_trabajo_o:</strong> Calcula la tasa de personas que
cambian o buscan una nueva oferta de trabajo. Es la proporción de
personas que cambiaron o buscaron una nueva oferta de trabajo
(cambio_u_oferta_de_trabajo) en relación con la población promedio,
multiplicada por 100,000.</p>
<p><strong>tasa_familia:</strong> Calcula la tasa de personas que se
reunieron con su familia. Es la proporción de personas que se reunieron
con la familia (reunirse_con_la_familia) en relación con la población
promedio, multiplicada por 100,000.</p>
<p><strong>tasa_caso:</strong> Calcula la tasa de personas que se
casaron o unieron. Es la proporción de personas que se casaron o unieron
(se_caso_o_unio) en relación con la población promedio, multiplicada por
100,000.</p>
<p><strong>tasa_estudiar:</strong> Calcula la tasa de personas que
migraron para estudiar. Es la proporción de personas que migraron para
estudiar (estudiar) en relación con la población promedio, multiplicada
por 100,000.</p>
<p><strong>as.numeric:</strong> Esta función se usa para asegurar que
las columnas utilizadas en los cálculos sean de tipo numérico. Si alguna
de estas columnas es de tipo carácter, <code>as.numeric</code> las
convierte a números antes de realizar las operaciones.</p>
<p>En resumen, creamos nuevas variables (tasas) en el data frame
data_mun15, las cuales representan tasas s por cada 100,000 personas
para diferentes motivos de migración.</p>
<pre class="r"><code>data_mun15 &lt;-  data_mun15  %&gt;%
  mutate(tasa_migracion = (as.numeric(poblacion_total) / pob_promedio) * 100000,
         tasa_trabajo_b = (as.numeric(buscar_trabajo) / pob_promedio) * 100000,
         tasa_trabajo_o = (as.numeric(cambio_u_oferta_de_trabajo) / pob_promedio) * 100000,
         tasa_familia = (as.numeric(reunirse_con_la_familia) / pob_promedio) * 100000,
         tasa_caso = (as.numeric(se_caso_o_unio) / pob_promedio) * 100000, 
         tasa_estudiar = (as.numeric(estudiar) / pob_promedio) * 100000)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Análisis de Datos<br />
3.1 Análisis Descriptivo por Cluster<br />
Realizamos un análisis descriptivo de los datos agrupados por clusters,
utilizando estadísticas resumidas.</li>
</ol>
<pre class="r"><code># Análisis

# Este código primero completa los valores perdidos en ciertas columnas con la mediana de los datos.
# Luego, selecciona las columnas numéricas, aplica el algoritmo k-means para identificar los clusters,
# agrega la variable de cluster al dataframe y calcula las coordenadas de los centroides de las geometrías.
# Finalmente, crea variables adicionales cuadrando algunas de las existentes.
# Calcular medianas de las variables con valores perdidos y completarlos
mediana_empleo &lt;- median(data_mun15$empleo, na.rm = TRUE)
mediana_gini &lt;- median(data_mun15$coef_de_gini, na.rm = TRUE)
mediana_iim_dp2 &lt;- median(data_mun15$iim_dp2, na.rm = TRUE)

data_mun15$empleo[is.na(data_mun15$empleo)] &lt;- mediana_empleo
data_mun15$coef_de_gini[is.na(data_mun15$coef_de_gini)] &lt;- mediana_gini
data_mun15$iim_dp2[is.na(data_mun15$iim_dp2)] &lt;- mediana_gini

# Seleccionar solo las columnas numéricas
datos_numericos &lt;- data_mun15 %&gt;%
  st_drop_geometry() %&gt;%
  select_if(is.numeric)

# Convertir datos a una matriz
datos_numericos_matrix &lt;- as.matrix(datos_numericos)

# Aplicar k-means para identificar los clusters
set.seed(123)
k &lt;- 4
modelo_clusterizacion &lt;- kmeans(datos_numericos_matrix, centers = k)

# Agregar la variable de cluster al dataframe
data_mun15$cluster &lt;- as.factor(modelo_clusterizacion$cluster)

# Calcular los centroides de las geometrías y agregar coordenadas al dataframe
centroides &lt;- st_centroid(data_mun15$geometry)
data_mun15$lon &lt;- st_coordinates(centroides)[, 1]
data_mun15$lat &lt;- st_coordinates(centroides)[, 2]

# Crear variables adicionales
data_mun15$tm15_17_cuadrado &lt;- data_mun15$tm15_17^2
data_mun15$tasa_prom_cuadrado &lt;- data_mun15$tasa_promedio_hom^2
data_mun15$tasa_suav_2 &lt;-data_mun15$tasa_hom_suav^2
data_mun15$ln_pob &lt;- log(data_mun15$pob_promedio)
# Aproximación A: Añadir 1 para evitar log(0)
data_mun15$log_tasa_dfi &lt;- log(data_mun15$tasa_dfi + 1)
data_mun15$log_th &lt;- log(data_mun15$tasa_promedio_hom + 1)

# Hacer mapas
#source(&quot;code4/mapas_smooth_rate_hom.R&quot;)

# Funciones útiles para interpretar y escribir resultados de los análisis
# Función para agregar estrellas de significancia
add_significance_stars &lt;- function(p_value) {
  if (p_value &lt; 0.001) {
    return(&quot;***&quot;)
  } else if (p_value &lt; 0.01) {
    return(&quot;**&quot;)
  } else if (p_value &lt; 0.05) {
    return(&quot;*&quot;)
  } else {
    return(&quot;&quot;)
  }
}

# Función para redondear y formatear los valores
format_value &lt;- function(value) {
  return(format(round(value, 3), nsmall = 3))
}

# Crear un subset del dataframe con las variables especificadas
variables_interes &lt;- c(&quot;tasa_dfi&quot;, &quot;tasa_promedio_hom&quot;, &quot;tasa_prom_cuadrado&quot;, &quot;salario_mun_mediano&quot;, &quot;presion_demo&quot;, &quot;ln_pob&quot;, &quot;cluster&quot;)

# Subset del dataframe con las variables de interés
dfi_subset &lt;- data_mun15[, variables_interes]

# Eliminar la columna de geometría
dfi_no_geom &lt;- st_set_geometry(dfi_subset, NULL)

# Crear el resumen descriptivo agrupado por &#39;cluster&#39; con estadísticas adicionales
summary_table &lt;- dfi_no_geom %&gt;%
  tbl_summary(
    by = cluster, # Agrupar por la variable &#39;cluster&#39;
    type = list(
      where(is.numeric) ~ &quot;continuous&quot;,
      where(is.factor) ~ &quot;categorical&quot;
    ),
    statistic = list(
      all_continuous() ~ &quot;{mean} ({sd}), {min} - {max}&quot;,
      all_categorical() ~ &quot;{n} / {N} ({p}%)&quot;
    )
  ) %&gt;% 
  add_overall()

# Convertir a flextable
summary_flextable &lt;- summary_table %&gt;%
  as_flex_table()

# Agregar el título y subtítulo usando flextable
summary_flextable &lt;- flextable::add_header(summary_flextable,
                                           top = TRUE, 
                                           values = c(&quot;Resumen Estadístico por Cluster&quot;),
                                           colspan = ncol(summary_flextable))

# Guardar en archivo Word
flextable::save_as_docx(summary_flextable, path = &quot;Resultados/summary_table_240620.docx&quot;)</code></pre>
<p>3.2 Cálculo de los 5 Vecinos Más Cercanos Primero, se calculan los 5
vecinos más cercanos para cada municipio, basados en las coordenadas de
sus centroides.</p>
<pre class="r"><code># Extraer geometría y centroides de los municipios
# Paso 2: Calcular los 5 vecinos más cercanos
mexico.geom &lt;- st_geometry(data_mun15)
mexico.coords &lt;- st_centroid(mexico.geom) 
mex5_nb = knn2nb(knearneigh(mexico.coords, k = 5))


# Abrir un dispositivo gráfico para guardar el plot en un archivo PNG
png(&quot;Resultados/vecinos_mas_cercanos.png&quot;, width = 800, height = 600)

# Crear el plot
plot(mexico.geom, 
     main = &quot;k = 5&quot;,
     reset = FALSE)

plot(mex5_nb, mexico.coords, 
     add = TRUE, 
     col = 2, 
     lwd = 1.5)

# Cerrar el dispositivo gráfico
dev.off()</code></pre>
<p>3.3 Construcción de la Matriz Espacial Luego, se construye una matriz
espacial basada en los 5 vecinos más cercanos, la cual es esencial para
los análisis de autocorrelación espacial.</p>
<pre class="r"><code># Crear la matriz espacial basada en los vecinos
mex5_vecinos &lt;- knn2nb(knearneigh(mexico.coords, k = 5))
listw_5 &lt;- nb2listw(mex5_vecinos, style = &quot;W&quot;, zero.policy = TRUE)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Modelado Estadístico 3.1 Preparación de Datos para el Modelado Se
añade una columna de pesos basada en la inversa de la población, y se
definen dos fórmulas para los modelos de regresión.</li>
</ol>
<pre class="r"><code># Crear la columna &#39;peso&#39; como la inversa de &#39;Población&#39;
data_mun15$peso &lt;- 1 / data_mun15$poblacion

# Definir las fórmulas para los modelos
formula_1 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob)
formula_2 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster)</code></pre>
<p>3.2 Construcción de Modelos con efecto no lineal de la violencia
sobre el DFI Se construyen varios modelos de regresión: OLS, Mínimos
Cuadrados Robustos, Autorregresivo Espacial, Mínimos Cuadrados
Generalizados, Ponderado y Aditivo Generalizado.</p>
<pre class="r"><code># Modelos
# Modelos
lm_model_2 &lt;- lm(formula_2, data = data_mun15)
rlm_model_2 &lt;- rlm(formula_2, data = data_mun15)
sar_model_2 &lt;- errorsarlm(formula_2, data = data_mun15, listw = listw_5)
gls_model_2 &lt;- gls(formula_1,
                   data = data_mun15,
                   weights = varIdent(form = ~ 1 | factor(cluster)))
weighted_model_2 &lt;- lm(formula_2,
                       data = data_mun15, weights = peso)
gam_model_2 &lt;- gam(tasa_dfi ~ s(tasa_promedio_hom) +   s(tasa_prom_cuadrado) +
                     s(salario_mun_mediano) + s(presion_demo) + s(ln_pob) + factor(cluster),
                   data = data_mun15)</code></pre>
<p>3.2.1 Evaluación de los Modelos no lineales Para conocer las bondades
de ajuste de los modelos y probar si cumplen con los supuestos, Se
define una función, la cual calcula el AIC, BIC, pruebas de
Breusch-Pagan y Jarque-Bera. También se calcula el índice de Moran si se
proporcionan datos espaciales.</p>
<pre class="r"><code># Función para obtener métricas y pruebas
model_metrics &lt;- function(model, model_name, listw = NULL) {
  # Summary del modelo
  model_summary &lt;- summary(model)
  
  # Coeficientes y p-values
  if (inherits(model, &quot;gls&quot;)) {
    coefficients &lt;- as.data.frame(coef(model_summary))
    coefficients$term &lt;- rownames(coefficients)
    rownames(coefficients) &lt;- NULL
    coefficients &lt;- coefficients %&gt;%
      rename(estimate = Value, `[std.error]` = Std.Error, p.value = `p-value`)
  } else if (inherits(model, &quot;gam&quot;)) {
    coefficients &lt;- summary(model)$s.table
    coefficients &lt;- as.data.frame(coefficients)
    coefficients$term &lt;- rownames(coefficients)
    rownames(coefficients) &lt;- NULL
    coefficients &lt;- coefficients %&gt;%
      rename(estimate = edf, `[std.error]` = Ref.df, p.value = `p-value`)
    if (!&quot;p.value&quot; %in% names(coefficients)) {
      coefficients$p.value &lt;- NA
    }
  } else {
    coefficients &lt;- tidy(model)
    coefficients &lt;- coefficients %&gt;%
      rename(`[std.error]` = std.error)
    if (!&quot;p.value&quot; %in% names(coefficients)) {
      coefficients$p.value &lt;- NA
    }
  }
  
  # AIC y BIC
  aic_value &lt;- AIC(model)
  bic_value &lt;- BIC(model)
  
  # Breusch-Pagan test
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;))) {
    bp_test &lt;- bptest(model)
    bp_stat &lt;- bp_test$statistic
    bp_pvalue &lt;- bp_test$p.value
  } else {
    bp_stat &lt;- NA
    bp_pvalue &lt;- NA
  }
  
  # Jarque-Bera test
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;, &quot;gls&quot;))) {
    jb_test &lt;- jarque.bera.test(residuals(model))
    jb_stat &lt;- jb_test$statistic
    jb_pvalue &lt;- jb_test$p.value
  } else {
    jb_stat &lt;- NA
    jb_pvalue &lt;- NA
  }
  
  # Prueba de multicolinealidad (VIF)
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;, &quot;gls&quot;))) {
    vif_values &lt;- vif(model)
    vif_df &lt;- as.data.frame(vif_values)
    vif_df$term &lt;- rownames(vif_df)
    rownames(vif_df) &lt;- NULL
    colnames(vif_df)[1] &lt;- &quot;vif&quot;
  } else {
    vif_df &lt;- data.frame(term = NA, vif = NA)
  }
  
  # Índice de Moran si se proporciona una estructura espacial
  if (!is.null(listw)) {
    moran_test &lt;- moran.test(residuals(model), listw)
    moran_i &lt;- moran_test$estimate[1]
    moran_p &lt;- moran_test$p.value
  } else {
    moran_i &lt;- NA
    moran_p &lt;- NA
  }
  
  # Consolidar resultados en una lista
  results &lt;- list(
    model_name = model_name,
    coefficients = coefficients,
    aic = aic_value,
    bic = bic_value,
    bp_stat = bp_stat,
    bp_pvalue = bp_pvalue,
    jb_stat = jb_stat,
    jb_pvalue = jb_pvalue,
    moran_i = moran_i,
    moran_p = moran_p,
    vif = vif_df
  )
  
  return(results)
}</code></pre>
<p>Se aplica la función a cada modelo y de consolidan los resultados en
una tabla para exportar en formato .csv y .docx</p>
<pre class="r"><code># Obtener métricas para cada modelo
lm_results &lt;- model_metrics(lm_model_2, &quot;Linear Model (lm)&quot;)
rlm_results &lt;- model_metrics(rlm_model_2, &quot;Robust Linear Model (rlm)&quot;)
sar_results &lt;- model_metrics(sar_model_2, &quot;Spatial Autoregressive Model (sar)&quot;, listw_5)
gls_results &lt;- model_metrics(gls_model_2, &quot;Generalized Least Squares (gls)&quot;)
weighted_results &lt;- model_metrics(weighted_model_2, &quot;Weighted Linear Model (lm)&quot;)
gam_results &lt;- model_metrics(gam_model_2, &quot;Generalized Additive Model (gam)&quot;)

# Crear una tabla consolidada
consolidate_results &lt;- function(results_list) {
  consolidated &lt;- bind_rows(lapply(results_list, function(result) {
    result$coefficients %&gt;%
      mutate(model = result$model_name,
             aic = result$aic,
             bic = result$bic,
             bp_stat = result$bp_stat,
             bp_pvalue = result$bp_pvalue,
             jb_stat = result$jb_stat,
             jb_pvalue = result$jb_pvalue,
             moran_i = result$moran_i,
             moran_p = result$moran_p) %&gt;%
      dplyr::select(model, term, estimate, `[std.error]`, p.value, aic, bic, bp_stat, bp_pvalue, jb_stat, jb_pvalue, moran_i, moran_p)
  }))
  
  # Añadir VIF
  vif_df &lt;- bind_rows(lapply(results_list, function(result) {
    if (!is.na(result$vif$term[1])) {
      result$vif %&gt;%
        mutate(model = result$model_name) %&gt;%
        dplyr::select(model, term, vif)
    } else {
      data.frame(model = result$model_name, term = NA, vif = NA)
    }
  }))
  
  consolidated &lt;- left_join(consolidated, vif_df, by = c(&quot;model&quot;, &quot;term&quot;))
  
  return(consolidated)
}

# Consolidar todos los resultados
all_results &lt;- consolidate_results(list(lm_results, rlm_results, sar_results, gls_results, weighted_results, gam_results))

# Definir una función para agregar estrellas de significancia
add_significance_stars &lt;- function(p_value) {
  if (is.na(p_value)) {
    return(&quot;&quot;)
  } else if (p_value &lt; 0.001) {
    return(&quot;***&quot;)
  } else if (p_value &lt; 0.01) {
    return(&quot;**&quot;)
  } else if (p_value &lt; 0.05) {
    return(&quot;*&quot;)
  } else if (p_value &lt; 0.1) {
    return(&quot;.&quot;)
  } else {
    return(&quot;&quot;)
  }
}

# Agregar estrellas de significancia
all_results &lt;- all_results %&gt;%
  mutate(significance = sapply(p.value, add_significance_stars))

# Renombrar &quot;term&quot; como &quot;Variable&quot; y poner std.error entre corchetes
colnames(all_results)[colnames(all_results) == &quot;term&quot;] &lt;- &quot;Variable&quot;
colnames(all_results)[colnames(all_results) == &quot;std.error&quot;] &lt;- &quot;[std.error]&quot;

# Concatenar &quot;std.error&quot; a &quot;estimate&quot; y asignarlo a &quot;Coeficiente&quot;
all_results$estimate &lt;- round(all_results$estimate,5)
all_results$&#39;[std.error]&#39; &lt;- round(all_results$&#39;[std.error]&#39;,5)
all_results &lt;- all_results %&gt;%
  mutate(Coeficiente = paste0(estimate, &quot; [&quot;, `[std.error]`, &quot;]&quot;, significance))

# Eliminar columnas innecesarias
resultados_completos &lt;- all_results %&gt;%
  dplyr::select(-estimate, -`[std.error]`, -p.value, -significance)


# Crear df1
df1 &lt;- resultados_completos %&gt;% 
  dplyr::select(model, Variable, Coeficiente) %&gt;%
  pivot_wider(names_from = model, values_from = Coeficiente)


df2 &lt;- resultados_completos %&gt;% 
  dplyr::select(model, aic, bic, bp_stat, bp_pvalue, jb_stat, jb_pvalue, moran_i, moran_p) %&gt;%
  distinct() %&gt;%
  pivot_longer(cols = -model, names_to = &quot;Variable&quot;, values_to = &quot;value&quot;) %&gt;%
  pivot_wider(names_from = model, values_from = value)


# Unir df1_long y df2 por la columna &#39;model&#39;
combined_df &lt;- rbind(df1, df2)


# Guardar la tabla de resultados transpuesta en un archivo CSV
write.csv(combined_df, &quot;Resultados/model_comparison_results_transposed.csv&quot;, row.names = FALSE)

library(knitr)
library(kableExtra)
library(officer)
library(flextable)

# Unir df1 y df2 por la columna &#39;model&#39;
combined_df &lt;- rbind(df1, df2)

# Crear la tabla kable
kable_table &lt;- combined_df %&gt;%
  kable(format = &quot;html&quot;, booktabs = TRUE) %&gt;%
  kable_styling(full_width = FALSE, position = &quot;center&quot;)

# Convertir la tabla kable a flextable
flex_table &lt;- combined_df %&gt;%
  regulartable() %&gt;%
  set_table_properties(width = .5, layout = &quot;autofit&quot;)

# Crear un documento Word y añadir la tabla flextable
doc &lt;- read_docx() %&gt;%
  body_add_flextable(flex_table)

# Obtener la fecha actual en formato YYYYMMDD
current_date &lt;- format(Sys.Date(), &quot;%Y%m%d&quot;)

# Crear el nombre del archivo incluyendo la fecha
file_name &lt;- paste0(&quot;Resultados/modelos_old_gam_sam_&quot;, current_date, &quot;.docx&quot;)

# Guardar el documento Word
print(doc, target = file_name)</code></pre>
<p>3.3 Modelos con efecto lineal de la violencia sobre el DFI</p>
<p>Se repite el procedimiento de arriba excluyendo la tasa de homicidios
al cuadrado.</p>
<pre class="r"><code>## MODELOS CON EFECTO LINEAL DE VIOLENCIA SOBRE DFI

# Construccion de matriz espacial 
mexico.geom &lt;- st_geometry(data_mun15)
mexico.coords &lt;- st_centroid(mexico.geom) 
mex5_vecinos = knn2nb(knearneigh(mexico.coords, k = 5))
listw_5 &lt;- nb2listw(mex5_vecinos, style = &quot;W&quot;, zero.policy = TRUE)

# Crear la columna &#39;peso&#39; como la inversa de &#39;Población&#39;
data_mun15$peso &lt;- 1 / data_mun15$poblacion

# Fórmulas para modelos
formula_1 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom  + salario_mun_mediano + presion_demo + ln_pob)
formula_2 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom  + salario_mun_mediano + presion_demo + ln_pob + cluster)

# Modelos
lm_model_2 &lt;- lm(formula_2, data = data_mun15)
rlm_model_2 &lt;- rlm(formula_2, data = data_mun15)
sar_model_2 &lt;- errorsarlm(formula_2, data = data_mun15, listw = listw_5)
gls_model_2 &lt;- gls(formula_1,
                   data = data_mun15,
                   weights = varIdent(form = ~ 1 | cluster))
weighted_model_2 &lt;- lm(formula_2,
                       data = data_mun15, weights = peso)
gam_model_2 &lt;- gam(tasa_dfi ~ s(tasa_promedio_hom) + 
                     s(salario_mun_mediano) + s(presion_demo) + s(ln_pob) + cluster,
                   data = data_mun15)


# Función para obtener métricas y pruebas
model_metrics &lt;- function(model, model_name, listw = NULL) {
  # Summary del modelo
  model_summary &lt;- summary(model)
  
  # Coeficientes y p-values
  if (inherits(model, &quot;gls&quot;)) {
    coefficients &lt;- as.data.frame(coef(model_summary))
    coefficients$term &lt;- rownames(coefficients)
    rownames(coefficients) &lt;- NULL
    coefficients &lt;- coefficients %&gt;%
      rename(estimate = Value, `[std.error]` = Std.Error, p.value = `p-value`)
  } else if (inherits(model, &quot;gam&quot;)) {
    coefficients &lt;- summary(model)$s.table
    coefficients &lt;- as.data.frame(coefficients)
    coefficients$term &lt;- rownames(coefficients)
    rownames(coefficients) &lt;- NULL
    coefficients &lt;- coefficients %&gt;%
      rename(estimate = edf, `[std.error]` = Ref.df, p.value = `p-value`)
    if (!&quot;p.value&quot; %in% names(coefficients)) {
      coefficients$p.value &lt;- NA
    }
  } else {
    coefficients &lt;- tidy(model)
    coefficients &lt;- coefficients %&gt;%
      rename(`[std.error]` = std.error)
    if (!&quot;p.value&quot; %in% names(coefficients)) {
      coefficients$p.value &lt;- NA
    }
  }
  
  # AIC y BIC
  aic_value &lt;- AIC(model)
  bic_value &lt;- BIC(model)
  
  # Breusch-Pagan test
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;))) {
    bp_test &lt;- bptest(model)
    bp_stat &lt;- bp_test$statistic
    bp_pvalue &lt;- bp_test$p.value
  } else {
    bp_stat &lt;- NA
    bp_pvalue &lt;- NA
  }
  
  # Jarque-Bera test
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;, &quot;gls&quot;))) {
    jb_test &lt;- jarque.bera.test(residuals(model))
    jb_stat &lt;- jb_test$statistic
    jb_pvalue &lt;- jb_test$p.value
  } else {
    jb_stat &lt;- NA
    jb_pvalue &lt;- NA
  }
  
  # Prueba de multicolinealidad (VIF)
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;, &quot;gls&quot;))) {
    vif_values &lt;- vif(model)
    vif_df &lt;- as.data.frame(vif_values)
    vif_df$term &lt;- rownames(vif_df)
    rownames(vif_df) &lt;- NULL
    colnames(vif_df)[1] &lt;- &quot;vif&quot;
  } else {
    vif_df &lt;- data.frame(term = NA, vif = NA)
  }
  
  # Índice de Moran si se proporciona una estructura espacial
  if (!is.null(listw)) {
    moran_test &lt;- moran.test(residuals(model), listw)
    moran_i &lt;- moran_test$estimate[1]
    moran_p &lt;- moran_test$p.value
  } else {
    moran_i &lt;- NA
    moran_p &lt;- NA
  }
  
  # Consolidar resultados en una lista
  results &lt;- list(
    model_name = model_name,
    coefficients = coefficients,
    aic = aic_value,
    bic = bic_value,
    bp_stat = bp_stat,
    bp_pvalue = bp_pvalue,
    jb_stat = jb_stat,
    jb_pvalue = jb_pvalue,
    moran_i = moran_i,
    moran_p = moran_p,
    vif = vif_df
  )
  
  return(results)
}

# Obtener métricas para cada modelo
lm_results &lt;- model_metrics(lm_model_2, &quot;Linear Model (lm)&quot;)
rlm_results &lt;- model_metrics(rlm_model_2, &quot;Robust Linear Model (rlm)&quot;)
sar_results &lt;- model_metrics(sar_model_2, &quot;Spatial Autoregressive Model (sar)&quot;, listw_5)
gls_results &lt;- model_metrics(gls_model_2, &quot;Generalized Least Squares (gls)&quot;)
weighted_results &lt;- model_metrics(weighted_model_2, &quot;Weighted Linear Model (lm)&quot;)
gam_results &lt;- model_metrics(gam_model_2, &quot;Generalized Additive Model (gam)&quot;)

# Crear una tabla consolidada
consolidate_results &lt;- function(results_list) {
  consolidated &lt;- bind_rows(lapply(results_list, function(result) {
    result$coefficients %&gt;%
      mutate(model = result$model_name,
             aic = result$aic,
             bic = result$bic,
             bp_stat = result$bp_stat,
             bp_pvalue = result$bp_pvalue,
             jb_stat = result$jb_stat,
             jb_pvalue = result$jb_pvalue,
             moran_i = result$moran_i,
             moran_p = result$moran_p) %&gt;%
      dplyr::select(model, term, estimate, `[std.error]`, p.value, aic, bic, bp_stat, bp_pvalue, jb_stat, jb_pvalue, moran_i, moran_p)
  }))
  
  # Añadir VIF
  vif_df &lt;- bind_rows(lapply(results_list, function(result) {
    if (!is.na(result$vif$term[1])) {
      result$vif %&gt;%
        mutate(model = result$model_name) %&gt;%
        dplyr::select(model, term, vif)
    } else {
      data.frame(model = result$model_name, term = NA, vif = NA)
    }
  }))
  
  consolidated &lt;- left_join(consolidated, vif_df, by = c(&quot;model&quot;, &quot;term&quot;))
  
  return(consolidated)
}

# Consolidar todos los resultados
all_results &lt;- consolidate_results(list(lm_results, rlm_results, sar_results, gls_results, weighted_results, gam_results))

# Definir una función para agregar estrellas de significancia
add_significance_stars &lt;- function(p_value) {
  if (is.na(p_value)) {
    return(&quot;&quot;)
  } else if (p_value &lt; 0.001) {
    return(&quot;***&quot;)
  } else if (p_value &lt; 0.01) {
    return(&quot;**&quot;)
  } else if (p_value &lt; 0.05) {
    return(&quot;*&quot;)
  } else if (p_value &lt; 0.1) {
    return(&quot;.&quot;)
  } else {
    return(&quot;&quot;)
  }
}

# Agregar estrellas de significancia
all_results &lt;- all_results %&gt;%
  mutate(significance = sapply(p.value, add_significance_stars))

# Renombrar &quot;term&quot; como &quot;Variable&quot; y poner std.error entre corchetes
colnames(all_results)[colnames(all_results) == &quot;term&quot;] &lt;- &quot;Variable&quot;
colnames(all_results)[colnames(all_results) == &quot;std.error&quot;] &lt;- &quot;[std.error]&quot;

# Concatenar &quot;std.error&quot; a &quot;estimate&quot; y asignarlo a &quot;Coeficiente&quot;
all_results$estimate &lt;- round(all_results$estimate,5)
all_results$&#39;[std.error]&#39; &lt;- round(all_results$&#39;[std.error]&#39;,5)
all_results &lt;- all_results %&gt;%
  mutate(Coeficiente = paste0(estimate, &quot; [&quot;, `[std.error]`, &quot;]&quot;, significance))

# Eliminar columnas innecesarias
resultados_completos &lt;- all_results %&gt;%
  dplyr::select(-estimate, -`[std.error]`, -p.value, -significance)


# Crear df1
df1_2 &lt;- resultados_completos %&gt;% 
  dplyr::select(model, Variable, Coeficiente) %&gt;%
  pivot_wider(names_from = model, values_from = Coeficiente)


df2_2 &lt;- resultados_completos %&gt;% 
  dplyr::select(model, aic, bic, bp_stat, bp_pvalue, jb_stat, jb_pvalue, moran_i, moran_p) %&gt;%
  distinct() %&gt;%
  pivot_longer(cols = -model, names_to = &quot;Variable&quot;, values_to = &quot;value&quot;) %&gt;%
  pivot_wider(names_from = model, values_from = value)


# Unir df1_long y df2 por la columna &#39;model&#39;
combined_df2 &lt;- rbind(df1, df2)

# # # Guardar la tabla de resultados transpuesta en un archivo CSV
write.csv(combined_df2, &quot;Resultados/model_comparison_results_transposed_LINEAL.csv&quot;, row.names = FALSE)

# Crear la tabla kable
kable_table2 &lt;- combined_df2 %&gt;%
  kable(format = &quot;html&quot;, booktabs = TRUE) %&gt;%
  kable_styling(full_width = FALSE, position = &quot;center&quot;)

# Convertir la tabla kable a flextable
flex_table2 &lt;- combined_df2 %&gt;%
  regulartable() %&gt;%
  set_table_properties(width = .5, layout = &quot;autofit&quot;)

# Crear un documento Word y añadir la tabla flextable
doc2 &lt;- read_docx() %&gt;%
  body_add_flextable(flex_table2)

# Obtener la fecha actual en formato YYYYMMDD
current_date &lt;- format(Sys.Date(), &quot;%Y%m%d&quot;)

# Crear el nombre del archivo incluyendo la fecha
file_name2 &lt;- paste0(&quot;Resultados/modelos_old_gam_sam_LINEAL&quot;, current_date, &quot;.docx&quot;)

# Guardar el documento Word
print(doc2, target = file_name2)


# DIAGNÓSTICO DE MODELOS
# Definir la función para generar gráficos de diagnóstico
generate_diagnostic_plots &lt;- function(model, model_name) {
  # Obtener valores ajustados
  fitted_values &lt;- fitted(model)
  
  # Obtener residuos estandarizados del modelo
  residuals_std &lt;- scale(residuals(model))
  #residuals_norm &lt;- residuals(model, type=&quot;n&quot;)
  
  # Histograma de los residuos
  hist(residuals_std, main = paste(&quot;Histograma de Residuos (&quot;, model_name, &quot;)&quot;, sep = &quot;&quot;), 
       xlab = &quot;Residuos Estandarizados&quot;, breaks = 30)
  
  # QQ Plot de los residuos
  qqnorm(residuals_std, main = paste(&quot;QQ Plot de Residuos (&quot;, model_name, &quot;)&quot;, sep = &quot;&quot;),
         xlab = &quot;Observados&quot;, ylab = &quot;Valores esperados&quot;, pch = 20, col = &quot;black&quot;)
  qqline(residuals_std, col = &quot;red&quot;)
  
  # Gráfico de Valores Ajustados vs Residuos
  plot(fitted_values, residuals_std, main = paste(&quot;Ajustados vs Residuos Estandarizados (&quot;, model_name, &quot;)&quot;, sep = &quot;&quot;),
       xlab = &quot;Valores Ajustados&quot;, ylab = &quot;Residuos Estandarizados&quot;, pch = 20, col = &quot;black&quot;)
  abline(h = 0, col = &quot;red&quot;, lty = 2)
}

# Guardar todas las gráficas en un solo archivo PNG
png(&quot;diagnostic_plots_all_models.png&quot;, width = 2400, height = 3200)

# Configurar la disposición de los gráficos
par(mfrow = c(3, 3))  # Suponiendo 5 modelos y 3 gráficos por modelo

# Generar gráficos para cada modelo
generate_diagnostic_plots(lm_model_2, &quot;LM&quot;)
generate_diagnostic_plots(weighted_model_2, &quot;Weighted&quot;)
generate_diagnostic_plots(rlm_model_2, &quot;RLM&quot;)

# Guardar todas las gráficas en un solo archivo PNG
png(&quot;Resultados/graficas_diagnostico/diagnostic_plots_LM_models.png&quot;, width = 2400, height = 3200)
dev.off()
# Configurar la disposición de los gráficos
dev.new()
par(mfrow = c(2, 3))  # Suponiendo 5 modelos y 3 gráficos por modelo
generate_diagnostic_plots(sar_model_2, &quot;SAR&quot;)
generate_diagnostic_plots(gls_model_2, &quot;GLS&quot;)
# Guardar todas las gráficas en un solo archivo PNG
png(&quot;Resultados/graficas_diagnostico/diagnostic_plots_SAR_models.png&quot;, width = 2400, height = 3200)

# Cerrar el dispositivo gráfico
dev.off()</code></pre>
<p>3.2.1 Efectos marginales de la violencia sobre el DFI</p>
<p>Este código nos permite visualizar el efecto de la tasa promedio de
homicidios sobre la tasa DFI, tomando en cuenta los diferentes
agrupamientos en tus datos.</p>
<p>Se crea una secuencia de valores para tasa_promedio_hom que va de 0 a
1500 en incrementos de 50. Esta secuencia se utilizará para predecir la
tasa DFI en diferentes niveles de la tasa promedio de homicidios.
DataFrame vacío (pred_data):</p>
<p>Se inicializa un DataFrame vacío donde se almacenarán las
predicciones para cada combinación de agrupamiento (cluster) y valores
de tasa_promedio_hom.<br />
Bucle sobre los niveles de agrupamiento (cluster):</p>
<p>Se itera sobre cada nivel de cluster en el conjunto de datos
data_mun15. Para cada cluster, se calcula un nuevo DataFrame que
contiene las predicciones para la tasa DFI, basado en la media de las
demás variables (salario_mun_mediano, presion_demo, ln_pob) dentro de
ese cluster. Predicciones e intervalos de confianza (predict):</p>
<p>Se utilizan las secuencias generadas para predecir los valores de
tasa_dfi utilizando el modelo rlm_model_2. Se calculan y almacenan los
valores predichos junto con los intervalos de confianza. Concatenar las
predicciones (rbind):</p>
<p>Se agregan las predicciones para cada cluster al DataFrame principal
pred_data. Visualización del resultado (ggplot2):</p>
<p>Se utiliza ggplot2 para crear un gráfico de líneas con barras de
error, que muestra cómo la tasa_promedio_hom afecta la tasa_dfi
predicha, con diferentes colores para cada agrupamiento (cluster).</p>
<p>El gráfico incluye barras de error que representan los intervalos de
confianza de las predicciones.</p>
<pre class="r"><code># Crear una secuencia de valores de tasa_promedio_hom de 0 a 1500 en incrementos de 50
tasa_promedio_hom_seq &lt;- seq(0, 1500, by = 50)

# Inicializar un DataFrame vacío para almacenar las predicciones
pred_data &lt;- data.frame()

# Iterar sobre cada agrupamiento (cluster) para calcular los promedios y predicciones
for (cluster_level in levels(data_mun15$cluster)) {
  cluster_data &lt;- data_mun15[data_mun15$cluster == cluster_level, ]
  
  # Crear un nuevo DataFrame para las predicciones por agrupamiento
  new_data_cluster &lt;- data.frame(
    tasa_promedio_hom = tasa_promedio_hom_seq,
    tasa_prom_cuadrado = tasa_promedio_hom_seq^2, # Cuadrado de la secuencia
    salario_mun_mediano = mean(cluster_data$salario_mun_mediano),
    presion_demo = mean(cluster_data$presion_demo),
    ln_pob = mean(cluster_data$ln_pob),
    cluster = factor(cluster_level, levels = levels(data_mun15$cluster))
  )
  
  # Calcular las predicciones y los intervalos de confianza para este agrupamiento
  predictions &lt;- predict(rlm_model_2, newdata = new_data_cluster, interval = &quot;confidence&quot;)
  
  # Agregar las predicciones y los intervalos de confianza al DataFrame
  new_data_cluster$predicted_tasa_dfi &lt;- predictions[, &quot;fit&quot;]
  new_data_cluster$lower &lt;- predictions[, &quot;lwr&quot;]
  new_data_cluster$upper &lt;- predictions[, &quot;upr&quot;]
  
  # Agregar las predicciones al DataFrame principal
  pred_data &lt;- rbind(pred_data, new_data_cluster)
}

# Verificar el DataFrame resultante
head(pred_data)

# Crear el gráfico con ggplot2 usando geom_errorbar
ggplot(pred_data, aes(x = tasa_promedio_hom, y = predicted_tasa_dfi, color = cluster)) +
  geom_line(size = 1) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 10, position = position_dodge(0.1)) +
  labs(title = &quot;Efecto lineal de la Tasa Promedio de Homicidios sobre la Tasa DFI por Agrupamiento&quot;,
       x = &quot;Tasa Promedio de Homicidios&quot;,
       y = &quot;Tasa DFI Predicha&quot;) +
  theme_minimal() +
  theme(legend.position = &quot;bottom&quot;) +
  scale_color_discrete(name = &quot;Agrupamiento&quot;)</code></pre>
<p>3.4. Modelos sar con medida alternativa de violencia ajusta tres
modelos espaciales autoregresivos (SAR) utilizando diferentes medidas de
violencia (logaritmo de la tasa de homicidios y una tasa suavizada) como
variables predictoras, junto con otras variables socioeconómicas y
demográficas.</p>
<p>sar_model_log_1: Ajusta un modelo SAR donde la variable dependiente
es la tasa de desplazamiento forzado interno (tasa_dfi), y una de las
variables predictoras clave es el logaritmo de la tasa de homicidios
(log_th).</p>
<p>sar_model_log_2: Similar al primer modelo, pero utilizando una
versión suavizada de la tasa de homicidios (th_prom_suav) en lugar del
logaritmo.</p>
<p>sar_model_log_3: En este modelo, la variable dependiente es el
logaritmo de la tasa de desplazamiento forzado interno (log_tasa_dfi), y
la variable predictora es nuevamente el logaritmo de la tasa de
homicidios (log_th).</p>
<ol start="2" style="list-style-type: decimal">
<li>Obtención de valores ajustados y residuos: Valores ajustados
(fitted_values): Se calculan los valores ajustados para cada uno de los
tres modelos, que representan las tasas de desplazamiento forzado
interno predichas por los modelos.</li>
</ol>
<p>Residuos (residuals): Se calculan los residuos en la escala
logarítmica para cada modelo. Los residuos son la diferencia entre los
valores observados y los valores ajustados por el modelo. Estos residuos
pueden transformarse de nuevo a la escala original si fuera
necesario.</p>
<ol start="3" style="list-style-type: decimal">
<li>Errores y RMSE: Errores (errors): Se calculan los errores como la
diferencia entre los valores observados y los valores ajustados. Esto
proporciona una medida de qué tan bien el modelo se ajusta a los datos
observados.</li>
</ol>
<p>RMSE (Root Mean Squared Error): Es una medida de la magnitud de los
errores del modelo. Un RMSE más bajo indica un mejor ajuste del modelo a
los datos.</p>
<ol start="4" style="list-style-type: decimal">
<li>Estadísticas y pruebas de diagnóstico: R²: Se calcula el coeficiente
de determinación (R²) para cada modelo, que mide la proporción de la
varianza total de la variable dependiente que es explicada por el
modelo.</li>
</ol>
<p>AIC y BIC: Se calculan los criterios de información de Akaike (AIC) y
Bayesiano (BIC) para cada modelo. Estos criterios se utilizan para
comparar modelos, donde un valor más bajo indica un mejor ajuste
penalizando la complejidad del modelo.</p>
<p>Pruebas de Breusch-Pagan (bp_test): Se realizan para detectar
heterocedasticidad en los modelos, es decir, si la varianza de los
residuos es constante o no.</p>
<p>Pruebas de Jarque-Bera (jb_test): Se realizan para evaluar la
normalidad de los residuos de los modelos.</p>
<p>Índice de Moran (moran_test): Se calcula para evaluar la
autocorrelación espacial en los residuos. Un valor significativo indica
la presencia de autocorrelación espacial, lo que sugiere que el modelo
puede no haber capturado toda la estructura espacial en los datos.</p>
<p>Pruebas de Kolmogorov-Smirnov (ks_test): Se utilizan para comparar la
distribución de los residuos con una distribución normal.</p>
<ol start="5" style="list-style-type: decimal">
<li>Resumen y Exportación de Resultados: Creación de tablas resumen: Se
combinan los resultados de las pruebas de diagnóstico y las estadísticas
de los modelos en un único DataFrame.</li>
</ol>
<p>Exportación: Los resultados se exportan a un archivo CSV y un archivo
Word con tablas bien formateadas.</p>
<p>Este código permite evaluar cómo diferentes medidas de violencia y
enfoques de modelado afectan la capacidad de los modelos SAR para
predecir la tasa de desplazamiento forzado interno. Se realiza una
evaluación exhaustiva del ajuste del modelo y la calidad de los
residuos, lo que proporciona una visión integral de la robustez y
validez de los modelos propuestos.</p>
<pre class="r"><code>## MODELOS SAR CON MEDIDA ALTERNATIVA DE VIOLENCIA
# Ajuste de lo smodelos SAR
sar_model_log_1 &lt;- errorsarlm(tasa_dfi ~ log_th + coef_de_gini +
                                salario_mun_mediano + presion_demo + ln_pob + cluster,
                              data = data_mun15, listw = listw_5)

sar_model_log_2 &lt;- errorsarlm(tasa_dfi ~ th_prom_suav + coef_de_gini +
                                salario_mun_mediano + presion_demo + ln_pob + cluster,
                              data = data_mun15, listw = listw_5)

sar_model_log_3 &lt;- errorsarlm(log_tasa_dfi ~ log_th + coef_de_gini +
                                salario_mun_mediano + presion_demo + ln_pob + cluster,
                              data = data_mun15, listw = listw_5)

### Resúmenes de los modelos y pruebas diagnóstico

# Obtener los valores ajustados de ambos modelos
fitted_values_sar_1 &lt;- fitted(sar_model_log_1)
fitted_values_sar_2 &lt;- fitted(sar_model_log_2)
fitted_values_sar_3 &lt;- fitted(sar_model_log_3)

# Obtener los residuos en la escala logarítmica
residuals_sar_1_log &lt;- residuals(sar_model_log_1)
residuals_sar_2_log &lt;- residuals(sar_model_log_2)
residuals_sar_3_log &lt;- residuals(sar_model_log_3)

# Transformar los residuos de vuelta a la escala original
# Nota: La transformación inversa correcta para los residuos logarítmicos es exp(residuales) - 1
# Esta transformación es consistente con la transformación de los valores ajustados.

# # Residuos transformados 
# residuals_sar_1 &lt;- exp(residuals_sar_1_log) - 1
# residuals_sar_2 &lt;- exp(residuals_sar_2_log) - 1
# residuals_sar_3 &lt;- exp(residuals_sar_3_log) - 1

# Calcular los errores de ambos modelos
errors_sar_1 &lt;- data_mun15$tasa_dfi - fitted_values_sar_1
errors_sar_2 &lt;- data_mun15$tasa_dfi - fitted_values_sar_2
errors_sar_3 &lt;- data_mun15$log_tasa_dfi - fitted_values_sar_3

# Calcular el RMSE para ambos modelos
RMSE_sar_1 &lt;- sqrt(mean(errors_sar_1^2))
RMSE_sar_2 &lt;- sqrt(mean(errors_sar_2^2))
RMSE_sar_3 &lt;- sqrt(mean(errors_sar_3^2))
# print(paste(&quot;RMSE del modelo SAR Logarítmico (Aproximación A):&quot;, RMSE_log_1))

# Lista de modelos
modelos_esp &lt;- list(sar_model_log_1, sar_model_log_2, sar_model_log_3)

# Resúmenes de los modelos
summary_mod_esp &lt;- lapply(modelos_esp, summary)

# Inicializar una lista para almacenar los R^2
r_squared_list &lt;- list()

# Iterar sobre cada modelo para calcular el R^2
for (i in 1:length(modelos_esp)) {
  modelo &lt;- modelos_esp[[i]]  # Corregir la lista de modelos
  
  # Valores observados y valores ajustados
  valores_observados &lt;- modelo$y
  valores_ajustados &lt;- fitted(modelo)
  
  # Suma de los cuadrados totales (SST)
  sst &lt;- sum((valores_observados - mean(valores_observados))^2)
  
  # Suma de los cuadrados residuales (SSR)
  ssr &lt;- sum(residuals(modelo)^2)
  
  # R^2
  r_squared &lt;- 1 - (ssr / sst)
  
  # Almacenar el R^2 en la lista
  r_squared_list[[i]] &lt;- r_squared
}

# Convertir la lista a un vector
r_squared_vector &lt;- unlist(r_squared_list)

# Imprimir los resultados
print(r_squared_vector)


# AIC y BIC
aic_mod_esp &lt;- sapply(modelos_esp, AIC)
bic_mod_esp &lt;- sapply(modelos_esp, BIC)

# Aplicar la prueba de Breusch-Pagan a cada modelo
bp_test_mod_esp &lt;- lapply(modelos_esp, function(mod) bptest.Sarlm(mod))

# Prueba Jarque-Bera para normalidad de los residuos
jb_test_mod_esp &lt;- lapply(modelos_esp, function(mod) jarque.bera.test(residuals(mod)))

# Indice de Moran
moran_test_mod_esp &lt;- lapply(modelos_esp, function(mod) moran.test(residuals(mod), listw = listw_5))

# Test de Kolmogorov-Smirnov para verificar la distribución de los residuos
ks_test_mod_esp &lt;- lapply(modelos_esp, function(mod) ks.test(residuals(mod), &quot;pnorm&quot;))

# Calcular el número de observaciones para cada modelo
num_observations_esp &lt;- sapply(modelos_esp, function(mod) length(residuals(mod)))

# Crear dataframe con los resultados
resultados_tests_esp &lt;- data.frame(
  Modelo = paste0(&quot;Mod_SAR&quot;, 1:3),
  AIC = aic_mod_esp,
  BIC = bic_mod_esp,
  JB_p_value = sapply(jb_test_mod_esp, function(jb_test) jb_test$p.value),
  KS_p_value = sapply(ks_test_mod_esp, function(ks_test) ks_test$p.value),
  BP_p_value = sapply(bp_test_mod_esp, function(test) test$p.value),
  MoranIndex = sapply(moran_test_mod_esp, function(moran_test) moran_test$p.value),
  Num_Observaciones = num_observations_esp,
  R2 = r_squared_vector,
  #Condition_Index = c(condition_number_model_1, condition_number_model_2),
  RMSE = c(RMSE_sar_1, RMSE_sar_2, RMSE_sar_3)
)

print(resultados_tests_esp)

# Agregar R2 a las otras métricas de valoración del modelo
resultados_tests_esp$R2 &lt;- r_squared_vector

# # Agregar los valores de Condition_Number al dataframe resultados_tests_esp
# resultados_tests_esp$Condition_Index &lt;- c(condition_number_model_1, condition_number_model_2)

# Transponer el dataframe y asignar nombres de columnas
resultados_transpuesto_esp &lt;- t(resultados_tests_esp)
colnames(resultados_transpuesto_esp) &lt;- resultados_transpuesto_esp[1, ]
resultados_transpuesto_esp &lt;- as.data.frame(resultados_transpuesto_esp[-1, ])
resultados_transpuesto_esp$Variable &lt;- rownames(resultados_transpuesto_esp)

#Función para agregar estrellas de significancia
add_significance_stars &lt;- function(summary_df) {
  summary_df$significance &lt;- cut(summary_df$p.value, breaks = c(-Inf, 0.001, 0.01, 0.05, Inf), labels = c(&quot;***&quot;, &quot;**&quot;, &quot;*&quot;, &quot;&quot;), include.lowest = TRUE)
  return(summary_df)
}

# Obtén los resúmenes de los modelos y agrega estrellas de significancia
resumen_m1 &lt;- add_significance_stars(tidy(summary(sar_model_log_1)))
resumen_m2 &lt;- add_significance_stars(tidy(summary(sar_model_log_2)))
resumen_m3 &lt;- add_significance_stars(tidy(summary(sar_model_log_3)))

# Agrega los nombres de los modelos como una nueva columna
resumen_m1$modelo &lt;- &quot;Mod_SAR1&quot;
resumen_m2$modelo &lt;- &quot;Mod_SAR2&quot;
resumen_m3$modelo &lt;- &quot;Mod_SAR3&quot;

# Combina los resúmenes en una sola tabla
resultados_completos &lt;- rbind(resumen_m1, resumen_m2, resumen_m3)

# Redondear todos los valores a 3 decimales y pegar &quot;significance&quot; a &quot;estimate&quot;
resultados_completos$estimate &lt;- round(resultados_completos$estimate, 5)
resultados_completos$std.error &lt;- round(resultados_completos$std.error, 8)
resultados_completos$significance &lt;- as.character(resultados_completos$significance)
resultados_completos$estimate &lt;- paste0(resultados_completos$estimate, resultados_completos$significance)

# Renombrar &quot;term&quot; como &quot;Variable&quot; y poner std.error entre corchetes
colnames(resultados_completos)[colnames(resultados_completos) == &quot;term&quot;] &lt;- &quot;Variable&quot;
colnames(resultados_completos)[colnames(resultados_completos) == &quot;std.error&quot;] &lt;- &quot;[std.error]&quot;

# # Eliminar las columnas p.value y std.error cuando modelo == m1 o m2
# resultados_completos &lt;- resultados_completos %&gt;%
#   filter(!(modelo == &quot;m1&quot; &amp; grepl(&quot;Mod_SAR1&quot;, Variable)) &amp; !(modelo == &quot;m2&quot; &amp; grepl(&quot;Mod_SAR2&quot;, Variable)))

# Concatenar &quot;std.error&quot; a &quot;estimate&quot; y asignarlo a &quot;Coeficiente&quot;
resultados_completos$Coeficiente &lt;- paste0(resultados_completos$estimate, &quot; [&quot;, resultados_completos$`[std.error]`, &quot;]&quot;, sep = &quot;\n&quot;)

# Eliminar columnas innecesarias
resultados_completos &lt;- resultados_completos %&gt;%
  dplyr::select(-estimate, -`[std.error]`, -p.value, -statistic, -significance)

# Recodificar los nombres de la variable modelo
# resultados_completos$modelo &lt;- recode(resultados_completos$modelo, &quot;m1&quot;=&quot;Mod_SAR1&quot;, &quot;m2&quot;=&quot;Mod_SAR2&quot;)


# Pivotar los datos para que cada columna corresponda a una de las dos categorías de modelo y los valores sean tomados de Coeficiente
resultados_pivot &lt;- resultados_completos %&gt;%
  pivot_wider(names_from = modelo, values_from = Coeficiente)

# Verificar el resultado
head(resultados_pivot)

tabla_mod_sar_log &lt;- rbind(resultados_pivot, resultados_transpuesto_esp)

#Escribir el dataframe en un archivo CSV
write.csv(tabla_mod_sar_log, &quot;Resultados/tabla_modelos_sar_log2_2.csv&quot;, row.names = TRUE, fileEncoding = &quot;latin1&quot;)


# Crear una tabla flextable a partir del dataframe
ft &lt;- flextable(tabla_mod_sar_log)

# Personaliza la tabla (opcional)
ft &lt;- ft %&gt;%
  set_header_labels(values = colnames(tabla_mod_sar_log)) %&gt;% # Asegura que los nombres de las columnas sean correctos
  theme_vanilla() %&gt;% 
  autofit()

# Obtener la fecha actual en formato YYYYMMDD
current_date &lt;- format(Sys.Date(), &quot;%Y%m%d&quot;)

# Crear el nombre del archivo incluyendo la fecha
file_name3 &lt;- paste0(&quot;Resultados/tabla_modelos_sar_log_&quot;, current_date, &quot;.docx&quot;)

# Crear un documento Word y añadir la tabla
doc3 &lt;- read_docx() %&gt;%
  body_add_flextable(ft)

# Guardar el documento Word con el nombre dinámico
print(doc3, target = file_name3)</code></pre>
<p>Diagnostico de los modelos</p>
<pre class="r"><code># Visualización de los Residuos
#dev.new()
par(mfrow = c(3, 2))

# Guardar el primer conjunto de gráficos en un archivo PNG
png(&quot;Resultados/graficas_diagnostico/sar_residuos_histogramas_qqplots_240528.png&quot;, width = 1200, height = 800)

# Histograma de los residuos del primer modelo
hist(residuals(sar_model_log_1), main = &quot;Histograma de Residuos (SAR No lineal)&quot;, xlab = &quot;Residuos Log&quot;, breaks = 30)

# QQ Plot de los residuos del primer modelo
qqnorm(residuals(sar_model_log_1), main = &quot;QQ Plot de Residuos (SAR No lineal)&quot;)
qqline(residuals(sar_model_log_1), col = &quot;red&quot;)

# Histograma de los residuos del segundo modelo
hist(residuals(sar_model_log_2), main = &quot;Histograma de Residuos (SAR Log-log)&quot;, xlab = &quot;Residuos Log&quot;, breaks = 30)

# QQ Plot de los residuos del segundo modelo
qqnorm(residuals(sar_model_log_2), main = &quot;QQ Plot de Residuos (SAR Log-log)&quot;)
qqline(residuals(sar_model_log_2), col = &quot;red&quot;)

# Histograma de los residuos del segundo modelo
hist(residuals(sar_model_log_3), main = &quot;Histograma de Residuos (SAR Log-log Smooth)&quot;, xlab = &quot;Residuos Log&quot;, breaks = 30)

# QQ Plot de los residuos del segundo modelo
qqnorm(residuals(sar_model_log_3), main = &quot;QQ Plot de Residuos (SAR Log-log Smooth)&quot;)
qqline(residuals(sar_model_log_3), col = &quot;red&quot;)


# Cerrar el dispositivo gráfico
dev.off()


# Establecer la ruta de salida
# pathOutput &lt;- &quot;/00_DFI/Resultados/&quot;


# Mapa de residuos
#dev.new()
par(mfrow = c(2, 3))

# Guardar segundo conjunto de gráficos en un archivo PNG
#png(&quot;residuos_sar_mapa_240528_2.png&quot;, width = 1200, height = 800)
# Valores Ajustados vs Observados
plot(fitted_values_sar_1, data_mun15$tasa_dfi, main = &quot;Valores Ajustados vs Observados (SAR - No lineal)&quot;,
     xlab = &quot;Valores Observados&quot;, ylab = &quot;Valores Ajustados&quot;)
abline(0, 1, col = &quot;red&quot;, lty = 2)

plot(fitted_values_sar_2, data_mun15$tasa_dfi, main = &quot;Valores Ajustados vs Observados (SAR - Log-log))&quot;,
     xlab = &quot;Valores Observados&quot;, ylab = &quot;Valores Ajustados&quot;)
abline(0, 1, col = &quot;red&quot;, lty = 2)

plot(fitted_values_sar_3, data_mun15$tasa_dfi, main = &quot;Valores Ajustados vs Observados (SAR - Log -log smooth)&quot;,
     xlab = &quot;Valores Observados&quot;, ylab = &quot;Valores Ajustados&quot;)
abline(0, 1, col = &quot;red&quot;, lty = 2)</code></pre>
<p>3.5 Modelos SAR para Variables de Migración y Empleo</p>
<p>Se realiza un análisis exhaustivo de cómo diferentes variables
relacionadas con la migración y el empleo pueden ser modeladas
utilizando un enfoque SAR, que considera la autocorrelación espacial
entre observaciones. Se evalúan múltiples modelos con criterios
estadísticos rigurosos (como AIC, BIC, y R²) y pruebas de diagnóstico
(como Breusch-Pagan, Jarque-Bera, y Moran’s I), proporcionando una
visión completa de la adecuación y robustez de los modelos propuestos.
Los residuos del modelo se visualizan para identificar posibles
problemas con la normalidad o patrones espaciales no capturados, lo que
es crucial para asegurar la validez de los resultados.</p>
<pre class="r"><code># Ajuste de los modelos SAR
sar_model_log_1 &lt;- errorsarlm(tasa_migracion ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster, data = data_mun15, listw = listw_5)

sar_model_log_2 &lt;- errorsarlm(tasa_trabajo_b ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster, data = data_mun15, listw = listw_5)

sar_model_log_3 &lt;- errorsarlm(tasa_familia ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster, data = data_mun15, listw = listw_5)

sar_model_log_4 &lt;- errorsarlm(tasa_caso ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster, data = data_mun15, listw = listw_5)

sar_model_log_5 &lt;- errorsarlm(tasa_estudiar ~ tasa_promedio_hom + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster, data = data_mun15, listw = listw_5)


# Lista de modelos
modelos_esp &lt;- list(sar_model_log_1, sar_model_log_2, sar_model_log_3, sar_model_log_4, sar_model_log_5)

# Resúmenes de los modelos
summary_mod_esp &lt;- lapply(modelos_esp, summary)

# Inicializar una lista para almacenar los R^2
r_squared_list &lt;- list()

# Iterar sobre cada modelo para calcular el R^2
for (i in 1:length(modelos_esp)) {
  modelo &lt;- modelos_esp[[i]]
  valores_observados &lt;- modelo$y
  valores_ajustados &lt;- fitted(modelo)
  sst &lt;- sum((valores_observados - mean(valores_observados))^2)
  ssr &lt;- sum(residuals(modelo)^2)
  r_squared &lt;- 1 - (ssr / sst)
  r_squared_list[[i]] &lt;- r_squared
}

# Convertir la lista a un vector
r_squared_vector &lt;- unlist(r_squared_list)

# Calcular AIC y BIC
aic_mod_esp &lt;- sapply(modelos_esp, AIC)
bic_mod_esp &lt;- sapply(modelos_esp, BIC)

# Aplicar la prueba de Breusch-Pagan a cada modelo
library(lmtest)
bp_test_mod_esp &lt;- lapply(modelos_esp, function(mod) bptest.Sarlm(mod))

# Prueba Jarque-Bera para normalidad de los residuos
library(tseries)
jb_test_mod_esp &lt;- lapply(modelos_esp, function(mod) jarque.bera.test(residuals(mod)))

# Índice de Moran
library(spdep)
moran_test_mod_esp &lt;- lapply(modelos_esp, function(mod) moran.test(residuals(mod), listw = listw_5))

# Test de Kolmogorov-Smirnov para verificar la distribución de los residuos
ks_test_mod_esp &lt;- lapply(modelos_esp, function(mod) ks.test(residuals(mod), &quot;pnorm&quot;))

# Calcular el número de observaciones para cada modelo
num_observations_esp &lt;- sapply(modelos_esp, function(mod) length(residuals(mod)))

# Crear dataframe con los resultados
resultados_tests_esp &lt;- data.frame(
  Modelo = paste0(&quot;Mod_SAR&quot;, 1:5),
  AIC = aic_mod_esp,
  BIC = bic_mod_esp,
  JB_p_value = sapply(jb_test_mod_esp, function(jb_test) jb_test$p.value),
  KS_p_value = sapply(ks_test_mod_esp, function(ks_test) ks_test$p.value),
  BP_p_value = sapply(bp_test_mod_esp, function(test) test$p.value),
  MoranIndex = sapply(moran_test_mod_esp, function(moran_test) moran_test$p.value),
  Num_Observaciones = num_observations_esp,
  R2 = r_squared_vector
)

print(resultados_tests_esp)


# Agregar R2 a las otras métricas de valoración del modelo
resultados_tests_esp$R2 &lt;- r_squared_vector

# # Agregar los valores de Condition_Number al dataframe resultados_tests_esp
# resultados_tests_esp$Condition_Index &lt;- c(condition_number_model_1, condition_number_model_2)

# Transponer el dataframe y asignar nombres de columnas
resultados_transpuesto_esp &lt;- t(resultados_tests_esp)
colnames(resultados_transpuesto_esp) &lt;- resultados_transpuesto_esp[1, ]
resultados_transpuesto_esp &lt;- as.data.frame(resultados_transpuesto_esp[-1, ])
resultados_transpuesto_esp$Variable &lt;- rownames(resultados_transpuesto_esp)

# Función para obtener el resumen del modelo y agregar estrellas de significancia
get_model_summary &lt;- function(model, model_name) {
  summary_df &lt;- tidy(summary(model))
  summary_df$significance &lt;- cut(summary_df$p.value, breaks = c(-Inf, 0.001, 0.01, 0.05, Inf), labels = c(&quot;***&quot;, &quot;**&quot;, &quot;*&quot;, &quot;&quot;), include.lowest = TRUE)
  summary_df$modelo &lt;- model_name
  return(summary_df)
}

# Lista de modelos y nombres
modelos &lt;- list(sar_model_log_1, sar_model_log_2, sar_model_log_3, sar_model_log_4, sar_model_log_5)
modelos_nombres &lt;- c(&quot;Mod_SAR1&quot;, &quot;Mod_SAR2&quot;, &quot;Mod_SAR3&quot;, &quot;Mod_SAR4&quot;, &quot;Mod_SAR5&quot;)

# Obtener los resúmenes de los modelos y agregar estrellas de significancia
resumenes &lt;- lapply(seq_along(modelos), function(i) {
  get_model_summary(modelos[[i]], modelos_nombres[i])
})

# Combinar los resúmenes en una sola tabla
resultados_completos &lt;- do.call(rbind, resumenes)

# Redondear todos los valores a 3 decimales y pegar &quot;significance&quot; a &quot;estimate&quot;
resultados_completos$estimate &lt;- round(resultados_completos$estimate, 5)
resultados_completos$std.error &lt;- round(resultados_completos$std.error, 8)
resultados_completos$significance &lt;- as.character(resultados_completos$significance)
resultados_completos$estimate &lt;- paste0(resultados_completos$estimate, resultados_completos$significance)

# Renombrar &quot;term&quot; como &quot;Variable&quot; y poner std.error entre corchetes
colnames(resultados_completos)[colnames(resultados_completos) == &quot;term&quot;] &lt;- &quot;Variable&quot;
colnames(resultados_completos)[colnames(resultados_completos) == &quot;std.error&quot;] &lt;- &quot;[std.error]&quot;

# Concatenar &quot;std.error&quot; a &quot;estimate&quot; y asignarlo a &quot;Coeficiente&quot;
resultados_completos$Coeficiente &lt;- paste0(resultados_completos$estimate, &quot; [&quot;, resultados_completos$`[std.error]`, &quot;]&quot;, sep = &quot;\n&quot;)

# Eliminar columnas innecesarias
resultados_completos &lt;- resultados_completos %&gt;%
  dplyr::select(-estimate, -`[std.error]`, -p.value, -statistic, -significance)

# Pivotar los datos para que cada columna corresponda a una de las dos categorías de modelo y los valores sean tomados de Coeficiente
resultados_pivot &lt;- resultados_completos %&gt;%
  pivot_wider(names_from = modelo, values_from = Coeficiente)

# Verificar el resultado
head(resultados_pivot)

tabla_mod_sar_log &lt;- rbind(resultados_pivot, resultados_transpuesto_esp)

# Escribir el dataframe en un archivo CSV
write.csv(tabla_mod_sar_log, &quot;Resultados/tabla_modelos_sar_emigraciones.csv&quot;, row.names = FALSE)


# Crear la tabla flextable a partir del dataframe
ft4 &lt;- flextable(tabla_mod_sar_log)

# Personalizar la tabla (opcional)
ft4 &lt;- ft4 %&gt;%
  set_header_labels(values = colnames(tabla_mod_sar_log)) %&gt;% # Asegura que los nombres de las columnas sean correctos
  theme_vanilla() %&gt;% 
  autofit()

# Obtener la fecha actual en formato YYYYMMDD
current_date &lt;- format(Sys.Date(), &quot;%Y%m%d&quot;)

# Crear el nombre del archivo incluyendo la fecha
file_name4 &lt;- paste0(&quot;Resultados/tabla_modelos_sar_emigraciones_&quot;, current_date, &quot;.docx&quot;)

# Crear un documento Word y añadir la tabla
doc4 &lt;- read_docx() %&gt;%
  body_add_flextable(ft4)

# Guardar el documento Word con el nombre dinámico
print(doc4, target = file_name4)

# Visualización de los Residuos
par(mfrow = c(5, 2))

# Histograma y QQ Plot de los residuos para cada modelo
for (i in 1:length(modelos_esp)) {
  model &lt;- modelos_esp[[i]]
  hist(residuals(model), main = paste(&quot;Histograma de Residuos (Modelo&quot;, i, &quot;)&quot;), xlab = &quot;Residuos&quot;, breaks = 30)
  qqnorm(residuals(model), main = paste(&quot;QQ Plot de Residuos (Modelo&quot;, i, &quot;)&quot;))
  qqline(residuals(model), col = &quot;red&quot;)
}</code></pre>
<p>3.6 Modelo sin municipios atípicos</p>
<p>Se realiza un análisis robusto de los modelos espaciales
autoregresivos (SAR) ajustados a los datos, con un enfoque en la
identificación y tratamiento de observaciones atípicas a través del
cálculo de distancias robustas de Mahalanobis. Luego de eliminar estas
observaciones atípicas, se ajustan y evalúan varios modelos
estadísticos. Las métricas obtenidas, como AIC, BIC, R², y pruebas de
diagnóstico como Breusch-Pagan y Moran’s I, permiten una evaluación
completa de la calidad y adecuación de los modelos. Finalmente, los
resultados se exportan de manera ordenada para su posterior
análisis.</p>
<p>3.6.1 Cálculo de Distancias Robustas de Mahalanobis: Se calculan los
residuos del modelo SAR (sar_model_2). Estos residuos representan las
diferencias entre los valores observados y los valores predichos por el
modelo.</p>
<p>Distancias robustas de Mahalanobis: Utilizando los residuos del
modelo, se calculan las distancias robustas de Mahalanobis. Estas
distancias se utilizan para identificar observaciones atípicas, teniendo
en cuenta la correlación entre las variables.</p>
<p>Asignación de distancias al conjunto de datos: Las distancias
calculadas se asignan a una nueva columna en el conjunto de datos
data_mun15, llamada distancia_maha_robusta.</p>
<p>3.6.2. Ajuste de Distribuciones a las Distancias Robustas: Ajuste de
diferentes distribuciones (fitdist, fitdistr): Se ajustan varias
distribuciones a las distancias robustas de Mahalanobis, incluyendo la
distribución normal, gamma, cauchy, chi-cuadrado, t de Student,
log-normal y Weibull.</p>
<p>Evaluación de los ajustes: Aunque parte de la evaluación se comenta,
el código sugiere que se calculan los criterios de información AIC y BIC
para cada distribución ajustada, con el fin de comparar qué distribución
se ajusta mejor a las distancias robustas.</p>
<p>Ajuste y uso de la distribución log-normal: Se selecciona la
distribución log-normal para continuar con el análisis. Se calculan los
valores p utilizando esta distribución, que permiten evaluar la
significancia de las distancias calculadas.</p>
<p>3.6.3. Identificación y Filtrado de Municipios Atípicos:<br />
Filtrado de municipios atípicos: Se identifican municipios atípicos
basados en los valores p calculados previamente. Los municipios con
valores p por debajo de 0.025 o por encima de 0.975 son considerados
atípicos.</p>
<p>Creación de un nuevo conjunto de datos sin atípicos: Se elimina del
conjunto de datos original (data_mun15) aquellos municipios que fueron
identificados como atípicos, generando un nuevo conjunto de datos
(data_mun15_sin_atipicos) para análisis posteriores.</p>
<p>3.6.4. Ajuste de Modelos sin Observaciones Atípicas: Construcción de
la matriz espacial: Se reconstruye la matriz espacial (listw_5_s_out)
para los datos sin observaciones atípicas, necesaria para ajustar
modelos espaciales.</p>
<p>Ajuste de varios modelos: Se ajustan varios tipos de modelos sobre
los datos sin observaciones atípicas, incluyendo:</p>
<p>Modelo lineal (lm_model_2): Un modelo de regresión lineal.<br />
Modelo robusto (rlm_model_2): Un modelo de regresión lineal robusto a
outliers.<br />
Modelo autoregresivo espacial (sar_model_2): Un modelo SAR que tiene en
cuenta la autocorrelación espacial.<br />
Modelo de mínimos cuadrados generalizados (gls_model_2): Un modelo GLS
que permite heterocedasticidad.<br />
Modelo aditivo generalizado (gam_model_2): Un GAM que permite relaciones
no lineales entre las variables.</p>
<p>3.6.5. Evaluación de los Modelos:<br />
Métricas y pruebas: Se utiliza una función model_metrics para calcular
varias métricas y pruebas de diagnóstico para cada modelo,
incluyendo:<br />
AIC y BIC: Criterios de información para evaluar la calidad del ajuste
del modelo.<br />
Prueba de Breusch-Pagan: Para detectar heterocedasticidad.<br />
Prueba de Jarque-Bera: Para evaluar la normalidad de los residuos.<br />
Índice de Moran: Para detectar autocorrelación espacial en los
residuos.<br />
VIF: Factor de inflación de la varianza para evaluar la
multicolinealidad.</p>
<p>3.6.6. Consolidación y Exportación de Resultados:<br />
Consolidación de resultados: Los resultados de las pruebas y métricas se
consolidan en un único data frame, que se transforma para ser exportado
y analizado.</p>
<p>Exportación a CSV y Word: Los resultados se exportan a un archivo CSV
y se formatean en una tabla en un documento Word, lo que facilita su
presentación y análisis.</p>
<pre class="r"><code>### Distancias robustas de Mahalanobis

# Calcular los residuos del modelo robusto
residuales_sar_2 &lt;- residuals(sar_model_2)

# Convertir los residuos en un vector
residuales_sar_2_vector &lt;- as.vector(residuales_sar_2)

mcd &lt;- covMcd(residuales_sar_2_vector)
distancias_robustas &lt;- sqrt((residuales_sar_2_vector - mcd$center)^2 / mcd$cov)

# Asignar las distancias robustas a las filas completas
data_mun15$distancia_maha_robusta[1:length(distancias_robustas)] &lt;- distancias_robustas



# Paso 2: Ajustar diferentes distribuciones a las distancias robustas
ajuste_normal &lt;- fitdist(distancias_robustas, &quot;norm&quot;)
ajuste_normal &lt;- fitdist(distancias_robustas, &quot;norm&quot;)
ajuste_gamma &lt;- fitdistr(distancias_robustas, &quot;gamma&quot;)
ajuste_cauchy &lt;- fitdistr(distancias_robustas, &quot;cauchy&quot;)
ajuste_chisq &lt;- fitdist(distancias_robustas, &quot;chisq&quot;, start = list(df = 2))
ajuste_student_t &lt;- fitdist(distancias_robustas, &quot;t&quot;, start = list(df = 2))
ajuste_lognormal &lt;- fitdist(distancias_robustas, &quot;lnorm&quot;)
ajuste_weibull &lt;- fitdistr(distancias_robustas, &quot;weibull&quot;)

# Lista de ajustes
ajustes &lt;- list(
  Normal = ajuste_normal,
  Gamma = ajuste_gamma,
  Cauchy = ajuste_cauchy,
  ChiSquare = ajuste_chisq,
  StudentT = ajuste_student_t,
  LogNormal = ajuste_lognormal,
  Weibull = ajuste_weibull
)

# # Paso 3: Evaluar los ajustes
# 
# # Pruebas Gráficas
# par(mfrow = c(3, 3))
# for (nombre in names(ajustes)) {
#   plot(ajustes[[nombre]], main = paste(&quot;Ajuste&quot;, nombre))
# }
# 
# # Pruebas de Ajuste de Máxima Verosimilitud
# aic_values &lt;- sapply(ajustes, function(x) x$aic)
# bic_values &lt;- sapply(ajustes, function(x) x$bic)
# 
# # Mostrar los valores AIC y BIC
# print(aic_values)
# print(bic_values)


# Ajustar la distribución de Weibull a las distancias robustas
ajuste_lognormal &lt;- fitdist(distancias_robustas, &quot;lnorm&quot;)

# Supongamos que ya tienes las distancias robustas y has ajustado la distribución log-normal

# Definir los parámetros de la distribución log-normal a partir del ajuste
shape &lt;- ajuste_lognormal$estimate[&quot;meanlog&quot;]
scale &lt;- ajuste_lognormal$estimate[&quot;sdlog&quot;]

# Calcular los valores p utilizando la distribución log-normal
p_values &lt;- plnorm(distancias_robustas, meanlog = shape, sdlog = scale, lower.tail = FALSE)


# Calcular los p-valores para las distancias robustas
# Utilizar la distribución log-normal para calcular los p-valores
p_values &lt;- plnorm(distancias_robustas, lower.tail = FALSE)

p_values &lt;- plnorm(distancias_robustas, shape, scale, lower.tail = FALSE)

# Añadir los p-valores al dataframe
data_mun15$p_values_lnorm &lt;- NA
data_mun15$p_values_lnorm[1:length(p_values)] &lt;- p_values


# Definir los límites superior e inferior para los p-valores
limite_inferior &lt;- 0.025
limite_superior &lt;- 0.975

# Filtrar los municipios atípicos
municipios_atipicos &lt;- data_mun15[data_mun15$p_values_lnorm  &lt; limite_inferior | data_mun15$p_values_lnorm  &gt; limite_superior, ]

# Ver los municipios atípicos
head(municipios_atipicos)

# Crear un nuevo dataframe sin los municipios atípicos
data_mun15_sin_atipicos &lt;- data_mun15[!(data_mun15$p_values_lnorm &lt; limite_inferior | data_mun15$p_values_lnorm &gt; limite_superior), ]


### MODELOS SIN ATIPICOS

# Construccion de matriz espacial 
mexico.geom &lt;- st_geometry(data_mun15_sin_atipicos)
mexico.coords &lt;- st_centroid(mexico.geom) 
mex5_vecinos = knn2nb(knearneigh(mexico.coords, k = 5))
listw_5_s_out &lt;- nb2listw(mex5_vecinos, style = &quot;W&quot;, zero.policy = TRUE)

# Crear la columna &#39;peso&#39; como la inversa de &#39;Población&#39;
data_mun15_sin_atipicos$peso &lt;- 1 / data_mun15_sin_atipicos$poblacion

# Fórmulas para modelos
formula_1 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom  + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob)
formula_2 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom  + tasa_prom_cuadrado + salario_mun_mediano + presion_demo + ln_pob + cluster)


# Fórmulas para modelos
formula_1 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom   + salario_mun_mediano + presion_demo + ln_pob)
formula_2 &lt;- as.formula(tasa_dfi ~ tasa_promedio_hom   + salario_mun_mediano + presion_demo + ln_pob + cluster)

# Modelos
lm_model_2 &lt;- lm(formula_2, data = data_mun15_sin_atipicos)
rlm_model_2 &lt;- rlm(formula_2, data = data_mun15_sin_atipicos)
sar_model_2 &lt;- errorsarlm(formula_2, data = data_mun15_sin_atipicos, listw = listw_5_s_out)
gls_model_2 &lt;- gls(formula_1,
                   data = data_mun15_sin_atipicos,
                   weights = varIdent(form = ~ 1 | cluster))
weighted_model_2 &lt;- lm(formula_2,
                       data = data_mun15_sin_atipicos, weights = peso)
gam_model_2 &lt;- gam(tasa_dfi ~ s(tasa_promedio_hom) + s(tasa_prom_cuadrado) +
                     s(salario_mun_mediano) + s(presion_demo) + s(ln_pob) + cluster,
                   data = data_mun15_sin_atipicos)


# Función para obtener métricas y pruebas
model_metrics &lt;- function(model, model_name, listw = NULL) {
  # Summary del modelo
  model_summary &lt;- summary(model)
  
  # Coeficientes y p-values
  if (inherits(model, &quot;gls&quot;)) {
    coefficients &lt;- as.data.frame(coef(model_summary))
    coefficients$term &lt;- rownames(coefficients)
    rownames(coefficients) &lt;- NULL
    coefficients &lt;- coefficients %&gt;%
      rename(estimate = Value, `[std.error]` = Std.Error, p.value = `p-value`)
  } else if (inherits(model, &quot;gam&quot;)) {
    coefficients &lt;- summary(model)$s.table
    coefficients &lt;- as.data.frame(coefficients)
    coefficients$term &lt;- rownames(coefficients)
    rownames(coefficients) &lt;- NULL
    coefficients &lt;- coefficients %&gt;%
      rename(estimate = edf, `[std.error]` = Ref.df, p.value = `p-value`)
    if (!&quot;p.value&quot; %in% names(coefficients)) {
      coefficients$p.value &lt;- NA
    }
  } else {
    coefficients &lt;- tidy(model)
    coefficients &lt;- coefficients %&gt;%
      rename(`[std.error]` = std.error)
    if (!&quot;p.value&quot; %in% names(coefficients)) {
      coefficients$p.value &lt;- NA
    }
  }
  
  # AIC y BIC
  aic_value &lt;- AIC(model)
  bic_value &lt;- BIC(model)
  
  # Breusch-Pagan test
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;))) {
    bp_test &lt;- bptest(model)
    bp_stat &lt;- bp_test$statistic
    bp_pvalue &lt;- bp_test$p.value
  } else {
    bp_stat &lt;- NA
    bp_pvalue &lt;- NA
  }
  
  # Jarque-Bera test
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;, &quot;gls&quot;))) {
    jb_test &lt;- jarque.bera.test(residuals(model))
    jb_stat &lt;- jb_test$statistic
    jb_pvalue &lt;- jb_test$p.value
  } else {
    jb_stat &lt;- NA
    jb_pvalue &lt;- NA
  }
  
  # Prueba de multicolinealidad (VIF)
  if (inherits(model, c(&quot;lm&quot;, &quot;rlm&quot;, &quot;gls&quot;))) {
    vif_values &lt;- vif(model)
    vif_df &lt;- as.data.frame(vif_values)
    vif_df$term &lt;- rownames(vif_df)
    rownames(vif_df) &lt;- NULL
    colnames(vif_df)[1] &lt;- &quot;vif&quot;
  } else {
    vif_df &lt;- data.frame(term = NA, vif = NA)
  }
  
  # Índice de Moran si se proporciona una estructura espacial
  if (!is.null(listw)) {
    moran_test &lt;- moran.test(residuals(model), listw)
    moran_i &lt;- moran_test$estimate[1]
    moran_p &lt;- moran_test$p.value
  } else {
    moran_i &lt;- NA
    moran_p &lt;- NA
  }
  
  # Consolidar resultados en una lista
  results &lt;- list(
    model_name = model_name,
    coefficients = coefficients,
    aic = aic_value,
    bic = bic_value,
    bp_stat = bp_stat,
    bp_pvalue = bp_pvalue,
    jb_stat = jb_stat,
    jb_pvalue = jb_pvalue,
    moran_i = moran_i,
    moran_p = moran_p,
    vif = vif_df
  )
  
  return(results)
}

# Obtener métricas para cada modelo
lm_results &lt;- model_metrics(lm_model_2, &quot;Linear Model (lm)&quot;)
rlm_results &lt;- model_metrics(rlm_model_2, &quot;Robust Linear Model (rlm)&quot;)
sar_results &lt;- model_metrics(sar_model_2, &quot;Spatial Autoregressive Model (sar)&quot;, listw_5_s_out)
gls_results &lt;- model_metrics(gls_model_2, &quot;Generalized Least Squares (gls)&quot;)
weighted_results &lt;- model_metrics(weighted_model_2, &quot;Weighted Linear Model (lm)&quot;)
gam_results &lt;- model_metrics(gam_model_2, &quot;Generalized Additive Model (gam)&quot;)

# Crear una tabla consolidada
consolidate_results &lt;- function(results_list) {
  consolidated &lt;- bind_rows(lapply(results_list, function(result) {
    result$coefficients %&gt;%
      mutate(model = result$model_name,
             aic = result$aic,
             bic = result$bic,
             bp_stat = result$bp_stat,
             bp_pvalue = result$bp_pvalue,
             jb_stat = result$jb_stat,
             jb_pvalue = result$jb_pvalue,
             moran_i = result$moran_i,
             moran_p = result$moran_p) %&gt;%
      dplyr::select(model, term, estimate, `[std.error]`, p.value, aic, bic, bp_stat, bp_pvalue, jb_stat, jb_pvalue, moran_i, moran_p)
  }))
  
  # Añadir VIF
  vif_df &lt;- bind_rows(lapply(results_list, function(result) {
    if (!is.na(result$vif$term[1])) {
      result$vif %&gt;%
        mutate(model = result$model_name) %&gt;%
        dplyr::select(model, term, vif)
    } else {
      data.frame(model = result$model_name, term = NA, vif = NA)
    }
  }))
  
  consolidated &lt;- left_join(consolidated, vif_df, by = c(&quot;model&quot;, &quot;term&quot;))
  
  return(consolidated)
}

# Consolidar todos los resultados
all_results &lt;- consolidate_results(list(lm_results, rlm_results, sar_results, gls_results, weighted_results, gam_results))

# Definir una función para agregar estrellas de significancia
add_significance_stars &lt;- function(p_value) {
  if (is.na(p_value)) {
    return(&quot;&quot;)
  } else if (p_value &lt; 0.001) {
    return(&quot;***&quot;)
  } else if (p_value &lt; 0.01) {
    return(&quot;**&quot;)
  } else if (p_value &lt; 0.05) {
    return(&quot;*&quot;)
  } else if (p_value &lt; 0.1) {
    return(&quot;.&quot;)
  } else {
    return(&quot;&quot;)
  }
}

# Agregar estrellas de significancia
all_results &lt;- all_results %&gt;%
  mutate(significance = sapply(p.value, add_significance_stars))

# Renombrar &quot;term&quot; como &quot;Variable&quot; y poner std.error entre corchetes
colnames(all_results)[colnames(all_results) == &quot;term&quot;] &lt;- &quot;Variable&quot;
colnames(all_results)[colnames(all_results) == &quot;std.error&quot;] &lt;- &quot;[std.error]&quot;

# Concatenar &quot;std.error&quot; a &quot;estimate&quot; y asignarlo a &quot;Coeficiente&quot;
all_results$estimate &lt;- round(all_results$estimate,5)
all_results$&#39;[std.error]&#39; &lt;- round(all_results$&#39;[std.error]&#39;,5)
all_results &lt;- all_results %&gt;%
  mutate(Coeficiente = paste0(estimate, &quot; [&quot;, `[std.error]`, &quot;]&quot;, significance))

# Eliminar columnas innecesarias
resultados_completos &lt;- all_results %&gt;%
  dplyr::select(-estimate, -`[std.error]`, -p.value, -significance)


# Crear df1
df1 &lt;- resultados_completos %&gt;% 
  dplyr::select(model, Variable, Coeficiente) %&gt;%
  pivot_wider(names_from = model, values_from = Coeficiente)


df2 &lt;- resultados_completos %&gt;% 
  dplyr::select(model, aic, bic, bp_stat, bp_pvalue, jb_stat, jb_pvalue, moran_i, moran_p) %&gt;%
  distinct() %&gt;%
  pivot_longer(cols = -model, names_to = &quot;Variable&quot;, values_to = &quot;value&quot;) %&gt;%
  pivot_wider(names_from = model, values_from = value)


# Unir df1_long y df2 por la columna &#39;model&#39;
combined_df6 &lt;- rbind(df1, df2)

# # Guardar la tabla de resultados transpuesta en un archivo CSV
write.csv(combined_df6, &quot;Resultados/DE_SAR_modelos_results_3_CUADRATICOS_SIN_OUTLIERS.csv&quot;, row.names = FALSE)

# Crear la tabla flextable a partir del dataframe
ft6 &lt;- flextable(combined_df6)

# Personalizar la tabla (opcional)
ft6 &lt;- ft6 %&gt;%
  set_header_labels(values = colnames(combined_df6)) %&gt;% # Asegura que los nombres de las columnas sean correctos
  theme_vanilla() %&gt;% 
  autofit()

# Obtener la fecha actual en formato YYYYMMDD
current_date &lt;- format(Sys.Date(), &quot;%Y%m%d&quot;)

# Crear el nombre del archivo incluyendo la fecha
file_name6 &lt;- paste0(&quot;Resultados/DE_SAR_modelos_results_3_CUADRATICOS_SIN_OUTLIERS_&quot;, current_date, &quot;.docx&quot;)

# Crear un documento Word y añadir la tabla
doc6 &lt;- read_docx() %&gt;%
  body_add_flextable(ft6)

# Guardar el documento Word con el nombre dinámico
print(doc6, target = file_name6)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Análisis de Colinealidad y Multicolinealidad:</li>
</ol>
<p>eigprop(mod=lm_model_2), eigprop(mod=rlm_model_2),
eigprop(mod=weighted_model_2): Propiedades de los valores propios
(eigprop): Estas líneas calculan las propiedades de los valores propios
de las matrices de diseño de los modelos lineales (lm_model_2), robusto
(rlm_model_2), y ponderado (weighted_model_2). Los valores propios
proporcionan información sobre la colinealidad en los modelos. Un
pequeño valor propio indica posible colinealidad, lo que sugiere que
algunas variables pueden estar altamente correlacionadas.</p>
<p>mctest(mod=lm_model_2), mctest(mod=rlm_model_2),
mctest(mod=weighted_model_2):<br />
Prueba de multicolinealidad (mctest): Estas pruebas evalúan la presencia
de multicolinealidad en los modelos. La multicolinealidad ocurre cuando
dos o más variables independientes en un modelo están altamente
correlacionadas, lo que puede hacer que sea difícil estimar los
coeficientes del modelo con precisión. 2. Evaluación del Modelo SAR:
Cálculo de residuos (residuals_sar_2): Se calculan los residuos del
modelo SAR (sar_model_2). Estos residuos representan la diferencia entre
los valores observados y los predichos por el modelo.</p>
<p>Cálculo del RMSE (RMSE_sar_2): El RMSE (Root Mean Squared Error) se
calcula para evaluar el error promedio de predicción del modelo. Un
valor más bajo de RMSE indica un mejor ajuste del modelo a los
datos.</p>
<p>Cálculo del R² (r_squared): El coeficiente de determinación R² se
calcula para el modelo SAR, midiendo la proporción de la varianza total
explicada por el modelo. Un valor de R² más cercano a 1 indica un modelo
que explica mejor la variabilidad de los datos.</p>
<ol start="3" style="list-style-type: decimal">
<li>Prueba de Breusch-Pagan para Heterocedasticidad: Prueba de
Breusch-Pagan (bp_test): Esta prueba evalúa la heterocedasticidad en los
residuos del modelo. La heterocedasticidad ocurre cuando la varianza de
los residuos no es constante a lo largo de los valores de las variables
independientes, lo que puede violar los supuestos de los modelos de
regresión lineal.</li>
</ol>
<p>Cálculo de residuos en un modelo GLS (residuos_gls): Se calculan los
residuos del modelo de mínimos cuadrados generalizados (gls_model_2)
usando el tipo de residuos “pearson”, que son residuos estandarizados.
Estos residuos se utilizan para realizar la prueba de Breusch-Pagan.</p>
<p>Modelo auxiliar (modelo_aux): Se define un modelo auxiliar en el que
los residuos cuadrados del modelo GLS se regresan sobre las variables
independientes. Este modelo auxiliar es necesario para calcular las
estadísticas de la prueba de Breusch-Pagan.</p>
<p>Resultados del test de Breusch-Pagan (bp_test): Finalmente, se
realiza y se muestran los resultados del test de Breusch-Pagan. Un
p-valor bajo indicaría la presencia de heterocedasticidad en el
modelo.</p>
<ol start="4" style="list-style-type: decimal">
<li>Evaluación Global del Modelo SAR y GLS:<br />
Validación y diagnóstico del modelo SAR (sar_model_2): Se calcula el R²
y el RMSE para el modelo SAR para verificar su capacidad de predicción y
ajuste a los datos.</li>
</ol>
<p>Pruebas de diagnóstico para modelos GLS y SAR: Las pruebas
adicionales, como el test de Breusch-Pagan, permiten diagnosticar
problemas potenciales en el modelo, como heterocedasticidad, que pueden
afectar la validez de las inferencias hechas a partir del modelo.</p>
<p>Conclusión:<br />
Este código realiza un análisis detallado de varios modelos de
regresión, incluyendo modelos lineales, robustos y ponderados, así como
un modelo espacial autoregresivo (SAR). Se abordan aspectos críticos
como la colinealidad entre las variables explicativas y la
heterocedasticidad de los residuos. Las métricas de rendimiento, como el
RMSE y el R², junto con pruebas de diagnóstico como Breusch-Pagan,
permiten evaluar la adecuación y calidad de los modelos ajustados. La
comprensión y corrección de estos problemas son esenciales para
garantizar la validez y fiabilidad de los resultados obtenidos a partir
de los modelos.</p>
<pre class="r"><code>eigprop(mod =lm_model_2)
eigprop(mod= rlm_model_2)
eigprop(mod= weighted_model_2)
mctest(mod=lm_model_2)
mctest(mod=rlm_model_2)
mctest(mod=weighted_model_2)
###############################################################################################
# Obtener los residuos del modelo SAR
residuals_sar_2 &lt;- residuals(sar_model_2)

# Calcular los errores del modelo SAR
errors_sar_2 &lt;- data_mun15$tasa_dfi - fitted(sar_model_2)
RMSE_sar_2 &lt;- sqrt(mean(errors_sar_2^2))

# Calcular el R^2
valores_observados &lt;- sar_model_2$y
valores_ajustados &lt;- fitted(sar_model_2)

# Suma de los cuadrados totales (SST)
sst &lt;- sum((valores_observados - mean(valores_observados))^2)

# Suma de los cuadrados residuales (SSR)
ssr &lt;- sum(residuals(sar_model_2)^2)

# R^2
r_squared &lt;- 1 - (ssr / sst)

# Mostrar los resultados
print(paste(&quot;RMSE (sar_model_2):&quot;, RMSE_sar_2))
print(paste(&quot;R^2 (sar_model_2):&quot;, r_squared))


### BREUCH PAGAN 

eigprop(mod =lm_model_2)
eigprop(mod= rlm_model_2)
eigprop(mod= weighted_model_2
)
# Obtener los valores ajustados de ambos modelos
fitted_values_sar_2 &lt;- fitted(sar_model_2)


# Obtener los residuos en la escala logarítmica
residuals_sar_2 &lt;- residuals(sar_model_2)

# Calcular los errores de ambos modelos
errors_sar_2 &lt;- data_mun15$tasa_dfi - fitted_values_sar_2

# Calcular el RMSE para ambos modelos
RMSE_sar_1 &lt;- sqrt(mean(errors_sar_1^2))
RMSE_sar_2 &lt;- sqrt(mean(errors_sar_2^2))
RMSE_sar_3 &lt;- sqrt(mean(errors_sar_3^2))
# print(paste(&quot;RMSE del modelo SAR Logarítmico (Aproximación A):&quot;, RMSE_log_1))

# Modelo SAR
modelo &lt;- sar_model_2

# Valores observados y valores ajustados
valores_observados &lt;- modelo$y
valores_ajustados &lt;- fitted(modelo)

# Suma de los cuadrados totales (SST)
sst &lt;- sum((valores_observados - mean(valores_observados))^2)

# Suma de los cuadrados residuales (SSR)
ssr &lt;- sum(residuals(modelo)^2)

# R^2
r_squared &lt;- 1 - (ssr / sst)

# Mostrar el valor de R^2
r_squared


# Calcular los residuos del modelo GLS
residuos_gls &lt;- residuals(gls_model_2, type = &quot;pearson&quot;)

# Definir el modelo auxiliar para el test de Breusch-Pagan
# Este modelo auxiliar es una regresión de los residuos cuadrados sobre las variables independientes
modelo_aux &lt;- lm(residuos_gls^2 ~ tasa_promedio_hom + tasa_prom_cuadrado  + salario_mun_mediano + presion_demo + ln_pob + factor(cluster), data = data_mun15_sin_atipicos)

# Obtener los estadísticos del test de Breusch-Pagan
bp_test &lt;- bptest(modelo_aux)

# Mostrar los resultados del test de Breusch-Pagan
print(bp_test)</code></pre>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
